#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#


# Continuation of 20220912-oom-experiments-01.txt

# Target: Run various configurations to try to run the notebook provided by Nigel (Work in progress with Gaia XP spectra) successfully
#    The following two cells fails for the existing configuration, with the symptoms being failed tasks, which eventually make the cells fail with an error message
#    The logs of the failed tasks, imply that the containers that are failing are running out of Memory


# The Cell in question:

# Cell # 1
----------
%pyspark
#similar_df.count()

# collect/cash stats to the head for plotting in the next cell - this actually actions the full workflow on the Spark cluster
stats_pdf = similar_df.select('p_xp_similar').toPandas()
# benchmarks: 1 in 10000 in 45min 21sec; no failed tasks
#             1 in 1000  in 45min 21sec; ditto
#             1 in 10    in 46min 57sec; successful completion albeit with 10 failed tasks 



# Second cell that also fails
# Cell # 2
----------
%pyspark

# convert to a Pandas dataframe for GaiaXPy
continuous_spectra = similar_df.toPandas()

# convert to sampled form:
sampled_spectra, sampling = convert(continuous_spectra, save_file = False)
    
# plot to sanity check:
plot_spectra(sampled_spectra, sampling = sampling, multi=True, show_plot=True, output_path=None, legend=True)


# Run the cell #1 with default settings

# Failed
20/2048 (160 failed) (130 killed: Stage cancelled)

# Check logs for memory messages

# fedora@worker01

cd /var/hadoop/logs/application_1661872020086_0036
grep -r "Memory"
..
container_1661872020086_0036_01_000041/stderr:2022-09-13 10:16:07,048 INFO memory.MemoryStore: MemoryStore started with capacity 3.6 GiB
container_1661872020086_0036_01_000041/stderr:2022-09-13 10:16:08,502 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 91.6 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000041/stderr:2022-09-13 10:16:08,608 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 160.3 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000041/stderr:2022-09-13 10:16:12,007 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000041/stderr:2022-09-13 10:16:12,109 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 459.7 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000041/stderr:2022-09-13 10:17:30,363 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 30.9 KiB, free 452.3 MiB)
container_1661872020086_0036_01_000041/stderr:2022-09-13 10:17:30,375 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 459.7 KiB, free 451.8 MiB)
container_1661872020086_0036_01_000082/launch_container.sh:exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx7168m -Djava.io.tmpdir=$PWD/tmp '-Dspark.driver.port=40811' -Dspark.yarn.app.container.log.dir=/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000082 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@zeppelin:40811 --executor-id 56 --hostname worker01 --cores 5 --app-id application_1661872020086_0036 --resourceProfileId 0 --user-class-path file:$PWD/__app__.jar 1>/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000082/stdout 2>/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000082/stderr"
container_1661872020086_0036_01_000082/prelaunch.err:/bin/bash: line 1: 180973 Killed                  /etc/alternatives/jre/bin/java -server -Xmx7168m -Djava.io.tmpdir=/var/hadoop/data/usercache/Evison/appcache/application_1661872020086_0036/container_1661872020086_0036_01_000082/tmp '-Dspark.driver.port=40811' -Dspark.yarn.app.container.log.dir=/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000082 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@zeppelin:40811 --executor-id 56 --hostname worker01 --cores 5 --app-id application_1661872020086_0036 --resourceProfileId 0 --user-class-path file:/var/hadoop/data/usercache/Evison/appcache/application_1661872020086_0036/container_1661872020086_0036_01_000082/__app__.jar > /var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000082/stdout 2> /var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000082/stderr
container_1661872020086_0036_01_000082/stderr:2022-09-13 10:22:07,707 INFO memory.MemoryStore: MemoryStore started with capacity 3.6 GiB
container_1661872020086_0036_01_000082/stderr:2022-09-13 10:22:14,721 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 91.6 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000082/stderr:2022-09-13 10:22:15,693 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 160.3 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000082/stderr:2022-09-13 10:22:32,998 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000082/stderr:2022-09-13 10:22:33,373 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 459.7 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000082/stderr:2022-09-13 10:24:34,600 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 30.9 KiB, free 313.7 MiB)
container_1661872020086_0036_01_000082/stderr:2022-09-13 10:24:34,663 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 459.7 KiB, free 313.3 MiB)
container_1661872020086_0036_01_000006/launch_container.sh:exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx7168m -Djava.io.tmpdir=$PWD/tmp '-Dspark.driver.port=40811' -Dspark.yarn.app.container.log.dir=/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000006 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@zeppelin:40811 --executor-id 5 --hostname worker01 --cores 5 --app-id application_1661872020086_0036 --resourceProfileId 0 --user-class-path file:$PWD/__app__.jar 1>/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000006/stdout 2>/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000006/stderr"
container_1661872020086_0036_01_000006/stderr:2022-09-13 10:14:09,114 INFO memory.MemoryStore: MemoryStore started with capacity 3.6 GiB
container_1661872020086_0036_01_000006/stderr:2022-09-13 10:14:29,216 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000006/stderr:2022-09-13 10:14:29,305 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.4 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000006/stderr:2022-09-13 10:14:39,833 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000006/stderr:2022-09-13 10:14:39,839 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 30.0 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000006/stderr:2022-09-13 10:15:41,722 INFO memory.MemoryStore: MemoryStore cleared
container_1661872020086_0036_01_000121/launch_container.sh:exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx7168m -Djava.io.tmpdir=$PWD/tmp '-Dspark.driver.port=40811' -Dspark.yarn.app.container.log.dir=/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000121 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@zeppelin:40811 --executor-id 73 --hostname worker01 --cores 5 --app-id application_1661872020086_0036 --resourceProfileId 0 --user-class-path file:$PWD/__app__.jar 1>/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000121/stdout 2>/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000121/stderr"
container_1661872020086_0036_01_000121/stderr:2022-09-13 10:29:18,612 INFO memory.MemoryStore: MemoryStore started with capacity 3.6 GiB
container_1661872020086_0036_01_000121/stderr:2022-09-13 10:29:26,698 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 91.6 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000121/stderr:2022-09-13 10:29:27,505 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 160.3 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000121/stderr:2022-09-13 10:29:47,111 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000121/stderr:2022-09-13 10:29:47,368 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 459.7 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000121/stderr:2022-09-13 10:31:28,392 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 30.9 KiB, free 464.1 MiB)
container_1661872020086_0036_01_000121/stderr:2022-09-13 10:31:28,463 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 459.7 KiB, free 463.7 MiB)
container_1661872020086_0036_01_000038/launch_container.sh:exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx7168m -Djava.io.tmpdir=$PWD/tmp '-Dspark.driver.port=40811' -Dspark.yarn.app.container.log.dir=/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000038 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@zeppelin:40811 --executor-id 23 --hostname worker01 --cores 5 --app-id application_1661872020086_0036 --resourceProfileId 0 --user-class-path file:$PWD/__app__.jar 1>/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000038/stdout 2>/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000038/stderr"
container_1661872020086_0036_01_000038/prelaunch.err:/bin/bash: line 1: 179914 Killed                  /etc/alternatives/jre/bin/java -server -Xmx7168m -Djava.io.tmpdir=/var/hadoop/data/usercache/Evison/appcache/application_1661872020086_0036/container_1661872020086_0036_01_000038/tmp '-Dspark.driver.port=40811' -Dspark.yarn.app.container.log.dir=/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000038 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@zeppelin:40811 --executor-id 23 --hostname worker01 --cores 5 --app-id application_1661872020086_0036 --resourceProfileId 0 --user-class-path file:/var/hadoop/data/usercache/Evison/appcache/application_1661872020086_0036/container_1661872020086_0036_01_000038/__app__.jar > /var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000038/stdout 2> /var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000038/stderr
container_1661872020086_0036_01_000038/stderr:2022-09-13 10:16:07,088 INFO memory.MemoryStore: MemoryStore started with capacity 3.6 GiB
container_1661872020086_0036_01_000038/stderr:2022-09-13 10:16:08,572 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 91.6 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000038/stderr:2022-09-13 10:16:08,669 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 160.3 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000038/stderr:2022-09-13 10:16:11,985 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000038/stderr:2022-09-13 10:16:12,104 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 459.7 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000038/stderr:2022-09-13 10:18:27,804 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 30.9 KiB, free 304.1 MiB)
container_1661872020086_0036_01_000038/stderr:2022-09-13 10:18:27,877 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 459.7 KiB, free 303.7 MiB)
container_1661872020086_0036_01_000034/launch_container.sh:exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx7168m -Djava.io.tmpdir=$PWD/tmp '-Dspark.driver.port=40811' -Dspark.yarn.app.container.log.dir=/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000034 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@zeppelin:40811 --executor-id 21 --hostname worker01 --cores 5 --app-id application_1661872020086_0036 --resourceProfileId 0 --user-class-path file:$PWD/__app__.jar 1>/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000034/stdout 2>/var/hadoop/logs/application_1661872020086_0036/container_1661872020086_0036_01_000034/stderr"
container_1661872020086_0036_01_000034/stderr:2022-09-13 10:14:36,503 INFO memory.MemoryStore: MemoryStore started with capacity 3.6 GiB
container_1661872020086_0036_01_000034/stderr:2022-09-13 10:14:40,108 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000034/stderr:2022-09-13 10:14:40,209 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 30.0 KiB, free 3.6 GiB)
container_1661872020086_0036_01_000034/stderr:2022-09-13 10:15:41,783 INFO memory.MemoryStore: MemoryStore cleared



# Why is MemoryStore set to 3.6 GiB?


# Set Executor memory and memoryOverhead to the limits of a Yarn container (Single Node capability)
# ---------------------------------------------------------------------------------------------------

# Calculated using Cheatsheet.xlsx
spark.driver.memory                 58982m
spark.driver.memoryOverhead          9216m
spark.driver.cores                       5
spark.driver.maxResultSize          40960m

spark.executor.memory                36168m
spark.executor.memoryOverhead        4024m
spark.executor.cores                     5
#spark.executor.instances               30

#spark.default.parallelism              300
#spark.sql.shuffle.partitions          300

# YARN Application Master settings
spark.yarn.am.memory                 2048m
spark.yarn.am.cores                      1

spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
spark.dynamicAllocation.minExecutors      1
 # spark.executor.instances from Cheatsheet
spark.dynamicAllocation.maxExecutors     30
 # maxExecutors / 2
spark.dynamicAllocation.initialExecutors          15
spark.dynamicAllocation.cachedExecutorIdleTimeout 60s
spark.dynamicAllocation.executorIdleTimeout       60s
spark.sql.execution.arrow.pyspark.enabled            true



# After 15 mins

# 0 Failed Tasks / 171 tasks Succeeded
# 25 Active Jobs
# Ram used # Aprox 50 - 60 % per worker node , 1 worker node has only 5% used


# Kill Job, and start cell #2 

# Job Failed after 16mins

 > Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 147 tasks (40.0 GiB) is bigger than spark.driver.maxResultSize (40.0 GiB)



# Check application/containers for messages referring to "Memory"

..
container_1661872020086_0041_01_000008/stderr:2022-09-13 10:51:37,661 INFO memory.MemoryStore: MemoryStore started with capacity 18.7 GiB
container_1661872020086_0041_01_000008/stderr:2022-09-13 10:51:38,737 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 91.6 KiB, free 18.7 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 10:51:38,819 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 160.3 KiB, free 18.7 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 10:51:41,263 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 18.7 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 10:51:41,315 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 589.9 KiB, free 18.7 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 10:52:23,988 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 30.9 KiB, free 11.9 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 10:52:23,999 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 589.9 KiB, free 11.9 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 11:04:06,035 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 101.3 KiB, free 18.7 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 11:04:06,038 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 189.5 KiB, free 18.7 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 11:04:06,176 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 31.6 KiB, free 18.7 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 11:04:06,187 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 589.9 KiB, free 18.7 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 11:04:47,483 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 30.9 KiB, free 11.9 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 11:04:47,493 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 589.9 KiB, free 11.9 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 11:05:33,363 INFO memory.MemoryStore: Block taskresult_4353 stored as bytes in memory (estimated size 273.5 MiB, free 17.1 GiB)

..
container_1661872020086_0041_01_000008/stderr:2022-09-13 11:16:41,790 INFO memory.MemoryStore: Block taskresult_4519 stored as bytes in memory (estimated size 280.9 MiB, free 7.2 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 11:16:45,573 INFO memory.MemoryStore: Block taskresult_4523 stored as bytes in memory (estimated size 277.6 MiB, free 7.3 GiB)
container_1661872020086_0041_01_000008/stderr:2022-09-13 11:16:45,575 INFO memory.MemoryStore: Block taskresult_4522 stored as bytes in memory (estimated size 278.3 MiB, free 7.0 GiB)
container_1661872020086_0041_01_000008/stderr:java.lang.OutOfMemoryError: GC overhead limit exceeded
container_1661872020086_0041_01_000008/stderr:java.lang.OutOfMemoryError: GC overhead limit exceeded
container_1661872020086_0041_01_000008/stderr:java.lang.OutOfMemoryError: GC overhead limit exceeded



# How do we get 18.7?




# Set Executors to 25
# ---------------------------------------------------------------------------------------------------

# Calculated using Cheatsheet.xlsx
spark.driver.memory                 58982m
spark.driver.memoryOverhead          9216m
spark.driver.cores                       5
spark.driver.maxResultSize          40960m

spark.executor.memory                7168m
spark.executor.memoryOverhead        1024m
spark.executor.cores                     5
#spark.executor.instances               30

#spark.default.parallelism              300
#spark.sql.shuffle.partitions          300

# YARN Application Master settings
spark.yarn.am.memory                 2048m
spark.yarn.am.cores                      1

spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
spark.dynamicAllocation.minExecutors      1
 # spark.executor.instances from Cheatsheet
spark.dynamicAllocation.maxExecutors     30
 # maxExecutors / 2
spark.dynamicAllocation.initialExecutors          15
spark.dynamicAllocation.cachedExecutorIdleTimeout 60s
spark.dynamicAllocation.executorIdleTimeout       60s
spark.sql.execution.arrow.pyspark.enabled            true


# Cell 1 Failed tasks..
# Check UI:
# 5 containers per node on 5 nodes, 3 containers on the last one, 40 Gb RAM used on the 5 nodes



# Modify config, so that we get less containers per node, and that we are not too close to the available RAM
# ----------------------------------------------------------------------------------------------------------


spark.driver.memory                 58982m
spark.driver.memoryOverhead          9216m
spark.driver.cores                       5
spark.driver.maxResultSize          40960m

spark.executor.memory                10240m
spark.executor.memoryOverhead        1024m
spark.executor.cores                     5
#spark.executor.instances               30

#spark.default.parallelism              300
#spark.sql.shuffle.partitions          300

# YARN Application Master settings
spark.yarn.am.memory                 2048m
spark.yarn.am.cores                      1

spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
spark.dynamicAllocation.minExecutors      1
 # spark.executor.instances from Cheatsheet
spark.dynamicAllocation.maxExecutors     23
 # maxExecutors / 2
spark.dynamicAllocation.initialExecutors          15
spark.dynamicAllocation.cachedExecutorIdleTimeout 60s
spark.dynamicAllocation.executorIdleTimeout       60s
spark.sql.execution.arrow.pyspark.enabled            true


# Run Cell #1
# After 20 mins, no failed tasks
# Kill job


# Modify config, same number of containers, less RAM per container
# ----------------------------------------------------------------------------------------------------------

spark.driver.memory                 58982m
spark.driver.memoryOverhead          9216m
spark.driver.cores                       5
spark.driver.maxResultSize          40960m

spark.executor.memory                3072m
spark.executor.memoryOverhead        360m
spark.executor.cores                     5
#spark.executor.instances               30

#spark.default.parallelism              300
#spark.sql.shuffle.partitions          300

# YARN Application Master settings
spark.yarn.am.memory                 2048m
spark.yarn.am.cores                      1

spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
spark.dynamicAllocation.minExecutors      1
 # spark.executor.instances from Cheatsheet
spark.dynamicAllocation.maxExecutors     30
 # maxExecutors / 2
spark.dynamicAllocation.initialExecutors          15
spark.dynamicAllocation.cachedExecutorIdleTimeout 60s
spark.dynamicAllocation.executorIdleTimeout       60s
spark.sql.execution.arrow.pyspark.enabled            true


# Run Cell #1
# After 30 mins, no failed tasks


# Modify config, 1 container per node
# ----------------------------------------------------------------------------------------------------------

spark.driver.memory                 58982m
spark.driver.memoryOverhead          9216m
spark.driver.cores                       5
spark.driver.maxResultSize          40960m

spark.executor.memory                30072m
spark.executor.memoryOverhead        3600m
spark.executor.cores                     24
#spark.executor.instances               30

#spark.default.parallelism              300
#spark.sql.shuffle.partitions          300

# YARN Application Master settings
spark.yarn.am.memory                 2048m
spark.yarn.am.cores                      1

spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
spark.dynamicAllocation.minExecutors	  1
 # spark.executor.instances from Cheatsheet
spark.dynamicAllocation.maxExecutors     6
 # maxExecutors / 2
spark.dynamicAllocation.initialExecutors          1
spark.dynamicAllocation.cachedExecutorIdleTimeout 60s
spark.dynamicAllocation.executorIdleTimeout	  60s
spark.sql.execution.arrow.pyspark.enabled            true



# Run Cell #1
# After 2 hours and 30 mins, no failed tasks


# One useful link to check out if a given config will run cell #1 is the nodes page of the Yarn UI:
   http://localhost:8088/cluster/nodes
   
# This shows us how many containers (executors) are running per node, and how much memory in total they are using on each node

   
   
# Modify config, 20 containers, 5 vcpu per container
# ----------------------------------------------------------------------------------------------------------

spark.driver.memory                 37888m
spark.driver.memoryOverhead           5120
spark.driver.cores                       5
spark.driver.maxResultSize          30720m

spark.executor.memory                 7168m
spark.executor.memoryOverhead         1024m
spark.executor.cores                     5
#spark.executor.instances               30

#spark.yarn.executor.memoryOverhead     1024m
#spark.yarn.driver.memoryOverhead  1024m        
#spark.yarn.am.memoryOverhead   1024m

#spark.default.parallelism              100
#spark.sql.shuffle.partitions          100

# YARN Application Master settings
spark.yarn.am.memory                 2048m
spark.yarn.am.cores                      1

spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
spark.dynamicAllocation.minExecutors	  1
 # spark.executor.instances from Cheatsheet
spark.dynamicAllocation.maxExecutors     20
 # maxExecutors / 2
spark.dynamicAllocation.initialExecutors          1
spark.dynamicAllocation.cachedExecutorIdleTimeout 60s
spark.dynamicAllocation.executorIdleTimeout	  60s
spark.sql.execution.arrow.pyspark.enabled            true

# Run Cell #1
# Completed After 3 hour 30 mins, no failed tasks


# Run Cell #2

# Failed
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 34 in stage 4.0 failed 4 times, most recent failure: Lost task 34.3 in stage 4.0 (TID 6310) (worker02 executor 53): ExecutorLostFailure (executor 53 exited caused by one of the running tasks) Reason: Container from a bad node: container_1661872020086_0068_01_000064 on host: worker02. Exit status: 143. Diagnostics: [2022-09-14 19:31:45.971]Container killed on request. Exit code is 143

# Restart Zeppelin and run cell #1 again to confirm that it works without failed tasks
# Run Cell #1
# 5 failed tasks after 10 mins..
	

   
# 30 containers
# 6 Gb RAM per executor
# 5 cores
# Change Yarn settings to reduce total RAM used per node
# ----------------------------------------------------------------------------------------------------------

spark.driver.memory                 37888m
spark.driver.memoryOverhead           5120
spark.driver.cores                       5
spark.driver.maxResultSize          30720m

spark.executor.memory                 6144m
spark.executor.memoryOverhead         614m
spark.executor.cores                     5
#spark.executor.instances               30

#spark.yarn.executor.memoryOverhead     1024m
#spark.yarn.driver.memoryOverhead  1024m        
#spark.yarn.am.memoryOverhead   1024m

#spark.default.parallelism              100
#spark.sql.shuffle.partitions          100

# YARN Application Master settings
spark.yarn.am.memory                 2048m
spark.yarn.am.cores                      1

spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
spark.dynamicAllocation.minExecutors	  1
 # spark.executor.instances from Cheatsheet
spark.dynamicAllocation.maxExecutors     30
 # maxExecutors / 2
spark.dynamicAllocation.initialExecutors          1
spark.dynamicAllocation.cachedExecutorIdleTimeout 60s
spark.dynamicAllocation.executorIdleTimeout	  60s
spark.sql.execution.arrow.pyspark.enabled            true

..


# Yarn:
   
cat /opt/hadoop/etc/hadoop/yarn-site.xml  

<property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>40960</value>
</property>

<!--+
    | Minimum limit of memory to allocate to each container request at the Resource Manager.
    +-->
<property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>1024</value>
</property>

<property>
    <name>yarn.scheduler.minimum-allocation-vcores</name>
    <value>1</value>
</property>

<property>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>25</value>
</property>

<property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>40960</value>
</property>
   
   
# Run Cell #1
# Several Failed jobs
# RAM usage going up to 41/42 on several nodes
# Yarn UI shows 5 containers on each of the nodes (6 on the node running the AM master)
# Mem used 35/40 (Note that we reduced Yarn RAM available to 40Gb)

# Run top on one of the worker nodes

 289827 fedora    20   0 9681008   7.4g      0 S  24.9  17.5  16:55.49 java                                                                                                                                
 290951 fedora    20   0 9417012   7.2g      0 S   0.0  17.1   6:07.11 java                                                                                                                                
 289990 fedora    20   0 9477280   7.1g      0 S   2.7  16.9  14:47.12 java                                                                                                                                
 291143 fedora    20   0 9435580   7.0g   3832 S   0.0  16.6   5:43.54 java                                                                                                                                
 291333 fedora    20   0 9451688   5.9g  12608 S   0.3  14.1   5:36.83 java                                                                                                                                
 287492 fedora    20   0   12.8g 423588      0 S   4.3   1.0   4:47.45 java                                                                                                                                
 287357 fedora    20   0   12.6g 346264      0 S   0.7   0.8   0:30.96 java                                                                                                                                
 291425 fedora    20   0 1087960 248988  43488 R 102.3   0.6  19:34.40 python                                                                                                                              
 291429 fedora    20   0 1087960 248924  43424 R 107.0   0.6  19:35.06 python                                                                                                                              
 291430 fedora    20   0 1087960 248920  43420 R 101.7   0.6  19:30.78 python                                                                                                                              
 291423 fedora    20   0 1087960 248916  43416 R 106.6   0.6  19:28.09 python                                                                                                                              
 291435 fedora    20   0 1087960 248916  43416 R 100.3   0.6  19:41.02 python                                                                                                                              
 290288 fedora    20   0 1105308 240112  17128 R 125.9   0.5  79:55.30 python                                                                                                                              
 290331 fedora    20   0 1104804 239636  17172 R 128.6   0.5  81:25.57 python                                                                                                                              
 290340 fedora    20   0 1105308 239244  16268 R 131.9   0.5  80:42.64 python                                                                                                                              
 290343 fedora    20   0 1103340 239104  18192 R 123.3   0.5  81:47.55 python                                                                                                                              
 290332 fedora    20   0 1104192 238840  16980 R 121.9   0.5  81:49.19 python                                                                                                                              
 290297 fedora    20   0 1102008 236956  17268 R 125.6   0.5  80:09.40 python                                                                                                                              
 290280 fedora    20   0 1089304 225168  18188 R 135.2   0.5  79:53.67 python                                                                                                                              
 290282 fedora    20   0 1089304 223832  16912 R 129.2   0.5  82:56.34 python                                                                                                                              
 291235 fedora    20   0 1089164 223256  16440 R 102.3   0.5  25:47.61 python                                                                                                                              
 291240 fedora    20   0 1089164 223236  16420 R 101.3   0.5  25:40.60 python                                                                                                                              
 291246 fedora    20   0 1089164 223160  16344 R 100.3   0.5  25:55.02 python                                                                                                                              
 291049 fedora    20   0 1089164 223072  16260 R  98.7   0.5  30:48.51 python                                                                                                                              
 291234 fedora    20   0 1089164 223012  16196 R 101.0   0.5  25:49.41 python                                                                                                                              
 291242 fedora    20   0 1089164 222972  16156 R 102.7   0.5  25:54.79 python                                                                                                                              
 291037 fedora    20   0 1089164 222452  15500 R  99.3   0.5  31:04.50 python                                                                                                                              
 291043 fedora    20   0 1089164 221984  15032 R 101.3   0.5  31:12.98 python                                                                                                                              
 291039 fedora    20   0 1089164 221668  14716 R 100.0   0.5  31:03.29 python                                                                                                                              
 291046 fedora    20   0 1089164 221616  14664 R 100.3   0.5  31:00.49 python   
 290304 fedora    20   0  965792  99444  15716 S   0.0   0.2  81:34.64 python                                                                                                                              
 291414 fedora    20   0  280148  36808  11080 S   0.0   0.1   0:01.09 python                                                                                                                              
 291225 fedora    20   0  280148  26232    504 S   0.0   0.1   0:01.10 python                                                                                                                              
 290241 fedora    20   0  280148  25728      0 S   0.0   0.1   0:02.11 python                                                                                                                              
 290263 fedora    20   0  280148  25728      0 S   0.0   0.1   0:02.05 python                                                                                                                              
 291028 fedora    20   0  280148  25728      0 S   0.0   0.1   0:02.32 python 
 ..
 
# java processes taking up 80% python processes taking up 10+%
 



   
# 30 containers
# 8 (9) Gb RAM per executor
# 5 cores
# Change Yarn settings to reduce total RAM used per node
# ----------------------------------------------------------------------------------------------------------

# Spark: 

spark.driver.memory                 37888m
spark.driver.memoryOverhead           5120
spark.driver.cores                       5
spark.driver.maxResultSize          30720m

spark.executor.memory                 8192m
spark.executor.memoryOverhead         819m
spark.executor.cores                     5
#spark.executor.instances               30

#spark.yarn.executor.memoryOverhead     1024m
#spark.yarn.driver.memoryOverhead  1024m        
#spark.yarn.am.memoryOverhead   1024m

#spark.default.parallelism              100
#spark.sql.shuffle.partitions          100

# YARN Application Master settings
spark.yarn.am.memory                 2048m
spark.yarn.am.cores                      1

spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
spark.dynamicAllocation.minExecutors	  1
 # spark.executor.instances from Cheatsheet
spark.dynamicAllocation.maxExecutors     30
 # maxExecutors / 2


# Yarn Settings

<!--+
    | Maximum limit of memory to allocate to each container request at the Resource Manager.
    +-->
<property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>35840</value>
</property>

<!--+
    | Minimum limit of memory to allocate to each container request at the Resource Manager.
    +-->
<property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>1024</value>
</property>

<property>
    <name>yarn.scheduler.minimum-allocation-vcores</name>
    <value>1</value>
</property>

<property>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>25</value>
</property>

<property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>35840</value>
</property>

<!--+
    | 1:1 -> 1:4 * 26 based on IO wait
    +-->
<property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>25</value>
</property>



# Run Cell #1
# Completed after 2.7 hours, no failed tasks

# Run public Examples

3. Source counts over the sky
  42 seconds

6. Working with cross-matched surveys
  1 min 46 seconds

7. Good astrometric solutions via ML Random Forrest classifier
  8 mins

