#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#

    Target:

        Deploy with more recent version of components:
          Zeppelin 0.9.2
          Spark 3.0.3
          Hadoop 3.1.2


    Result:





# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    docker run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --publish 3000:3000 \
        --publish 8088:8088 \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        atolmis/ansible-client:2020.12.02 \
        bash


# -----------------------------------------------------
# Set the target cloud.
#[root@ansibler]

    cloudname=gaia-test


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

	> Done

	> real	3m48.394s
	> user	0m46.416s
	> sys	0m4.422s


# -----------------------------------------------------
# Create everything, using a standard config.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}" \
            'cclake-medium-04' 


	> real	68m23.293s
	> user	15m49.227s
	> sys	3m38.770s

# -----------------------------------------------------
# Run Notebook tests

# From Zeppelin UI

# Run Set Up:

# Cell #1
java.lang.RuntimeException: Interpreter Setting 'md' is not ready, its status is DOWNLOADING_DEPENDENCIES
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:428)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:72)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:182)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


# Run a Spark Cell (Cell #2)

org.apache.zeppelin.interpreter.InterpreterException: java.io.IOException: Fail to launch interpreter process:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/fedora/zeppelin-0.9.0-bin-all/interpreter/spark/spark-interpreter-0.9.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/spark-3.0.3-bin-hadoop2.7/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local
	at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:936)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:238)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)


# Something wrong with configuration


# -----------------------------------------------------
# Stop Zeppelin, Remove Zeppelin folder, and install latest manually

wget https://dlcdn.apache.org/zeppelin/zeppelin-0.10.0/zeppelin-0.10.0-bin-all.tgz
tar -xzvf zeppelin-0.10.0-bin-all.tgz 


# -----------------------------------------------------
# Set Configurations
cd zeppelin-0.10.0-bin-all/


# -----------------------------------------------------
# Create Shiro conf file, and our user: gaiauser
cp conf/shiro.ini.template conf/shiro.ini



# -----------------------------------------------------
# Create zeppelin-site file, and set Zeppelin IP Address
cp conf/zeppelin-site.xml.template conf/zeppelin-site.xml


# -----------------------------------------------------
# Setup Hadoop / Spark settings in Spark Interpreter

 # In Zeppelin UI / Spark Interpreter:

	SPARK_HOME	/opt/spark	 
	spark.master	yarn
	spark.submit.deployMode	client


# ----------------------------------------------------------------
# Start Zeppelin
/home/fedora/zeppelin-0.10.0-bin-all/bin/zeppelin-daemon.sh start


# ------------------------------------------------------------------------------------------------
# Import Notebooks from https://github.com/wfau/aglais-testing/tree/main/notebooks/public_examples



# -----------------------------------------------------------------------------------------------------------------------------------------
# Run notebook https://github.com/wfau/aglais-testing/blob/main/notebooks/public_examples/SetUp.json

Exception...

Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user= , access=WRITE, inode="/":fedora:supergroup:drwxr-xr-x zeppelin


[Failed]



# Temp fix:
# Set hdfs-site.xml settings to (Zeppelin & Master):

<property>
  <name>dfs.permissions.enabled</name>
  <value>false</value>
</property>


# -----------------------------------------------------------------------------------------------------------------------------------------
# Run notebook https://github.com/wfau/aglais-testing/blob/main/notebooks/public_examples/SetUp.json

[Success]


# -----------------------------------------------------------------------------------------------------------------------------------------
# Run notebook https://github.com/wfau/aglais-testing/blob/main/notebooks/public_examples/Source_counts_over_the_sky.json

# Plot up the results
> Took 2 min 31 sec. Last updated by gaiauser at October 11 2021, 11:31:04 AM.

[Success]



# Start again with newer version of Hadoop & Spark

