#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


Target:

  Debug issue with GDMP where job is not starting
  
Result:

  In Progress


# Description of issue (from user's email):

 > Hi Dave,
 > I have been using the platform and it is very impressive. However since yesterday afternoon any runs I start, including the example books stay at 0% and seem to never start.
 > An you help ?
 

#--------------------
# Check Hadoop UI

Hadoop UI:

   Memory Used Total: 232 / 252 Gb

6 Active nodes, all seem healthy



6 Apps with 2 running containers (Zeppelin)



1 App using 61 % of Cluster:

	User: AKyrieleis

	Running Containers: 20
	% of Queue: 61.5

	Active Tasks: 25309	
	Failed Tasks: 486
	
	

# Check again after a few minutes:

       Active Tasks: 25293
       
# Job does seem to be running


# I can't see any tasks that are currently running which the executors would correspond to (?)
 


# ----------------------------
# Check active tasks

Most recent failed tasks submitted at:

   2023/07/09 17:04:01	      
   Duration ~= 5 mins , status: failed


Most recent successful tasks submitted at:

   2023/07/09 16:48:23	
   Duration 63 ms	



#----------------------
# Check Zeppelin 

# Login as that user, check the active notebooks:


The following jobs seem to have been started

 - counter_3 
 
    # Cell: Mean RA proper motion plot  # ABORTED (KeyboardInterrupt)
)
       
 - counter_2 - spark # Completed, not all jobs were run though, some cells are set to Ready. (Maybe user just run the particular cells?)
 
 - counter_1 - spark   # Completed

 - 4. Mean proper motions over the sky - spark4
 
      # Cell: Mean RA proper motion plot # ABORTED
      

# ------------------------------------------------------
# Check which notebooks the active tasks correspond to

# Check the failed jobs for the application
# http://localhost:8088/proxy/application_1680528233262_0031/jobs/

# On the left, note the Job Id & paragraph ID: 
64 (zeppelin|AKyrieleis|2J4MQV1KA|20210510-111939_1386609632)	

# Submitted at 2023/07/09 17:04:01	 
# Duration 5.4 min	
# Stages 0/1 (1 failed)


# Note the ID after the username: 2J7GRM3F1
# Paragraph ID: 20210510-111939_1386609632

# Check notebook in Zeppelin:

  # Notebook: counter_3
  
  
# Notebook has 3 spark cells:

%pyspark
import math

# compute relevant pixelisation quantities
nside = int(math.pow(2, healpix_level))
powers_of_2 = 35 + (12 - healpix_level)*2
divisor = int(math.pow(2, powers_of_2))

# formulate SQL query
query = "SELECT floor(source_id /  %d"%(divisor) + ") AS hpx_id, COUNT(*) AS n, AVG(pmra) AS avg_pmra, AVG(pmdec) AS avg_pmdec FROM gaia_source GROUP BY hpx_id"

# define a data frame aggregation of the relevant quantities (note this is cached for use in two subsequent cells)
df = spark.sql(query).cache()

# Took 0 sec. Last updated by AKyrieleis at July 09 2023, 8:55:00 PM.


%pyspark

# plot up the sky counts
import matplotlib.pyplot as plot
import numpy as np
import healpy as hp

# set a figure to use along with a plot size (landscape, golden ratio)
plot.figure(1, figsize = (16.18, 10.0))

# healpy constants appropriate to the HEALPix indexing encoded in Gaia source IDs
npix = hp.nside2npix(nside)

# do the visualisation
array_data = np.empty(npix)
for item in df.rdd.collect():  array_data[item[0]] = item[2]
hp.mollview(array_data, fig = 1, coord='C', unit='mas/yr', nest=True, title='Mean RA proper motion at HEALPix level %d'%(healpix_level), cmap='rainbow')
hp.graticule(coord='C', color='white')


# Took 6 min 18 sec. Last updated by AKyrieleis at July 10 2023, 10:39:38 AM. (outdated)

# Aborted after 6 mins


%pyspark

plot.figure(2, figsize = (16.18, 10.0))

array_data = np.empty(npix)
for item in df.rdd.collect():  array_data[item[0]] = item[3]
hp.mollview(array_data, fig=2, coord='C', unit='mas/yr', nest=True, title='Mean Dec proper motion at HEALPix level %d'%(healpix_level), cmap='rainbow')
hp.graticule(coord='C', color='white')

# Took 56 sec. Last updated by AKyrieleis at July 09 2023, 8:12:13 PM.


# The paragraph ID corresponds to the second cell (which was aborted)




      
# ----------------------------
# Check Grafana      

# Check resource usage in Grafana:

# Free memory on worker nodes aprox. 500Mb - 3Gb
# Active ~= 30 Gb
# Cached ~= 10 Gb



# ----------------------------------------------------------------
# Check logs of worker05, which has active executors (from Yarn UI)
# fedora@iris-gaia-green-20230308-worker05


tail -f -n 1000 /var/hadoop/logs/application_1680528233262_0031/container_1680528233262_0031_01_000171/stderr 

2023-07-10 07:33:45,058 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-01923-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_01923.c000.snappy.parquet, range: 268435456-307080903, partition values: [empty row]
2023-07-10 07:33:45,118 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-01715-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_01715.c000.snappy.parquet, range: 268435456-307197108, partition values: [empty row]
2023-07-10 07:33:45,230 INFO executor.Executor: Finished task 4578.0 in stage 93.0 (TID 51510). 2843 bytes result sent to driver
2023-07-10 07:33:45,231 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 51610
2023-07-10 07:33:45,231 INFO executor.Executor: Running task 4678.0 in stage 93.0 (TID 51610)
2023-07-10 07:33:45,234 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-01938-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_01938.c000.snappy.parquet, range: 268435456-307055025, partition values: [empty row]
2023-07-10 07:33:45,486 INFO executor.Executor: Finished task 4609.0 in stage 93.0 (TID 51541). 2843 bytes result sent to driver
2023-07-10 07:33:45,487 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 51629
2023-07-10 07:33:45,488 INFO executor.Executor: Running task 4697.0 in stage 93.0 (TID 51629)
2023-07-10 07:33:45,490 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-00291-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_00291.c000.snappy.parquet, range: 268435456-307009886, partition values: [empty row]
2023-07-10 07:33:45,492 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-01993-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_01993.c000.snappy.parquet, range: 268435456-307118592, partition values: [empty row]
2023-07-10 07:33:45,505 INFO executor.Executor: Finished task 4594.0 in stage 93.0 (TID 51526). 2843 bytes result sent to driver
2023-07-10 07:33:45,506 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 51633
2023-07-10 07:33:45,506 INFO executor.Executor: Running task 4701.0 in stage 93.0 (TID 51633)
2023-07-10 07:33:45,509 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-01697-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_01697.c000.snappy.parquet, range: 268435456-307003859, partition values: [empty row]
2023-07-10 07:33:45,686 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-00429-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_00429.c000.snappy.parquet, range: 268435456-307080529, partition values: [empty row]
2023-07-10 07:33:45,795 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-01513-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_01513.c000.snappy.parquet, range: 268435456-307054556, partition values: [empty row]
2023-07-10 07:33:45,838 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-00796-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_00796.c000.snappy.parquet, range: 268435456-307009381, partition values: [empty row]
2023-07-10 07:33:46,032 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-01575-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_01575.c000.snappy.parquet, range: 268435456-307002911, partition values: [empty row]
2023-07-10 07:33:46,069 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-02015-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_02015.c000.snappy.parquet, range: 268435456-307117809, partition values: [empty row]
2023-07-10 07:33:46,263 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-01333-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_01333.c000.snappy.parquet, range: 268435456-307079901, partition values: [empty row]
2023-07-10 07:33:46,346 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-00532-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_00532.c000.snappy.parquet, range: 268435456-307002736, partition values: [empty row]
2023-07-10 07:33:46,370 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-01373-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_01373.c000.snappy.parquet, range: 268435456-307009070, partition values: [empty row]
2023-07-10 07:33:46,421 INFO executor.Executor: Finished task 4645.0 in stage 93.0 (TID 51577). 2843 bytes result sent to driver
2023-07-10 07:33:46,422 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 51709
2023-07-10 07:33:46,422 INFO executor.Executor: Running task 4777.0 in stage 93.0 (TID 51709)
2023-07-10 07:33:46,425 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-01470-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_01470.c000.snappy.parquet, range: 268435456-306449219, partition values: [empty row]
2023-07-10 07:33:46,500 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-00788-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_00788.c000.snappy.parquet, range: 268435456-307053735, partition values: [empty row]
2023-07-10 07:33:46,638 INFO executor.Executor: Finished task 4697.0 in stage 93.0 (TID 51629). 2843 bytes result sent to driver
2023-07-10 07:33:46,686 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-00967-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_00967.c000.snappy.parquet, range: 268435456-306433934, partition values: [empty row]
2023-07-10 07:33:46,850 INFO executor.Executor: Finished task 4666.0 in stage 93.0 (TID 51598). 2843 bytes result sent to driver
2023-07-10 07:33:46,893 INFO executor.Executor: Finished task 4678.0 in stage 93.0 (TID 51610). 2843 bytes result sent to driver
2023-07-10 07:33:47,021 INFO executor.Executor: Finished task 4701.0 in stage 93.0 (TID 51633). 2843 bytes result sent to driver
2023-07-10 07:33:47,049 INFO datasources.FileScanRDD: Reading File path: file:///data/gaia/GDR3/GDR3_GAIASOURCE/part-01111-18ac60bc-42ce-42dd-ab5b-fc0027a56a2c_01111.c000.snappy.parquet, range: 268435456-306422103, partition values: [empty row]
2023-07-10 07:33:47,206 INFO executor.Executor: Finished task 4777.0 in stage 93.0 (TID 51709). 2843 bytes result sent to driver
2023-07-10 07:39:43,357 INFO storage.BlockManager: Removing RDD 147
2023-07-10 08:08:13,918 INFO storage.BlockManager: Removing RDD 147

# ------------------------------------------------------------
# fedora@iris-gaia-green-20230308-master01
# Check master01 logs:

tail -f -n 1000 /var/hadoop/logs/hadoop-fedora-resourcemanager-iris-gaia-green-20230308-master01.log

2023-07-10 10:21:32,561 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Reserved container=container_1680528233262_0031_01_000167, on node=host: worker01:34295 #containers=5 available=<memory:7168, vCores:21> used=<memory:35840, vCores:5> with resource=<memory:8192, vCores:1>
2023-07-10 10:21:32,562 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-07-10 10:21:32,778 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Reserved container=container_1680528233262_0031_01_000166, on node=host: worker02:35137 #containers=5 available=<memory:7168, vCores:21> used=<memory:35840, vCores:5> with resource=<memory:8192, vCores:1>
2023-07-10 10:21:32,778 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-07-10 10:21:33,562 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Reserved container=container_1680528233262_0031_01_000167, on node=host: worker01:34295 #containers=5 available=<memory:7168, vCores:21> used=<memory:35840, vCores:5> with resource=<memory:8192, vCores:1>
2023-07-10 10:21:33,562 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-07-10 10:21:33,779 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Reserved container=container_1680528233262_0031_01_000166, on node=host: worker02:35137 #containers=5 available=<memory:7168, vCores:21> used=<memory:35840, vCores:5> with resource=<memory:8192, vCores:1>
2023-07-10 10:21:33,779 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted


# --------------------------------
# Restart Zeppelin
# fedora@iris-gaia-green-20230308-zeppelin
 
zeppelin-daemon.sh restart
   > Zeppelin stop                                              [  OK  ]
   > Zeppelin start                                             [  OK  ]


# Run notebooks
#  Setup & Source counts [Success]
