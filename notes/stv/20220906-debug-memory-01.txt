#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#

# -----------------------------------------------
# Run new deploy with same configuration as live
# zeppelin-26.43-spark-6.26.43


# -----------------------------------------------------
# Create test users.
#[root@ansibler]

 source /deployments/zeppelin/bin/create-user-tools.sh

 import-test-users

# -------------------------------------------------------------------------------
# Login and import the example notebook that causes the errors mentioned by Nigel

# Notebook: Work in progress with Gaia XP spectra (Copied from Live deploy)

# Errors: Notebook gets failed tasks, and eventually fails for the full dataset
# If selecting a subset (10%) the notebook completes, but with failed tasks
# [root@ansibler]




# -------------------------------------------------------------------------------
# Manually Edit notebook, modify cell that selects a subset
# Remove Mod so that we get the full dataset

%pyspark

# select a template for the search, e.g. Proxima Cen
#sid = 5853498713190525696 # Proxima Cen
sid = 2585856120791459584 # HIP 7585 (Solar twin)
template_df = spark.sql('SELECT * FROM xp_continuous_mean_spectrum WHERE source_id = %d'%(sid))
#spark.sql('SELECT xp.*, g.parallax FROM xp_sampled_mean_spectrum AS xp INNER JOIN gaia_source AS g ON g.source_id = xp.source_id WHERE g.source_id = %d'%(sid))

# grab the template parallax
#template_parallax = template_df.collect()[0]['parallax']

# define a query over the entire dataset, restricting to low reddening for simplicity
query = 'SELECT xp.*  ' + \
        'FROM xp_continuous_mean_spectrum AS xp INNER JOIN gaia_source AS g ON g.source_id = xp.source_id ' + \
        'WHERE g.ag_gspphot < 0.1'
        
# -----------------------------------------------
# Run Cell That takes a while to complete / Fails


%pyspark
#similar_df.count()

# collect/cash stats to the head for plotting in the next cell - this actually actions the full workflow on the Spark cluster
stats_pdf = similar_df.select('p_xp_similar').toPandas()
# benchmarks: 1 in 10000 in 45min 21sec; no failed tasks
#             1 in 1000  in 45min 21sec; ditto
#             1 in 10    in 46min 57sec; successful completion albeit with 10 failed tasks 


Took 20 mins

# Notebook output:

/opt/spark/python/pyspark/sql/pandas/conversion.py:137: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.
  An error occurred while calling o255.getResult.
: org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
	at org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:97)
	at org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:93)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 99 in stage 7.0 failed 4 times, most recent failure: Lost task 99.3 in stage 7.0 (TID 8779) (worker03 executor 140): ExecutorLostFailure (executor 140 exited caused by one of the running tasks) Reason: Executor Process Lost
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3629)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3633)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3610)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3610)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3609)
	at org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:139)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:141)
	at org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:136)
	at org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:113)
	at org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:107)
	at org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:68)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:68)


Py4JJavaError: An error occurred while calling o255.getResult.
: org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
	at org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:97)
	at org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:93)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 99 in stage 7.0 failed 4 times, most recent failure: Lost task 99.3 in stage 7.0 (TID 8779) (worker03 executor 140): ExecutorLostFailure (executor 140 exited caused by one of the running tasks) Reason: Executor Process Lost
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3629)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3633)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3610)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3610)
	at org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3609)
	at org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:139)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:141)
	at org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:136)
	at org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:113)
	at org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:107)
	at org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:68)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:68)

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o255.getResult.\n', JavaObject id=o256), <traceback object at 0x7f89040ab2d0>)


# -----------------------------------------------------------------------------------------------------
# Check Worker02 logs
# fedora@worker02

tail -f -n 10000 /var/hadoop/logs/hadoop-fedora-nodemanager-iris-gaia-red-20220826-worker02.log

..
2022-08-29 15:06:22,791 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1661525926408_0002_02_000015 is : 143
..

# -----------------------------------------------------------------------------------------------------
# Check application logs

grep -r "Exception" /var/hadoop/logs/application_1661525926408_0002

/var/hadoop/logs/application_1661525926408_0002/container_1661525926408_0002_01_000052/stderr:java.lang.IllegalStateException: Memory was leaked by query. Memory leaked: (65536)
/var/hadoop/logs/application_1661525926408_0002/container_1661525926408_0002_01_000188/stderr:org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
/var/hadoop/logs/application_1661525926408_0002/container_1661525926408_0002_01_000188/stderr:	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
/var/hadoop/logs/application_1661525926408_0002/container_1661525926408_0002_01_000188/stderr:	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)
/var/hadoop/logs/application_1661525926408_0002/container_1661525926408_0002_01_000188/stderr:Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]


..

# -----------------------------------------------------------------------------------------------------
# Check  /var/hadoop/logs/application_1661525926408_0002/container_1661525926408_0002_01_000052/stderr

2022-08-29 13:41:46,756 WARN python.ArrowPythonRunner: Incomplete task 80.0 in stage 3 (TID 4177) interrupted: Attempting to kill Python Worker
2022-08-29 13:41:46,756 WARN python.ArrowPythonRunner: Incomplete task 84.0 in stage 3 (TID 4181) interrupted: Attempting to kill Python Worker
2022-08-29 13:41:46,800 WARN python.ArrowPythonRunner: Incomplete task 81.0 in stage 3 (TID 4178) interrupted: Attempting to kill Python Worker
2022-08-29 13:41:46,803 ERROR memory.BaseAllocator: Memory was leaked by query. Memory leaked: (65536)
Allocator(stdin reader for python) 0/65536/133120/9223372036854775807 (res/actual/peak/limit)

2022-08-29 13:41:46,804 ERROR spark.TaskContextImpl: Error in TaskCompletionListener
java.lang.IllegalStateException: Memory was leaked by query. Memory leaked: (65536)
Allocator(stdin reader for python) 0/65536/133120/9223372036854775807 (res/actual/peak/limit)

	at org.apache.arrow.memory.BaseAllocator.close(BaseAllocator.java:431)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.$anonfun$new$1(PythonArrowOutput.scala:63)
	at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.$anonfun$new$1$adapted(PythonArrowOutput.scala:59)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)
	at org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)
	at org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)
	at org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)



# ------------------------------
# Some interesting/related Links

https://www.slideshare.net/SparkSummit/top-5-mistakes-when-writing-spark-applications-63071421
https://sparkbyexamples.com/spark/difference-between-spark-sql-shuffle-partitions-and-spark-default-parallelism/
https://community.cloudera.com/t5/Support-Questions/Diagnostics-Container-killed-on-request-Exit-code-is-143/td-p/240929
https://stackoverflow.com/questions/41057311/the-value-of-spark-yarn-executor-memoryoverhead-setting
https://spark.apache.org/docs/latest/sql-performance-tuning.html
https://stackoverflow.com/questions/32349611/what-should-be-the-optimal-value-for-spark-sql-shuffle-partitions-or-how-do-we-i
https://zeppelin.apache.org/docs/0.10.0/interpreter/spark.html
https://stackoverflow.com/questions/42972908/container-killed-by-the-applicationmaster-exit-code-is-143

# Some responses from stackoverflow on the issues:

"""
Exit code 143 is related to Memory/GC issues. Your default Mapper/reducer memory setting may not be sufficient to run the large data set. Thus, try setting up higher AM, MAP and REDUCER memory when a large yarn job is invoked.
"""

"""
From the answer here, spark.sql.shuffle.partitions configures the number of partitions that are used when shuffling data for joins or aggregations.

spark.default.parallelism is the default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set explicitly by the user. Note that spark.default.parallelism seems to only be working for raw RDD and is ignored when working with dataframes.
"""

"""
spark.default.parallelism is the default number of partition set by spark which is by default 200. and if you want to increase the number of partition than you can apply the property spark.sql.shuffle.partitions to set number of partition in the spark configuration or while running spark SQL.
"""

"""
also If number of partitions is near to 2000 then increase it to more than 2000. As spark applies different logic for partition < 2000 and > 2000 which will increase your code performance by decreasing the memory footprint as data default is highly compressed if >2000.
"""



# -----------------------------------------
# Try modifying the spark.sql.shuffle.partitions

spark.sql.shuffle.partitions  2001

# Run same cells
# Failed, same errors


# -----------------------------------------
# Try modifying the memory per executor
# fedora@zeppelin

# Set spark.executor.memory                7168m ->	5120m

# Cell completes successfully
..

%pyspark
#similar_df.count()

# collect/cash stats to the head for plotting in the next cell - this actually actions the full workflow on the Spark cluster
stats_pdf = similar_df.select('p_xp_similar').toPandas()

> Took 8 hrs 4 min 33 sec. Last updated by Evison at August 31 2022, 6:10:22 AM.


# ------------------------------------------------------------
# Repeat Test, to check if we can rerun the cell consistently

%pyspark
#similar_df.count()

# collect/cash stats to the head for plotting in the next cell - this actually actions the full workflow on the Spark cluster
stats_pdf = similar_df.select('p_xp_similar').toPandas()

> Took 8 hrs 31 min 4 sec. Last updated by Evison at September 06 2022, 4:42:57 AM.


# -------------------------
# Run the next few cells..

# The following cell failed after 37 mins

%pyspark

# convert to a Pandas dataframe for GaiaXPy
continuous_spectra = similar_df.toPandas()

# convert to sampled form:
sampled_spectra, sampling = convert(continuous_spectra, save_file = False)
    
# plot to sanity check:
plot_spectra(sampled_spectra, sampling = sampling, multi=True, show_plot=True, output_path=None, legend=True)
Py4JJavaError: An error occurred while calling o280.getResult.
: org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
	at org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:97)
	at org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:93)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 8.0 failed 4 times, most recent failure: Lost task 3.3 in stage 8.0 (TID 12934) (worker01 executor 212): ExecutorLostFailure (executor 212 exited caused by one of the running tasks) Reason: Container from a bad node: container_1661872020086_0003_02_000047 on host: worker01. Exit status: 143. Diagnostics: [2022-09-06 09:26:08.468]Container killed on request. Exit code is 143
[2022-09-06 09:26:08.468]Container exited with a non-zero exit code 143. 
[2022-09-06 09:26:08.469]Killed by external signal

# Two error codes / types of messages in the Spark UI

ExecutorLostFailure (executor 180 exited caused by one of the running tasks) Reason: Container from a bad node: container_1661872020086_0003_02_000008 on host: worker01. Exit status: 143. Diagnostics: [2022-09-06 09:22:07.897]Container killed on request. Exit code is 143

ExecutorLostFailure (executor 133 exited caused by one of the running tasks) Reason: Container from a bad node: container_1661872020086_0003_01_000165 on host: worker03. Exit status: 137. Diagnostics: [2022-09-06 08:49:39.446]Container killed on request. Exit code is 137





