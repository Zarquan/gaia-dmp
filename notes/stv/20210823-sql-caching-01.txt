#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#


    Target:

        Test & Document Caching

    Result:

        Existing ML Notebook runs very slow the second time it is run
        Spark is unable to create a cache the second time, and the assumption is that it is thus slow because it reads from disk for each cell
        Changing to SQL caching (spark.sql("CACHE TABLE raw_sources")) fixes this issue, and repeated runs create and delete cache consistently, and take the expected amount of time to complete



# This test was run on the  the deploy described here

https://github.com/stvoutsin/aglais/blob/ed3c47cb753bf2c67b5dd5088789fa550ab9a20c/notes/stv/20210822-validation-multi-user-01.txt


# Restart Spark Interpreter for all users

# -------------------------------------
# admin@firefox
# Run ML Notebook


        /AglaisPublicExamples/SetUp

        [Success]



        /AglaisPublicExamples/Good astrometric solutions via ML Random Forrest classifier

        # Raw catalogue with selected columns
        > Took 6 min 6 sec. Last updated by admin at August 23 2021, 3:41:15 PM.


	# Cached RDDs (Yarn UI - Storage):

         #  Query (*) 277.4 MB
         #  MapPartitionsRDD 1298.3 MB	

(*) RDD Storage Info for *(1) Project [source_id#40L, random_index#41L, phot_g_mean_mag#107, phot_bp_rp_excess_factor#123, bp_rp#124, g_rp#126, parallax#47, ra#43, dec#45, b#134, ((1.15436 + (0.033772 * cast(bp_rp#124 as double))) + ((0.032277 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) AS fgbp_grp_0p5#33, (((1.162004 + (0.011464 * cast(bp_rp#124 as double))) + ((0.049255 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) - (((0.005879 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) AS fgbp_grp_0p5_4p0#34, (1.057572 + (0.0140537 * cast(bp_rp#124 as double))) AS fgbp_grp_4p0#35, (0.0059898 + (8.817481E-12 * POWER(cast(phot_g_mean_mag#107 as double), 7.618399))) AS sig_cstarg#36, parallax_error#48, abs(parallax_over_error#49) AS parallax_over_error#37, astrometric_sigma5d_max#85, pmra_error#52, pmdec_error#54, astrometric_excess_noise#71, ipd_gof_harmonic_amplitude#89, ruwe#93, visibility_periods_used#84, pmdec#53, ... 6 more fields] +- *(1) Filter (isnotnull(parallax#47...



	# RAM Usage

	  # worker01: 64%
	  # worker02: 90%
	  # worker03: 87%
	  # worker04: 89%


	

        # Train up the Random Forrest
        > Took 4 min 44 sec. Last updated by admin at August 23 2021, 3:46:09 PM.

        [Success]


	# Wait > 60 Seconds (Idle timeout) and check Storage & RAM usage



	# Cached RDDs (Yarn UI - Storage):

         #  Query_1 (*) 277.4 MB	
         #  Query_2 (*) 600.6 MB	

(*) Query_1: RDD Storage Info for *(1) Project [source_id#40L, random_index#41L, phot_g_mean_mag#107, phot_bp_rp_excess_factor#123, bp_rp#124, g_rp#126, parallax#47, ra#43, dec#45, b#134, ((1.15436 + (0.033772 * cast(bp_rp#124 as double))) + ((0.032277 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) AS fgbp_grp_0p5#33, (((1.162004 + (0.011464 * cast(bp_rp#124 as double))) + ((0.049255 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) - (((0.005879 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) AS fgbp_grp_0p5_4p0#34, (1.057572 + (0.0140537 * cast(bp_rp#124 as double))) AS fgbp_grp_4p0#35, (0.0059898 + (8.817481E-12 * POWER(cast(phot_g_mean_mag#107 as double), 7.618399))) AS sig_cstarg#36, parallax_error#48, abs(parallax_over_error#49) AS parallax_over_error#37, astrometric_sigma5d_max#85, pmra_error#52, pmdec_error#54, astrometric_excess_noise#71, ipd_gof_harmonic_amplitude#89, ruwe#93, visibility_periods_used#84, pmdec#53, ... 6 more fields] +- *(1) Filter (isnotnull(parallax#47...

(*) Query_2: RDD Storage Info for *(1) Project [source_id#40L, random_index#41L, phot_g_mean_mag#107, phot_bp_rp_excess_factor#123, bp_rp#124, g_rp#126, parallax#47, ra#43, dec#45, b#134, fgbp_grp_0p5#33, fgbp_grp_0p5_4p0#34, fgbp_grp_4p0#35, sig_cstarg#36, parallax_error#48, parallax_over_error#37, astrometric_sigma5d_max#85, pmra_error#52, pmdec_error#54, astrometric_excess_noise#71, ipd_gof_harmonic_amplitude#89, ruwe#93, visibility_periods_used#84, pmdec#53, ... 10 more fields] +- *(1) Project [source_id#40L, random_index#41L, phot_g_mean_mag#107, phot_bp_rp_excess_factor#123, bp_rp#124, g_rp#126, parallax#47, ra#43, dec#45, b#134, fgbp_grp_0p5#33, fgbp_grp_0p5_4p0#34, fgbp_grp_4p0#35, sig_cstarg#36, parallax_error#48, parallax_over_error#37, astrometric_sigma5d_max#85, pmra_error#52, pmdec_error#54, astrometric_excess_noise#71, ipd_gof_harmonic_amplitude#89, ruwe#93, visibility_periods_used#84, pmdec#53, ... 7 more fields] +- *(1) Filter (isnotnull(parallax#47) && (parallax#47 > 8.0)) +- *(1) InMemoryTableScan [source_id#40L,...



	# RAM Usage

	  # worker01: 19%
	  # worker02: 19%
	  # worker03: 20%
	  # worker04: 42%



# -------------------------------------
# admin@firefox
# Run ML Notebook again
# No Spark Interpreter restarts


        /AglaisPublicExamples/Good astrometric solutions via ML Random Forrest classifier

        # Raw catalogue with selected columns
        > Took 40 sec. Last updated by gaiauser at August 23 2021, 2:42:25 PM.

	# All notebooks running slow (As observed consistently after repeated runs of the ML notebook)

	# Cached RDDs (Yarn UI - Storage):

         # Empty page, no storage showing up


	# RAM Usage

	  # worker01: 40%
	  # worker02: 45%
	  # worker03: 47%
	  # worker04: 55%

	# Restart Spark Intepreter (Don't wait for the rest of the cells would be expected to take > 1 hour)



# -------------------------------------
# admin@firefox
# Run ML Notebook again with the change to caching
# No Spark Interpreter restarts


# Change "Raw catalogue with selected columns" to:

----

%spark.pyspark

# clear any previously cached data in the context (cells may be executed in any order, and out-dated by changes from here onwards)
sqlContext.clearCache()

# a conservative selection of everything that COULD be within 100pc, including things with measured 
# distances putting them outside the 100pc horizon when their true distances are within, and also including 
# loads of spurious chaff with the wheat of course, plus bad things with significant, unphysical parallaxes:
raw_sources_df = spark.sql('SELECT source_id, random_index, phot_g_mean_mag, phot_bp_rp_excess_factor, bp_rp, g_rp, parallax, ra, dec, b, ' + photometric_consistency_indicators + features_select_string + ' FROM gaia_source WHERE ABS(parallax) > 8.0')

# cache it for speedy access below (all subsequent samples are derived from this):

# ... some good advice concerning caching in Spark here: https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34

# register as SQL-queryable
raw_sources_df.createOrReplaceTempView('raw_sources')

raw_sources_df.count()
spark.sql("CACHE TABLE raw_sources")

-----


        /AglaisPublicExamples/Good astrometric 	solutions via ML Random Forrest classifier


        # Raw catalogue with selected columns
        > Took 6 min 29 sec. Last updated by gaiauser at August 23 2021, 2:57:38 PM.



	# Cached RDDs (Yarn UI - Storage):

        In-memory table `raw_sources`	277.4 MB
        MapPartitionsRDD	1298.3 MB	


	# RAM Usage

	  # worker01: 83%
	  # worker02: 68%
	  # worker03: 84%
	  # worker04: 85%



        # Train up the Random Forrest
        > Took 4 min 29 sec. Last updated by gaiauser at August 23 2021, 3:02:18 PM.

        [Success]




	# Wait > 60 Seconds (Idle timeout) and check Storage & RAM usage



	# Cached RDDs (Yarn UI - Storage):

         #  In-memory table `raw_sources`  277.4 MB
         #  Query_1 (*) 600.6 MB

(*) Query_1: 

*(1) Project [source_id#45L, random_index#46L, phot_g_mean_mag#112, phot_bp_rp_excess_factor#128, bp_rp#129, g_rp#131, parallax#52, ra#48, dec#50, b#139, fgbp_grp_0p5#38, fgbp_grp_0p5_4p0#39, fgbp_grp_4p0#40, sig_cstarg#41, parallax_error#53, parallax_over_error#42, astrometric_sigma5d_max#90, pmra_error#57, pmdec_error#59, astrometric_excess_noise#76, ipd_gof_harmonic_amplitude#94, ruwe#98, visibility_periods_used#89, pmdec#58, ... 10 more fields] +- *(1) Project [source_id#45L, random_index#46L, phot_g_mean_mag#112, phot_bp_rp_excess_factor#128, bp_rp#129, g_rp#131, parallax#52, ra#48, dec#50, b#139, fgbp_grp_0p5#38, fgbp_grp_0p5_4p0#39, fgbp_grp_4p0#40, sig_cstarg#41, parallax_error#53, parallax_over_error#42, astrometric_sigma5d_max#90, pmra_error#57, pmdec_error#59, astrometric_excess_noise#76, ipd_gof_harmonic_amplitude#94, ruwe#98, visibility_periods_used#89, pmdec#58, ... 7 more fields] +- *(1) Filter (isnotnull(parallax#52) && (parallax#52 > 8.0)) +- *(1) InMemoryTableScan [source_id#45L,...



	# RAM Usage

	  # worker01: 20%
	  # worker02: 22%
	  # worker03: 20%
	  # worker04: 43%




# -------------------------------------
# admin@firefox
# Run ML Notebook again
# No Spark Interpreter restart


        /AglaisPublicExamples/Good astrometric solutions via ML Random Forrest classifier

        # Raw catalogue with selected columns
        > Took 7 min 45 sec. Last updated by gaiauser at August 23 2021, 3:20:34 PM.


	# Cached RDDs (Yarn UI - Storage):

        In-memory table `raw_sources`	277.4 MB
        MapPartitionsRDD	1298.3 MB	


	# RAM Usage

	  # worker01: 87%
	  # worker02: 70%
	  # worker03: 88%
	  # worker04: 90%


        # Train up the Random Forrest
        > Took 4 min 34 sec. Last updated by gaiauser at August 23 2021, 3:25:17 PM.


        # Wait 60 seconds..


	# Cached RDDs (Yarn UI - Storage):

         #  In-memory table `raw_sources`  277.4 MB
         #  Query_1 (*) 600.6 MB

(*) Query_1: 

*(1) Project [source_id#45L, random_index#46L, phot_g_mean_mag#112, phot_bp_rp_excess_factor#128, bp_rp#129, g_rp#131, parallax#52, ra#48, dec#50, b#139, fgbp_grp_0p5#38, fgbp_grp_0p5_4p0#39, fgbp_grp_4p0#40, sig_cstarg#41, parallax_error#53, parallax_over_error#42, astrometric_sigma5d_max#90, pmra_error#57, pmdec_error#59, astrometric_excess_noise#76, ipd_gof_harmonic_amplitude#94, ruwe#98, visibility_periods_used#89, pmdec#58, ... 10 more fields] +- *(1) Project [source_id#45L, random_index#46L, phot_g_mean_mag#112, phot_bp_rp_excess_factor#128, bp_rp#129, g_rp#131, parallax#52, ra#48, dec#50, b#139, fgbp_grp_0p5#38, fgbp_grp_0p5_4p0#39, fgbp_grp_4p0#40, sig_cstarg#41, parallax_error#53, parallax_over_error#42, astrometric_sigma5d_max#90, pmra_error#57, pmdec_error#59, astrometric_excess_noise#76, ipd_gof_harmonic_amplitude#94, ruwe#98, visibility_periods_used#89, pmdec#58, ... 7 more fields] +- *(1) Filter (isnotnull(parallax#52) && (parallax#52 > 8.0)) +- *(1) InMemoryTableScan [source_id#45L,...


	# RAM Usage

	  # worker01: 20%
	  # worker02: 22%
	  # worker03: 20%
	  # worker04: 44%


    [Success]
    

    # As a sanity check, run a single cell with a cache clear:

	%spark.pyspark
	sqlContext.clearCache()



	# Cached RDDs (Yarn UI - Storage):

         # Empty page, no storage


	# RAM Usage

	  # worker01: 20%
	  # worker02: 22%
	  # worker03: 20%
	  # worker04: 44%


