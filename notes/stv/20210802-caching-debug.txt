#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#




    Target:

        Test Dynamic Allocation with config for clearing Cache after idle timeouts


    Result:

        Good astrometric solutions via ML Random Forrest classifier 
          # Single user run       - SUCCESS
          # Second user single run       - SUCCESS

     
#-------------------------------------------------------
# Changes to config

spark.dynamicAllocation.cachedExecutorIdleTimeout    60s	
spark.dynamicAllocation.executorIdleTimeout	60s


# user1 : gaiauser
# user2 : admin


#-------------------------------------------------------
# Run Single user run of ML notebook
  # Run the /AglaisPublicExamples/SetUp notebook (Latest version from Github)
     # https://raw.githubusercontent.com/wfau/aglais-notebooks/main/2G7GZKWUH/note.json
  # Run the /AglaisPublicExamples/Good astrometric solutions via ML Random Forrest classifier
     # https://raw.githubusercontent.com/wfau/aglais-notebooks/main/2G5NU6HTK/note.json
  

# admin@firefox


	/AglaisPublicExamples/Set up

        [Success]

        # Yarn details:
        # 1 applcation
        # 47% Usage



	/AglaisPublicExamples/Good astrometric solutions via ML Random Forrest classifier

	# Raw catalogue with selected columns
	> Took 6 min 21 sec. Last updated by admin at August 02 2021, 12:44:05 AM.



        # Yarn details:
        # 1 applcation
        # 95% Usage
 
        # Spark details:
        # Storage:
        # Two entries, both only in memory
        # 277.4 MB	-   MapPartitionsRDD	
        # 1298.3 MB	-   *(1) Project [source_id#40L, random_index#41L, phot_g_mean_mag#107, phot_bp_rp_excess_factor#123, bp_rp#124, g_rp#126, parallax#47, ra#43, dec#45, b#134, ((1.15436 + (0.033772 * cast(bp_rp#124 as double))) + ((0.032277 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) AS fgbp_grp_0p5#33, (((1.162004 + (0.011464 * cast(bp_rp#124 as double))) + ((0.049255 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) - (((0.005879 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) AS fgbp_grp_0p5_4p0#34, (1.057572 + (0.0140537 * cast(bp_rp#124 as double))) AS fgbp_grp_4p0#35, (0.0059898 + (8.817481E-12 * POWER(cast(phot_g_mean_mag#107 as double), 7.618399))) AS sig_cstarg#36, parallax_error#48, abs(parallax_over_error#49) AS parallax_over_error#37, astrometric_sigma5d_max#85, pmra_error#52, pmdec_error#54, astrometric_excess_noise#71, ipd_gof_harmonic_amplitude#89, ruwe#93, visibility_periods_used#84, pmdec#53, ... 6 more fields] +- *(1) Filter (isnotnull(parallax#47...



	# Train up the Random Forrest
	> Took 3 min 50 sec. Last updated by admin at August 02 2021, 12:48:04 AM.



        # Wait 60 seconds (cache timeout) and check Yarn & Spark UI


        # Yarn details:
        # 1 applcation
        # 47% Usage
 
        # Spark details:
        # Storage:
        # Two entries, both only in memory
        # 277.4 MB 	-     *(1) Project [source_id#40L, random_index#41L, phot_g_mean_mag#107, phot_bp_rp_excess_factor#123, bp_rp#124, g_rp#126, parallax#47, ra#43, dec#45, b#134, ((1.15436 + (0.033772 * cast(bp_rp#124 as double))) + ((0.032277 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) AS fgbp_grp_0p5#33, (((1.162004 + (0.011464 * cast(bp_rp#124 as double))) + ((0.049255 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) - (((0.005879 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) AS fgbp_grp_0p5_4p0#34, (1.057572 + (0.0140537 * cast(bp_rp#124 as double))) AS fgbp_grp_4p0#35, (0.0059898 + (8.817481E-12 * POWER(cast(phot_g_mean_mag#107 as double), 7.618399))) AS sig_cstarg#36, parallax_error#48, abs(parallax_over_error#49) AS parallax_over_error#37, astrometric_sigma5d_max#85, pmra_error#52, pmdec_error#54, astrometric_excess_noise#71, ipd_gof_harmonic_amplitude#89, ruwe#93, visibility_periods_used#84, pmdec#53, ... 6 more fields] +- *(1) Filter (isnotnull(parallax#47...	
       
        # 600.6 MB	- *(1) Project [source_id#40L, random_index#41L, phot_g_mean_mag#107, phot_bp_rp_excess_factor#123, bp_rp#124, g_rp#126, parallax#47, ra#43, dec#45, b#134, fgbp_grp_0p5#33, fgbp_grp_0p5_4p0#34, fgbp_grp_4p0#35, sig_cstarg#36, parallax_error#48, parallax_over_error#37, astrometric_sigma5d_max#85, pmra_error#52, pmdec_error#54, astrometric_excess_noise#71, ipd_gof_harmonic_amplitude#89, ruwe#93, visibility_periods_used#84, pmdec#53, ... 10 more fields] +- *(1) Project [source_id#40L, random_index#41L, phot_g_mean_mag#107, phot_bp_rp_excess_factor#123, bp_rp#124, g_rp#126, parallax#47, ra#43, dec#45, b#134, fgbp_grp_0p5#33, fgbp_grp_0p5_4p0#34, fgbp_grp_4p0#35, sig_cstarg#36, parallax_error#48, parallax_over_error#37, astrometric_sigma5d_max#85, pmra_error#52, pmdec_error#54, astrometric_excess_noise#71, ipd_gof_harmonic_amplitude#89, ruwe#93, visibility_periods_used#84, pmdec#53, ... 7 more fields] +- *(1) Filter (isnotnull(parallax#47) && (parallax#47 > 8.0)) +- *(1) InMemoryTableScan [source_id#40L,...

	[Success]


        # It looks like the cache was cleared & the resources were indeed released from the application as expected


#-------------------------------------------------------
# Run Single user run of ML notebook as the second user
# We have not cleared the spark context or killed any applications
# First user's application is using 47%

# gaiauser@firefox


	/AglaisPublicExamples/Set up

        [Success]

        # Yarn details:
        # 2 applications
        # 47% Usage (application 1)
        # 47% Usage (application 2)
 


	/AglaisPublicExamples/Good astrometric solutions via ML Random Forrest classifier

	# Raw catalogue with selected columns
	> Took 8 min 5 sec. Last updated by gaiauser at August 02 2021, 1:02:44 AM.



	# Train up the Random Forrest
	> Took 4 min 51 sec. Last updated by gaiauser at August 02 2021, 1:07:45 AM.

        # Wait 60 seconds (cache timeout) and check Yarn & Spark UI


        # Yarn details:
        # 2 applications
        # 47% Usage (application 1)
        # 47% Usage (application 2)
 

        # Spark details:
        # Storage (Same for both applications):
        # Two entries, both only in memory
        # 277.4 MB 	-     *(1) Project [source_id#40L, random_index#41L, phot_g_mean_mag#107, phot_bp_rp_excess_factor#123, bp_rp#124, g_rp#126, parallax#47, ra#43, dec#45, b#134, ((1.15436 + (0.033772 * cast(bp_rp#124 as double))) + ((0.032277 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) AS fgbp_grp_0p5#33, (((1.162004 + (0.011464 * cast(bp_rp#124 as double))) + ((0.049255 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) - (((0.005879 * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double)) * cast(bp_rp#124 as double))) AS fgbp_grp_0p5_4p0#34, (1.057572 + (0.0140537 * cast(bp_rp#124 as double))) AS fgbp_grp_4p0#35, (0.0059898 + (8.817481E-12 * POWER(cast(phot_g_mean_mag#107 as double), 7.618399))) AS sig_cstarg#36, parallax_error#48, abs(parallax_over_error#49) AS parallax_over_error#37, astrometric_sigma5d_max#85, pmra_error#52, pmdec_error#54, astrometric_excess_noise#71, ipd_gof_harmonic_amplitude#89, ruwe#93, visibility_periods_used#84, pmdec#53, ... 6 more fields] +- *(1) Filter (isnotnull(parallax#47...	
       
        # 600.6 MB	- *(1) Project [source_id#40L, random_index#41L, phot_g_mean_mag#107, phot_bp_rp_excess_factor#123, bp_rp#124, g_rp#126, parallax#47, ra#43, dec#45, b#134, fgbp_grp_0p5#33, fgbp_grp_0p5_4p0#34, fgbp_grp_4p0#35, sig_cstarg#36, parallax_error#48, parallax_over_error#37, astrometric_sigma5d_max#85, pmra_error#52, pmdec_error#54, astrometric_excess_noise#71, ipd_gof_harmonic_amplitude#89, ruwe#93, visibility_periods_used#84, pmdec#53, ... 10 more fields] +- *(1) Project [source_id#40L, random_index#41L, phot_g_mean_mag#107, phot_bp_rp_excess_factor#123, bp_rp#124, g_rp#126, parallax#47, ra#43, dec#45, b#134, fgbp_grp_0p5#33, fgbp_grp_0p5_4p0#34, fgbp_grp_4p0#35, sig_cstarg#36, parallax_error#48, parallax_over_error#37, astrometric_sigma5d_max#85, pmra_error#52, pmdec_error#54, astrometric_excess_noise#71, ipd_gof_harmonic_amplitude#89, ruwe#93, visibility_periods_used#84, pmdec#53, ... 7 more fields] +- *(1) Filter (isnotnull(parallax#47) && (parallax#47 > 8.0)) +- *(1) InMemoryTableScan [source_id#40L,...

	[Success]


	# Runs a bit slower than previous, but makes sense with only 47% of resources compared to 95%





#-------------------------------------------------------
# Run Single user run of ML notebook as the first user again
# We have not cleared the spark context or killed any applications
# Currently in Yarn we see two applications, with 47% each
# admin@firefox


	/AglaisPublicExamples/Good astrometric solutions via ML Random Forrest classifier

	# Raw catalogue with selected columns
	> Took 39 sec. Last updated by admin at August 02 2021, 1:12:54 AM.


        # Visualisation (colour / absolute-magnitue diagram) of the raw catalogue
	> Took 3 min 5 sec. Last updated by admin at August 02 2021, 1:17:20 AM.

	# 3 minutes longer than what this cell takes the first time it is run (4 seconds)

	# All cells running much slower

	# While running check monitor node:
        # Two worker nodes using 91% RAM
        # Zeppelin node at 84% 

	# Train up the Random Forrest
	> Took 57 min 48 sec. Last updated by gaiauser at August 02 2021, 2:29:19 AM.
	
	

        # Yarn details:
        # 2 applications
        # 47% Usage (application 1)
        # 47% Usage (application 2)
 


	# Spark UI, not storage shown in application 1 now
        # A guess of what seems to be happening:
        # Once the cell is run once, there is a cache and temp Spark data stored in memory. 
        # After 60 seconds the cache is cleared automatically, but there is still leftover data in memory
        # When the notebook is run again, there is no space leftover in memory for caching the raw_sources dataframe (hence why the notebook runs so fast)
        # So all the other notebooks that use raw_sources, run very slowly

        # Check to see if we are clearing cache the right way, and if there is a way to garbage collect other leftover data in RAM

        # https://stackoverflow.com/questions/56869630/how-to-free-up-memory-in-pyspark-session

	# Should it be spark.catalog.clearCache() or sqlContext.clearCache() ? 
        # sqlContext.clearCache() doesn't seem to be working

        # https://intellipaat.com/community/14810/un-persisting-all-dataframes-in-py-spark
		For Spark2.x+, you can use Catalog.clearCache:

		from pyspark.sql import SparkSession

		spark = SparkSession.builder.getOrCreate

		...

		spark.catalog.clearCache()

		And for Spark 1.x, you can use SQLContext.clearCache method. It basically removes all cached tables from the in-memory cache.

		from pyspark.sql import SQLContext

		from pyspark import SparkContext

		sqlContext = SQLContext.getOrCreate(SparkContext.getOrCreate())

		...

		sqlContext.clearCache()
        
        # So, I think we are using the wrong version of clearCache

		

        # Replace clearCache command in "Raw catalogue with selected columns" cell with spark.catalog.clearCache() 
        # Run notebook again..
	# Nope, we see the same result


	# Clearing the cache, seems to empty what we see in Storage in the Spark UI, but when we run the notebook that caches the "raw_sources" again, it runs within 30 seconds,
	# Nothing shows up under Spark UI / Storage and rest of cells run slow again.


	# --------------------------------------------
	
	# Try caching using Spark SQL

        # OR use spark.sql("CACHE TABLE raw_sources")

	
	# Seems to do the trick:
	# Run "Raw catalogue with selected columns" cell, then run spark.sql("CACHE TABLE raw_sources")
        # Most cells run quite fast (Visualization notebook took 3 minutes though)
 
        # Run spark.catalog.clearCache() then run the repeat the step above and run rest of cells again.
        # Cells seem to be running with consistent timing now..
        
        # We also see the cached data in the "Storage" Tab of the Spark UI





        # So to summarize:
 
        # With existing code, when we run the ML notebook, after the "Raw catalogue with selected columns" cell, a dataframe is cached in memory.
        # When we run it again, the cache is cleared, but Spark does not seem to re-cache the data

        # However if instead of df.cache() we use spark.sql("CACHE TABLE raw_sources"), then clearing cache and re-caching seems to work properly, and we get consistent results



