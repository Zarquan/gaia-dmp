#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2024, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: []
#

    Target:

        Diagnose and fix issues with the test suite.
        Previous deplyment passed the command line tests but the notebooks failed with Python errors.
        See: notes/zrq/20240208-01-notebook-errors.txt

        New deployment for testing
        See: notes/zrq/20240213-01-bash-dash.txt

    Result:

        Work in progress ...

# -----------------------------------------------------
# From previous notes [notes/zrq/20240213-01-bash-dash.txt]
#[root@ansibler]

    source /deployments/hadoop-yarn/bin/deploy.sh

    source /deployments/admin/bin/create-user-tools.sh
    import-test-users

    >   ....
    >   ....


# -----------------------------------------------------
# Install our test framework.
#[root@ansibler]

    git clone https://github.com/stvoutsin/aglais-testing

    pushd aglais-testing/

      pip install -r pip-requirements

      python3 setup.py install

    popd


# -----------------------------------------------------
# Run our 'quick' benchmark test with 1 user.
#[root@ansibler]

    pushd aglais-testing/gdmp_benchmark

        python3 \
            gdmp_benchmark.py \
                --zeppelin_url "https://${cloudname}.gaia-dmp.uk" \
                --usercount 1 \
                --notebook_config /deployments/zeppelin/test/config/quick.json \
                --user_config /tmp/test-users.json \
                --delay_start 0 \
                --delay_notebook 0

    popd


    >   [
    >       {
    >           "result": "SUCCESS",
    >           "msg": "",
    >           "output": [],
    >           "notebookid": "2JRVYNW16",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "SLOW",
    >               "totaltime": 58,
    >               "start": "2024-02-13T15:22:34.739797",
    >               "finish": "2024-02-13T15:23:33.394758",
    >               "expected": 50
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "GaiaDMPSetup"
    >       },
    >       {
    >           "result": "SUCCESS",
    >           "msg": "",
    >           "output": [],
    >           "notebookid": "2JS9UDD6W",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "SLOW",
    >               "totaltime": 316,
    >               "start": "2024-02-13T15:23:33.394936",
    >               "finish": "2024-02-13T15:28:49.661471",
    >               "expected": 125
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "Mean_proper_motions_over_the_sky"
    >       },
    >       {
    >           "result": "SUCCESS",
    >           "msg": "",
    >           "output": [],
    >           "notebookid": "2JQMX62XX",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "SLOW",
    >               "totaltime": 108,
    >               "start": "2024-02-13T15:28:49.662087",
    >               "finish": "2024-02-13T15:30:38.429457",
    >               "expected": 55
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "Source_counts_over_the_sky.json"
    >       },
    >       {
    >           "result": "ERROR",
    >           "msg": "
    >               Fail to execute line 6: assert numpy.__version__ == \"1.21.0\"
    >               Traceback (most recent call last):
    >               File \"/tmp/python8830830169252037312/zeppelin_python.py\", line 167, in <module>
    >               exec(code, _zcUserQueryNameSpace)
    >               File \"<stdin>\", line 6, in <module>
    >               AssertionError
    >               ",
    >           "output": [
    >               "
    >               Fail to execute line 6: assert numpy.__version__ == \"1.21.0\"
    >               Traceback (most recent call last):
    >               File \"/tmp/python8830830169252037312/zeppelin_python.py\", line 167, in <module>
    >               exec(code, _zcUserQueryNameSpace)
    >               File \"<stdin>\", line 6, in <module>
    >               AssertionError
    >               "
    >           ],
    >           "notebookid": "2JR2ZPWGT",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "ERROR",
    >               "totaltime": 3,
    >               "start": "2024-02-13T15:30:38.429647",
    >               "finish": "2024-02-13T15:30:41.555897",
    >               "expected": 10
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "Library_Validation.json"
    >       }
    >   ]

    #
    # Seems to pass the notebook tests but fails the Python library version checks.
    # The latter part is expected because we haven't updated the Python library versions since merging Amy's PR.
    # However, I suspect that the notebook tests should not have passed either.
    #


# -----------------------------------------------------
# Get the test user login details.
#[root@ansibler]

    source /deployments/admin/bin/create-user-tools.sh

    list-shiro-full /tmp/test-users.json

    >   [
    >     {
    >       "username": "Reyesfan",
    >       "password": "punctured dormitory astride crease",
    >       "passhash": "$shiro1$SHA-256$500000$Gv9Z+fDBzowrIckOBLbr6Q==$S1GqLRsGvvZsQ+xrEPrw5VdSBkrww0NNacsOK7y6YHU=",
    >       "hashhash": "1fd4ddaec05d94645b93b0dac8236eb2"
    >     },
    >     {
    >       "username": "Evison",
    >       "password": "anemia pedicure another helpline",
    >       "passhash": "$shiro1$SHA-256$500000$yPh0EN8X6pXRBHK5DwxBKQ==$jF7Qf00po1ZUW0Q2HCenHL+iREgvwQdIiVrsChxbVlE=",
    >       "hashhash": "803b0558fbf67f788326fd8cee84920c"
    >     },
    >     {
    >       "username": "Surbron",
    >       "password": "craving studio escalate pacify",
    >       "passhash": "$shiro1$SHA-256$500000$yW8hlM0DxKD64AQPuxrSHA==$SSR+zSpfw6DJzN//ND6ISnupRd1lBm7JEc3YhBQZRqA=",
    >       "hashhash": "c65e5415a536a1d097a9ab470d5677c6"
    >     },
    >     {
    >       "username": "Florelight",
    >       "password": "polyester trilogy sheep stinger",
    >       "passhash": "$shiro1$SHA-256$500000$zwrmNDHzjfasj//za/VWIg==$Wr5u/HblPDq+AABZyQnTJKiiORTM1HRX937uGvd1n9U=",
    >       "hashhash": "38685ea5ceacfa85853508787f0995bb"
    >     },
    >     {
    >       "username": "Nelia",
    >       "password": "rimmed unclamped sullen icing",
    >       "passhash": "$shiro1$SHA-256$500000$W5z14d+FH6xWuzJdvK3l0g==$5S3ypPKWtjuU2M2kvE27HlatQ1QPQQCQmwtyrYV/HZQ=",
    >       "hashhash": "74b142170aabb69826673398942cc820"
    >     },
    >     {
    >       "username": "Ghoria",
    >       "password": "armchair unlinked puppet skedaddle",
    >       "passhash": "$shiro1$SHA-256$500000$LPt0TA54LMZWE/n64T+u8Q==$WKYenIPXhdRGO0MVQ6CvMSRh2M+gFdp1R6LmmnRxMBE=",
    >       "hashhash": "bed19e466d9455a44b22b3da29b32997"
    >     },
    >     {
    >       "username": "Wenia",
    >       "password": "manpower undercook drift delegator",
    >       "passhash": "$shiro1$SHA-256$500000$oIu0lyRYUBM7BO1sHi7GVw==$BP42TdtRO+muPw7UJPVKFAlrPbD9n2N/3ZXNVkH+My4=",
    >       "hashhash": "b44196c9710c66ffb944586534b6eb4f"
    >     },
    >     {
    >       "username": "Thozzt",
    >       "password": "moonscape spoiling tinsmith shimmer",
    >       "passhash": "$shiro1$SHA-256$500000$7GpgTbbwgqsuQumwyH3nqw==$SZlVr4OZ9vMqjD7VuPKUqVlmmeTdLB4cfbx0Z7mLtD0=",
    >       "hashhash": "a8a099a4218da10890c958ef891b282c"
    >     },
    >     {
    >       "username": "Surli",
    >       "password": "pyromania deuce anymore outskirts",
    >       "passhash": "$shiro1$SHA-256$500000$1kKhG34euacnC9fRiMqShw==$htvJvWijIAFjpt8IdOcXFkuyaTvDhO/y6v9s0qJPv6k=",
    >       "hashhash": "dc6a61255e7521139b68901ed74d14d5"
    >     },
    >     {
    >       "username": "Drizzbinson",
    >       "password": "greasily virus automated antler",
    >       "passhash": "$shiro1$SHA-256$500000$73oLyZUhvNRkMCjz9Mx6Uw==$ZYK4yDQ4IqNxB2Kf8mNFqGMzqPiF1YzDNvp+oh5i1R8=",
    >       "hashhash": "41882fe3647c8ac8d3758e5456840c44"
    >     }
    >   ]


# -----------------------------------------------------
# Manually test the examples.
#[user@desktop]

    firefox \
        --new-window \
        'https://red.gaia-dmp.uk/' \
        &

    #
    # Login as Reyesfan and run all the notebooks to check.
    #

    1. Start here - PASS
    2. Data holdings - PASS
    3. Source counts over the sky - PASS
    4. Mean proper motions over the sky - PASS

    5. Working with Gaia XP spectra - FAIL

    >   Traceback (most recent call last):
    >     File "/opt/spark/python/pyspark/serializers.py", line 437, in dumps
    >       return cloudpickle.dumps(obj, pickle_protocol)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 73, in dumps
    >       cp.dump(obj)
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 563, in dump
    >       return Pickler.dump(self, obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 653, in reducer_override
    >       return self._function_reduce(obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 526, in _function_reduce
    >       return self._dynamic_function_reduce(obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 507, in _dynamic_function_reduce
    >       state = _function_getstate(func)
    >               ^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 157, in _function_getstate
    >       f_globals_ref = _extract_code_globals(func.__code__)
    >                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle.py", line 236, in _extract_code_globals
    >       out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
    >                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle.py", line 236, in <setcomp>
    >       out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
    >                    ~~~~~^^^^^^^
    >   IndexError: tuple index out of range


    6. Working with cross-matched surveys - FAIL

    >   Traceback (most recent call last):
    >     File "/opt/spark/python/pyspark/serializers.py", line 437, in dumps
    >       return cloudpickle.dumps(obj, pickle_protocol)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 73, in dumps
    >       cp.dump(obj)
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 563, in dump
    >       return Pickler.dump(self, obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 653, in reducer_override
    >       return self._function_reduce(obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 526, in _function_reduce
    >       return self._dynamic_function_reduce(obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 507, in _dynamic_function_reduce
    >       state = _function_getstate(func)
    >               ^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 157, in _function_getstate
    >       f_globals_ref = _extract_code_globals(func.__code__)
    >                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle.py", line 236, in _extract_code_globals
    >       out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
    >                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle.py", line 236, in <setcomp>
    >       out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
    >                    ~~~~~^^^^^^^
    >   IndexError: tuple index out of range

    7. Good astrometric solutions via ML Random Forest classifier - PASS

        * VERY slow to complete the initial spark.sql SELECT statement.
        * Pop up error "Note is now running sequentially. Can not be performed: COMMIT_PARAGRAPH".
        * 13min for the spark.sql SELECT statement to run.
        * 2min to train the random forest.

    8. Tips and tricks - FAIL

    >   org.apache.zeppelin.interpreter.InterpreterException: org.apache.zeppelin.interpreter.InterpreterException: Fail to open JupyterKernelInterpreter:
    >   org.apache.zeppelin.interpreter.InterpreterException: Kernel prerequisite is not meet: jupyter-client is not installed, installed packages:
    >   acme==2.9.0
    >   aenum==3.1.15
    >   ....
    >   ....
    >   yarncleaner==0.1
    >   zipp==3.17.0
    >
    >   	at org.apache.zeppelin.jupyter.JupyterKernelInterpreter.open(JupyterKernelInterpreter.java:129)
    >   	at org.apache.zeppelin.python.IPythonInterpreter.open(IPythonInterpreter.java:110)
    >   	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:844)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:752)
    >   	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
    >   	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)
    >   	at org.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:750)
    >
    >   	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:76)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:844)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:752)
    >   	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
    >   	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)
    >   	at org.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:750)
    >   Caused by: org.apache.zeppelin.interpreter.InterpreterException: Fail to open JupyterKernelInterpreter:
    >   org.apache.zeppelin.interpreter.InterpreterException: Kernel prerequisite is not meet: jupyter-client is not installed, installed packages:
    >   acme==2.9.0
    >   aenum==3.1.15
    >   ....
    >   ....
    >   yarncleaner==0.1
    >   zipp==3.17.0
    >
    >   	at org.apache.zeppelin.jupyter.JupyterKernelInterpreter.open(JupyterKernelInterpreter.java:129)
    >   	at org.apache.zeppelin.python.IPythonInterpreter.open(IPythonInterpreter.java:110)
    >   	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:844)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:752)
    >   	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
    >   	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)
    >   	at org.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:750)
    >
    >   	at org.apache.zeppelin.jupyter.JupyterKernelInterpreter.open(JupyterKernelInterpreter.java:145)
    >   	at org.apache.zeppelin.python.IPythonInterpreter.open(IPythonInterpreter.java:110)
    >   	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)
    >   	... 8 more
    >   Caused by: org.apache.zeppelin.interpreter.InterpreterException: Kernel prerequisite is not meet: jupyter-client is not installed, installed packages:
    >   acme==2.9.0
    >   aenum==3.1.15
    >   ....
    >   ....
    >   yarncleaner==0.1
    >   zipp==3.17.0
    >
    >   	at org.apache.zeppelin.jupyter.JupyterKernelInterpreter.open(JupyterKernelInterpreter.java:129)
    >   	... 10 more


# -----------------------------------------------------
# Manually test the examples.
#[user@desktop]

    #
    # Logout as Reyesfan.
    # Login as Florelight and run all the notebooks to check.
    # Error listing the user's notebooks.
    #

    >   ....
    >   Insufficient privileges to READER note.
    >   Allowed users or roles: [Reyesfan]
    >   But the user Florelight belongs to: [Florelight]....


# -----------------------------------------------------
# Start with a new Firefox window.
#[user@desktop]

    firefox \
        --new-window \
        'https://red.gaia-dmp.uk/' \
        &

    #
    # Login as Florelight and run all the notebooks to check.
    #

    1. Start here - PASS
    2. Data holdings - PASS
    3. Source counts over the sky - PASS
    4. Mean proper motions over the sky - PASS
    5. Working with Gaia XP spectra - FAIL

    >   Traceback (most recent call last):
    >     File "/opt/spark/python/pyspark/serializers.py", line 437, in dumps
    >       return cloudpickle.dumps(obj, pickle_protocol)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 73, in dumps
    >       cp.dump(obj)
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 563, in dump
    >       return Pickler.dump(self, obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 653, in reducer_override
    >       return self._function_reduce(obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 526, in _function_reduce
    >       return self._dynamic_function_reduce(obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 507, in _dynamic_function_reduce
    >       state = _function_getstate(func)
    >               ^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 157, in _function_getstate
    >       f_globals_ref = _extract_code_globals(func.__code__)
    >                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle.py", line 247, in _extract_code_globals
    >       out_names |= _extract_code_globals(const)
    >                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle.py", line 236, in _extract_code_globals
    >       out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
    >                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle.py", line 236, in <setcomp>
    >       out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
    >                    ~~~~~^^^^^^^
    >   IndexError: tuple index out of range
    >   ....

    6. Working with cross-matched surveys - FAIL

    >   Traceback (most recent call last):
    >     File "/opt/spark/python/pyspark/serializers.py", line 437, in dumps
    >       return cloudpickle.dumps(obj, pickle_protocol)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 73, in dumps
    >       cp.dump(obj)
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 563, in dump
    >       return Pickler.dump(self, obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 653, in reducer_override
    >       return self._function_reduce(obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 526, in _function_reduce
    >       return self._dynamic_function_reduce(obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 507, in _dynamic_function_reduce
    >       state = _function_getstate(func)
    >               ^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 157, in _function_getstate
    >       f_globals_ref = _extract_code_globals(func.__code__)
    >                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle.py", line 236, in _extract_code_globals
    >       out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
    >                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle.py", line 236, in <setcomp>
    >       out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
    >                    ~~~~~^^^^^^^
    >   IndexError: tuple index out of range
    >   Fail to execute line 41: spark.udf.register('rasterize', cmd_raster_index, IntegerType())
    >   Traceback (most recent call last):
    >     File "/opt/spark/python/pyspark/serializers.py", line 437, in dumps
    >       return cloudpickle.dumps(obj, pickle_protocol)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 73, in dumps
    >       cp.dump(obj)
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 563, in dump
    >       return Pickler.dump(self, obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 653, in reducer_override
    >       return self._function_reduce(obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 526, in _function_reduce
    >       return self._dynamic_function_reduce(obj)
    >              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 507, in _dynamic_function_reduce
    >       state = _function_getstate(func)
    >               ^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py", line 157, in _function_getstate
    >       f_globals_ref = _extract_code_globals(func.__code__)
    >                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle.py", line 236, in _extract_code_globals
    >       out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
    >                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/cloudpickle/cloudpickle.py", line 236, in <setcomp>
    >       out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}
    >                    ~~~~~^^^^^^^
    >   IndexError: tuple index out of range
    >
    >   During handling of the above exception, another exception occurred:
    >
    >   Traceback (most recent call last):
    >     File "/tmp/python6849858457342153301/zeppelin_python.py", line 162, in <module>
    >       exec(code, _zcUserQueryNameSpace)
    >     File "<stdin>", line 41, in <module>
    >     File "/opt/spark/python/pyspark/sql/udf.py", line 362, in register
    >       self.sparkSession._jsparkSession.udf().registerPython(name, register_udf._judf)
    >                                                                   ^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/sql/udf.py", line 161, in _judf
    >       self._judf_placeholder = self._create_judf()
    >                                ^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/sql/udf.py", line 170, in _create_judf
    >       wrapped_func = _wrap_function(sc, self.func, self.returnType)
    >                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/sql/udf.py", line 34, in _wrap_function
    >       pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)
    >                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/rdd.py", line 2816, in _prepare_for_python_RDD
    >       pickled_command = ser.dumps(command)
    >                         ^^^^^^^^^^^^^^^^^^
    >     File "/opt/spark/python/pyspark/serializers.py", line 447, in dumps
    >       raise pickle.PicklingError(msg)
    >   _pickle.PicklingError: Could not serialize object: IndexError: tuple index out of range
    >   ....
    >   ....


    7. Good astrometric solutions via ML Random Forest classifier - PASS

        * Slow to complete the initial spark.sql SELECT statement.
        * 15min for the spark.sql SELECT statement to run.
        * 2min to train the random forest.

    8. Tips and tricks - FAIL

    >   org.apache.zeppelin.interpreter.InterpreterException: org.apache.zeppelin.interpreter.InterpreterException: Fail to open JupyterKernelInterpreter:
    >   org.apache.zeppelin.interpreter.InterpreterException: Kernel prerequisite is not meet: jupyter-client is not installed, installed packages:
    >   acme==2.9.0
    >   aenum==3.1.15
    >   ....
    >   ....
    >   yarncleaner==0.1
    >   zipp==3.17.0
    >
    >   	at org.apache.zeppelin.jupyter.JupyterKernelInterpreter.open(JupyterKernelInterpreter.java:129)
    >   	at org.apache.zeppelin.python.IPythonInterpreter.open(IPythonInterpreter.java:110)
    >   	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:844)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:752)
    >   	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
    >   	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)
    >   	at org.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:750)
    >
    >   	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:76)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:844)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:752)
    >   	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
    >   	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)
    >   	at org.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:750)
    >   Caused by: org.apache.zeppelin.interpreter.InterpreterException: Fail to open JupyterKernelInterpreter:
    >   org.apache.zeppelin.interpreter.InterpreterException: Kernel prerequisite is not meet: jupyter-client is not installed, installed packages:
    >   acme==2.9.0
    >   aenum==3.1.15
    >   ....
    >   ....
    >   yarncleaner==0.1
    >   zipp==3.17.0
    >
    >   	at org.apache.zeppelin.jupyter.JupyterKernelInterpreter.open(JupyterKernelInterpreter.java:129)
    >   	at org.apache.zeppelin.python.IPythonInterpreter.open(IPythonInterpreter.java:110)
    >   	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:844)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:752)
    >   	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
    >   	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)
    >   	at org.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:750)
    >
    >   	at org.apache.zeppelin.jupyter.JupyterKernelInterpreter.open(JupyterKernelInterpreter.java:145)
    >   	at org.apache.zeppelin.python.IPythonInterpreter.open(IPythonInterpreter.java:110)
    >   	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)
    >   	... 8 more
    >   Caused by: org.apache.zeppelin.interpreter.InterpreterException: Kernel prerequisite is not meet: jupyter-client is not installed, installed packages:
    >   acme==2.9.0
    >   aenum==3.1.15
    >   ....
    >   ....
    >   yarncleaner==0.1
    >   zipp==3.17.0
    >
    >   	at org.apache.zeppelin.jupyter.JupyterKernelInterpreter.open(JupyterKernelInterpreter.java:129)
    >   	... 10 more....

# -----------------------------------------------------
# -----------------------------------------------------

    #
    # Based on what we found last time, reverting to the version
    # before Amy's branch was merged fixed the issues.
    #
    # Try a slightly different option this time.
    # Keep the changes to the OS and settings and
    # just revert the Python requirements.txt.
    #


# -----------------------------------------------------
# Revert to pre-merge version.
#[user@laptop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        # Save our development branch name.
        devbranch=$(
            git branch --show-current
            )

        # Checkout the commit before the merge
        git checkout 7c1034a5362788fb099139cb3f7fa2e872e116a3

    >   Note: switching to '7c1034a5362788fb099139cb3f7fa2e872e116a3'.
    >   ....
    >   ....
    >   HEAD is now at 7c1034a Working deployment on Somerville Jade


        # Grab a copy of requirements.txt
        cp deployments/common/pip/requirements.txt \
           deployments/common/pip/requirements.old

        # Checkout our development branch.
        git checkout "${devbranch}"

    >   Previous HEAD position was 7c1034a Working deployment on Somerville Jade
    >   Switched to branch '20240213-zrq-notebook-tests'
    >   ....
    >   ....


        # Replace the existing requirements.txt
        rm deployments/common/pip/requirements.txt
        cp deployments/common/pip/requirements.old \
           deployments/common/pip/requirements.txt

        git diff

    >   index ca66827..beb2327 100644
    >   --- a/deployments/common/pip/requirements.txt
    >   +++ b/deployments/common/pip/requirements.txt
    >   @@ -1,20 +1,21 @@
    >   -numpy==1.26.2
    >   -scipy==1.11.4
    >   -matplotlib==3.8.2
    >   -grpcio==1.59.3
    >   +numpy==1.21.0
    >   +scipy==1.7.3
    >   +matplotlib==3.4.2
    >   +grpcio==1.53.0
    >    jupyter==1.0.0
    >   -Cython==0.29.36
    >   -protobuf==4.25.1
    >   -pandas==2.1.3
    >   -healpy==1.16.6
    >   -astropy==5.3.4
    >   -astroquery==0.4.6
    >   -scikit-learn==1.3.2
    >   -joblib==1.3.2
    >   -hdbscan==0.8.33
    >   +Cython==0.29.23
    >   +protobuf==3.18.3
    >   +pandas==1.2.4
    >   +healpy==1.14.0
    >   +astropy==4.2.1
    >   +astroquery==0.4.1
    >   +scikit-learn==0.24.2
    >   +joblib==1.2.0
    >   +hdbscan==0.8.31
    >    pyvo==1.1
    >   -pyarrow==14.0.1
    >   -GaiaXPy==2.1.0
    >   +pyarrow==12.0.1
    >   +koalas==1.8.2
    >   +GaiaXPy==1.1.4
    >    git+https://github.com/wfau/gaiadmpsetup@v0.1.5
    >    git+https://github.com/stvoutsin/yarncleaner@v0.1.0
    >    dustmaps==1.0.12

    popd

# -----------------------------------------------------
# -----------------------------------------------------

    #
    # Same deployment process as before.
    # See: notes/zrq/20240213-01-bash-dash.txt
    #

    >   ....
    >   ....
    >   TASK [Install the required Python packages] ************************************
    >   fatal: [worker01]: FAILED! => {
    >       "changed": false,
    >       "cmd": [
    >           "/usr/bin/python3", "-m", "pip.__main__", "install", "-r", "/tmp/requirements.txt"
    >           ],
    >       "msg": "
    >           stdout: Collecting git+https://github.com/wfau/gaiadmpsetup@v0.1.5 (from -r /tmp/requirements.txt (line 19))
    >           Cloning https://github.com/wfau/gaiadmpsetup (to revision v0.1.5) to ./pip-req-build-p2wp3klw
    >           Resolved https://github.com/wfau/gaiadmpsetup to commit 0236a286a6d6444c763dba49ddda0529bff7f017
    >           Preparing metadata (setup.py): started
    >           Preparing metadata (setup.py): finished with status 'done'
    >           Collecting git+https://github.com/stvoutsin/yarncleaner@v0.1.0 (from -r /tmp/requirements.txt (line 20))
    >           Cloning https://github.com/stvoutsin/yarncleaner (to revision v0.1.0) to ./pip-req-build-wigiy12t
    >           Resolved https://github.com/stvoutsin/yarncleaner to commit a1e261944b954864b7ac84f5a4d726cc783074b9
    >           Preparing metadata (setup.py): started
    >           Preparing metadata (setup.py): finished with status 'done'
    >           Collecting numpy==1.21.0
    >           Downloading numpy-1.21.0.zip (10.3 MB)
    >           ----------------------------- 10.3/10.3 MB 28.6 MB/s eta 0:00:00
    >           Installing build dependencies: started
    >           Installing build dependencies: finished with status 'done'
    >           Getting requirements to build wheel: started
    >           Getting requirements to build wheel: finished with status 'done'
    >           Preparing metadata (pyproject.toml): started
    >           Preparing metadata (pyproject.toml): finished with status 'done'
    >           :stderr:
    >           Running command git clone --filter=blob:none --quiet https://github.com/wfau/gaiadmpsetup /tmp/pip-req-build-p2wp3klw
    >           Running command git clone --filter=blob:none --quiet https://github.com/stvoutsin/yarncleaner /tmp/pip-req-build-wigiy12t
    >           ERROR: Ignored the following versions that require a different python version:
    >               1.21.2 Requires-Python >=3.7,<3.11;
    >               1.21.3 Requires-Python >=3.7,<3.11;
    >               1.21.4 Requires-Python >=3.7,<3.11;
    >               1.21.5 Requires-Python >=3.7,<3.11;
    >               1.21.6 Requires-Python >=3.7,<3.11;
    >               1.6.2 Requires-Python >=3.7,<3.10;
    >               1.6.3 Requires-Python >=3.7,<3.10;
    >               1.7.0 Requires-Python >=3.7,<3.10;
    >               1.7.1 Requires-Python >=3.7,<3.10;
    >               1.7.2 Requires-Python >=3.7,<3.11;
    >               1.7.3 Requires-Python >=3.7,<3.11;
    >               1.8.0 Requires-Python >=3.8,<3.11;
    >               1.8.0rc1 Requires-Python >=3.8,<3.11;
    >               1.8.0rc2 Requires-Python >=3.8,<3.11;
    >               1.8.0rc3 Requires-Python >=3.8,<3.11;
    >               1.8.0rc4 Requires-Python >=3.8,<3.11;
    >               1.8.1 Requires-Python >=3.8,<3.11
    >           ERROR: Could not find a version that satisfies the requirement scipy==1.7.3 (
    >               from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0,
    >               0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0,
    >               0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1,
    >               1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2,
    >               1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.9.0rc1, 1.9.0rc2, 1.9.0rc3, 1.9.0, 1.9.1, 1.9.2,
    >               1.9.3, 1.10.0rc1, 1.10.0rc2, 1.10.0, 1.10.1, 1.11.0rc1, 1.11.0rc2, 1.11.0,
    >               1.11.1, 1.11.2, 1.11.3, 1.11.4, 1.12.0rc1, 1.12.0rc2, 1.12.0
    >               )
    >           ERROR: No matching distribution found for scipy==1.7.3
    >           "}
    >   ....
    >   ....

    #
    # No, this isn't an option.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Revert to unmodified version.
#[user@laptop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        git branch

    >     ....
    >     ....
    >   * 20240213-zrq-notebook-tests
    >     ....
    >     master

        git status

    >   ....
    >   Changes not staged for commit:
    >     (use "git add <file>..." to update what will be committed)
    >     (use "git restore <file>..." to discard changes in working directory)
    >   	modified:   deployments/common/pip/requirements.txt
    >   ....

        git restore deployments/common/pip/requirements.txt


    popd


# -----------------------------------------------------
# -----------------------------------------------------

    #
    # Same deployment process as before.
    # See: notes/zrq/20240213-01-bash-dash.txt
    #

    source "${HOME:?}/aglais.env"
    ansi-client 'red'

        source /deployments/hadoop-yarn/bin/deploy.sh

        source /deployments/admin/bin/create-user-tools.sh
        import-test-users


# -----------------------------------------------------
# Run our 'quick' benchmark test with 1 user.
#[root@ansibler]

        git clone https://github.com/stvoutsin/aglais-testing
        pushd aglais-testing/
            pip install -r pip-requirements
            python3 setup.py install
        popd

        pushd aglais-testing/gdmp_benchmark
            python3 \
                gdmp_benchmark.py \
                    --zeppelin_url "https://${cloudname}.gaia-dmp.uk" \
                    --usercount 1 \
                    --notebook_config /deployments/zeppelin/test/config/quick.json \
                    --user_config /tmp/test-users.json \
                    --delay_start 0 \
                    --delay_notebook 0
        popd

    >   [
    >       {
    >           "result": "SUCCESS",
    >           "msg": "",
    >           "output": [],
    >           "notebookid": "2JQEG6AMF",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "SLOW",
    >               "totaltime": 53,
    >               "start": "2024-02-14T06:09:05.187799",
    >               "finish": "2024-02-14T06:09:58.974697",
    >               "expected": 50
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "GaiaDMPSetup"
    >       },
    >       {
    >           "result": "SUCCESS",
    >           "msg": "",
    >           "output": [],
    >           "notebookid": "2JQTACFF5",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "SLOW",
    >               "totaltime": 142,
    >               "start": "2024-02-14T06:09:58.975011",
    >               "finish": "2024-02-14T06:12:21.259271",
    >               "expected": 125
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "Mean_proper_motions_over_the_sky"
    >       },
    >       {
    >           "result": "SUCCESS",
    >           "msg": "",
    >           "output": [],
    >           "notebookid": "2JQ4USPEN",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "FAST",
    >               "totaltime": 35,
    >               "start": "2024-02-14T06:12:21.260166",
    >               "finish": "2024-02-14T06:12:56.786763",
    >               "expected": 55
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "Source_counts_over_the_sky.json"
    >       },
    >       {
    >           "result": "ERROR",
    >           "msg": "Fail to execute line 6: assert numpy.__version__ == \"1.21.0\" \nTraceback (most recent call last):\n  File \"/tmp/python4055065928475849700/zeppelin_python.py\", line 167, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 6, in <module>\nAssertionError",
    >           "output": [
    >               "Fail to execute line 6: assert numpy.__version__ == \"1.21.0\" \nTraceback (most recent call last):\n  File \"/tmp/python4055065928475849700/zeppelin_python.py\", line 167, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 6, in <module>\nAssertionError"
    >           ],
    >           "notebookid": "2JQQRXQRA",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "ERROR",
    >               "totaltime": 3,
    >               "start": "2024-02-14T06:12:56.786947",
    >               "finish": "2024-02-14T06:13:00.131876",
    >               "expected": 10
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "Library_Validation.json"
    >       }
    >   ]

    #
    # Passes the simple notebook tests.
    # Fails the library versions check.
    #

    GaiaDMPSetup - PASS
    Mean_proper_motions_over_the_sky - PASS
    Source_counts_over_the_sky - PASS
    Library_Validation - FAIL


# -----------------------------------------------------
# Run our 'basic' benchmark test with 1 user.
#[root@ansibler]

        pushd aglais-testing/gdmp_benchmark
            python3 \
                gdmp_benchmark.py \
                    --zeppelin_url "https://${cloudname}.gaia-dmp.uk" \
                    --usercount 1 \
                    --notebook_config /deployments/zeppelin/test/config/basic.json \
                    --user_config /tmp/test-users.json \
                    --delay_start 0 \
                    --delay_notebook 0
        popd

    >   [
    >       {
    >           "result": "SUCCESS",
    >           "msg": "",
    >           "output": [],
    >           "notebookid": "2JPZ2H7C4",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "FAST",
    >               "totaltime": 47,
    >               "start": "2024-02-14T06:19:53.706154",
    >               "finish": "2024-02-14T06:20:41.688403",
    >               "expected": 50
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "GaiaDMPSetup"
    >       },
    >       {
    >           "result": "SUCCESS",
    >           "msg": "",
    >           "output": [],
    >           "notebookid": "2JQXH66JN",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "SLOW",
    >               "totaltime": 128,
    >               "start": "2024-02-14T06:20:41.688742",
    >               "finish": "2024-02-14T06:22:50.063364",
    >               "expected": 120
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "Mean_proper_motions_over_the_sky"
    >       },
    >       {
    >           "result": "SUCCESS",
    >           "msg": "",
    >           "output": [],
    >           "notebookid": "2JSKTA3EA",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "FAST",
    >               "totaltime": 34,
    >               "start": "2024-02-14T06:22:50.063924",
    >               "finish": "2024-02-14T06:23:24.687742",
    >               "expected": 55
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "Source_counts_over_the_sky.json"
    >       },
    >       {
    >           "result": "SUCCESS",
    >           "msg": "",
    >           "output": [],
    >           "notebookid": "2JNZKBXUS",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "FAST",
    >               "totaltime": 554,
    >               "start": "2024-02-14T06:23:24.687996",
    >               "finish": "2024-02-14T06:32:39.238576",
    >               "expected": 650
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "Good_astrometric_solutions_via_ML_Random_Forrest_classifier"
    >       },
    >       {
    >           "result": "ERROR",
    >           "msg": "Traceback (most recent call last):\n  File \"/opt/spark/python/pyspark/serializers.py\", line 437, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n    return Pickler.dump(self, obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 653, in reducer_override\n    return self._function_reduce(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 526, in _function_reduce\n    return self._dynamic_function_reduce(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 507, in _dynamic_function_reduce\n    state = _function_getstate(func)\n            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n    f_globals_ref = _extract_code_globals(func.__code__)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in _extract_code_globals\n    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in <setcomp>\n    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n                 ~~~~~^^^^^^^\nIndexError: tuple index out of range",
    >           "output": [
    >               "Traceback (most recent call last):\n  File \"/opt/spark/python/pyspark/serializers.py\", line 437, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n    return Pickler.dump(self, obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 653, in reducer_override\n    return self._function_reduce(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 526, in _function_reduce\n    return self._dynamic_function_reduce(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 507, in _dynamic_function_reduce\n    state = _function_getstate(func)\n            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n    f_globals_ref = _extract_code_globals(func.__code__)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in _extract_code_globals\n    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in <setcomp>\n    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n                 ~~~~~^^^^^^^\nIndexError: tuple index out of range"
    >           ],
    >           "notebookid": "2JPM61SKF",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "ERROR",
    >               "totaltime": 4,
    >               "start": "2024-02-14T06:32:39.238963",
    >               "finish": "2024-02-14T06:32:43.278848",
    >               "expected": 190
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "Working_with_cross_matched_surveys"
    >       },
    >       {
    >           "result": "ERROR",
    >           "msg": "Traceback (most recent call last):\n  File \"/opt/spark/python/pyspark/serializers.py\", line 437, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n    return Pickler.dump(self, obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 653, in reducer_override\n    return self._function_reduce(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 526, in _function_reduce\n    return self._dynamic_function_reduce(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 507, in _dynamic_function_reduce\n    state = _function_getstate(func)\n            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n    f_globals_ref = _extract_code_globals(func.__code__)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in _extract_code_globals\n    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in <setcomp>\n    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n                 ~~~~~^^^^^^^\nIndexError: tuple index out of range",
    >           "output": [
    >               "Traceback (most recent call last):\n  File \"/opt/spark/python/pyspark/serializers.py\", line 437, in dumps\n    return cloudpickle.dumps(obj, pickle_protocol)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n    return Pickler.dump(self, obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 653, in reducer_override\n    return self._function_reduce(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 526, in _function_reduce\n    return self._dynamic_function_reduce(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 507, in _dynamic_function_reduce\n    state = _function_getstate(func)\n            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n    f_globals_ref = _extract_code_globals(func.__code__)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in _extract_code_globals\n    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in <setcomp>\n    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n                 ~~~~~^^^^^^^\nIndexError: tuple index out of range"
    >           ],
    >           "notebookid": "2JSGPDXQ4",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "ERROR",
    >               "totaltime": 4,
    >               "start": "2024-02-14T06:32:43.279182",
    >               "finish": "2024-02-14T06:32:47.400433",
    >               "expected": 190
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "Working_with_Gaia_XP_spectra.json"
    >       },
    >       {
    >           "result": "ERROR",
    >           "msg": "Fail to execute line 6: assert numpy.__version__ == \"1.21.0\" \nTraceback (most recent call last):\n  File \"/tmp/python8191193644883281528/zeppelin_python.py\", line 167, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 6, in <module>\nAssertionError",
    >           "output": [
    >               "Fail to execute line 6: assert numpy.__version__ == \"1.21.0\" \nTraceback (most recent call last):\n  File \"/tmp/python8191193644883281528/zeppelin_python.py\", line 167, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 6, in <module>\nAssertionError"
    >           ],
    >           "notebookid": "2JP8GBETE",
    >           "user_config": "/tmp/user1.yml",
    >           "messages": [],
    >           "logs": "",
    >           "time": {
    >               "result": "ERROR",
    >               "totaltime": 3,
    >               "start": "2024-02-14T06:32:47.400703",
    >               "finish": "2024-02-14T06:32:50.497138",
    >               "expected": 10
    >           },
    >           "outputs": {
    >               "valid": true
    >           },
    >           "name": "Library_Validation.json"
    >       }
    >   ]

    #
    # Passes the simple notebook tests.
    # Fails the cross_matched_surveys and Gaia_XP_spectra tests.
    # Fails the library versions check.
    #

    GaiaDMPSetup - PASS
    Mean_proper_motions_over_the_sky - PASS
    Source_counts_over_the_sky - PASS
    Good_astrometric_solutions_via_ML_Random_Forrest_classifier - PASS
    Working_with_cross_matched_surveys - FAIL
    Working_with_Gaia_XP_spectra.json - FAIL
    Library_Validation.json - FAIL

    #
    # We have an automated test that we can use as a health check.
    # Why wasn't the 'basic' test used before the changes were merged ?
    # Suggests bad process on our part.
    #
    # Where do we go from here ?
    # We can't use the current HEAD branch for a new deployment, the example notebooks don't work.
    # Reverting back to an old version of Fedora and Python is not a great option either.
    #



