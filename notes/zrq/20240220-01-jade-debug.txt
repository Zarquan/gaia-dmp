#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2024, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: []
#

    Target:

        Collecting more details about the failed deployment from yesterday.

    Result:

        Embarrassing/iritating outcome.
        While I was collecting all of this infomation, I dsciovered that the deployment was working.
        Some time in the 12hrs between when I did the initial deploy and when I was collecting the data
        the cluster finally managed to fix itself.

        Discovered some more thinsg to check if/when we get the same problem again.

        We need to come up with a way of measuting how long it takes for a cluster to resolve itself.
        Something like `watch`, but one that periodically checks the cluster status and can detect
        if/when it becomes healthy.


# -----------------------------------------------------

        Message from Scott Davidson on Slack
        Hi
        @Dave Morris
        I hope you donâ€™t mind me jumping in here but I was just looking at your notes from the Kubernetes issues
        you were having yesterday and these lines stand out to me.
        It looks like 172.24.0.1 might be the internal IP address of the Kubernetes API server (I think you can
        check with kubectl get svc -n default ) in which case a HTTP 500 from the Kubernetes API server itself
        looks pretty suspicious.
        Do you (or anyone else here) happen to know if the OpenStack VMs that make up the cluster are volume-backed?
        The reason I ask is that there are known issues around running etcd on slow(er) storage devices such as
        network-attached storage and sometimes even with HDD local disks.
        If the storage backing the VMs is too slow (or is somehow sensitive to other workloads happening on the cloud)
        then that might explain the intermittent issues you have been seeing.

# -----------------------------------------------------
# -----------------------------------------------------
#[user@desktop]

    #
    # Re-connect a client using notes.
    # notes/zrq/20240219-03-jade-reconnect.txt
    #

# -----------------------------------------------------
# List the nodes, flavors and images.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        server list

    >   +--------------------------------------+------------------------------------------------------------+--------+----------------------------------------------------------------------------+-----------------------------------+----------------+
    >   | ID                                   | Name                                                       | Status | Networks                                                                   | Image                             | Flavor         |
    >   +--------------------------------------+------------------------------------------------------------+--------+----------------------------------------------------------------------------+-----------------------------------+----------------+
    >   | 32bdadee-9ab1-4f7e-ade4-463908234aa5 | somerville-jade-20240219-work-md-0-fb50a5e8-fhxtc          | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.157 | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.26vcpu |
    >   | 7f6c01ab-059f-4709-8149-36ce4864570b | somerville-jade-20240219-work-md-0-fb50a5e8-bh6d5          | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.223 | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.26vcpu |
    >   | ed9a635d-ed0a-4b2e-a054-41160b3feb80 | somerville-jade-20240219-work-md-0-fb50a5e8-9bsxs          | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.225 | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.26vcpu |
    >   | d7c145c8-b699-4555-bde6-4ec7973a5ba7 | somerville-jade-20240219-work-md-0-fb50a5e8-zjwtj          | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.243 | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.26vcpu |
    >   | f380cceb-6385-48e5-bf70-030ace07b8e7 | somerville-jade-20240219-work-control-plane-ac9af912-v42dq | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.171 | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.2vcpu  |
    >   | 2ff8c8d9-34a1-444d-b32c-8db1a806e833 | somerville-jade-20240219-work-control-plane-ac9af912-m4vdt | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.47  | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.2vcpu  |
    >   | d64cc19f-f0eb-4044-b28d-476fc39208f9 | somerville-jade-20240219-work-md-0-fb50a5e8-ntqbd          | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.129 | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.26vcpu |
    >   | d05a0082-33f7-4d8d-bc57-b33757c67cd2 | somerville-jade-20240219-work-md-0-fb50a5e8-whft4          | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.113 | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.26vcpu |
    >   | 3eca33f6-b2f0-414a-b7ca-c2a35541022e | somerville-jade-20240219-work-control-plane-ac9af912-gjv45 | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.186 | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.2vcpu  |
    >   | d8d63532-0ca9-4a0c-9e84-93644df8af49 | somerville-jade-20240219-bootstrap-node                    | ACTIVE | somerville-jade-20240219-bootstrap-network=10.10.0.211, 192.41.122.174     | gaia-dmp-fedora-cloud-38-1.6      | gaia.vm.2vcpu  |
    >   +--------------------------------------+------------------------------------------------------------+--------+----------------------------------------------------------------------------+-----------------------------------+----------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        server show \
            somerville-jade-20240219-work-md-0-fb50a5e8-fhxtc

    >   +-------------------------------------+----------------------------------------------------------------------------+
    >   | Field                               | Value                                                                      |
    >   +-------------------------------------+----------------------------------------------------------------------------+
    >   | OS-DCF:diskConfig                   | MANUAL                                                                     |
    >   | OS-EXT-AZ:availability_zone         | nova                                                                       |
    >   | OS-EXT-SRV-ATTR:host                | sv-hpe-0-6                                                                 |
    >   | OS-EXT-SRV-ATTR:hypervisor_hostname | sv-hpe-0-6                                                                 |
    >   | OS-EXT-SRV-ATTR:instance_name       | instance-000075e9                                                          |
    >   | OS-EXT-STS:power_state              | Running                                                                    |
    >   | OS-EXT-STS:task_state               | None                                                                       |
    >   | OS-EXT-STS:vm_state                 | active                                                                     |
    >   | OS-SRV-USG:launched_at              | 2024-02-19T17:13:10.000000                                                 |
    >   | OS-SRV-USG:terminated_at            | None                                                                       |
    >   | accessIPv4                          |                                                                            |
    >   | accessIPv6                          |                                                                            |
    >   | addresses                           | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.157 |
    >   | config_drive                        |                                                                            |
    >   | created                             | 2024-02-19T17:13:06Z                                                       |
    >   | flavor                              | gaia.vm.26vcpu (f5bf7c55-d6aa-4ef7-ba91-6e15683ab557)                      |
    >   | hostId                              | f790b78efb6cb4355ad73dd6a6f953627fb3e8c2a0457196852611a8                   |
    >   | id                                  | 32bdadee-9ab1-4f7e-ade4-463908234aa5                                       |
    >   | image                               | gaia-dmp-ubuntu-2204-kube-v1.26.7 (2bfecf33-9fd4-4687-bf6a-569e43c47999)   |
    >   | key_name                            | somerville-jade-20240219-keypair                                           |
    >   | name                                | somerville-jade-20240219-work-md-0-fb50a5e8-fhxtc                          |
    >   | progress                            | 0                                                                          |
    >   | project_id                          | be227fe0300b4ce5b03f44264df615df                                           |
    >   | properties                          |                                                                            |
    >   | security_groups                     | name='k8s-cluster-default-somerville-jade-20240219-work-secgroup-worker'   |
    >   | status                              | ACTIVE                                                                     |
    >   | updated                             | 2024-02-19T17:13:10Z                                                       |
    >   | user_id                             | c4aad146ab7acaf44819e90e3e67a4d0490c164fbb02d388823c1ac9f0ae2e13           |
    >   | volumes_attached                    |                                                                            |
    >   +-------------------------------------+----------------------------------------------------------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        image show \
            gaia-dmp-ubuntu-2204-kube-v1.26.7

    >   +------------------+---------------------------------------------------------------------------------+
    >   | Field            | Value                                                                           |
    >   +------------------+---------------------------------------------------------------------------------+
    >   | checksum         | eb33d889f410ee521e87d313f1b200ce                                                |
    >   | container_format | bare                                                                            |
    >   | created_at       | 2024-01-06T03:39:13Z                                                            |
    >   | disk_format      | qcow2                                                                           |
    >   | file             | /v2/images/2bfecf33-9fd4-4687-bf6a-569e43c47999/file                            |
    >   | id               | 2bfecf33-9fd4-4687-bf6a-569e43c47999                                            |
    >   | min_disk         | 0                                                                               |
    >   | min_ram          | 0                                                                               |
    >   | name             | gaia-dmp-ubuntu-2204-kube-v1.26.7                                               |
    >   | owner            | be227fe0300b4ce5b03f44264df615df                                                |
    >   | properties       | direct_url='rbd://84c5........7999/snap',                                       |
    >   |                  | os_hash_algo='sha512',                                                          |
    >   |                  | os_hash_value='7015........147e',                                               |
    >   |                  | os_hidden='False',                                                              |
    >   |                  | owner_specified.openstack.md5='',                                               |
    >   |                  | owner_specified.openstack.object='images/gaia-dmp-ubuntu-2204-kube-v1.26.7',    |
    >   |                  | owner_specified.openstack.sha256='',                                            |
    >   |                  | stores='rbd'                                                                    |
    >   | protected        | False                                                                           |
    >   | schema           | /v2/schemas/image                                                               |
    >   | size             | 10737418240                                                                     |
    >   | status           | active                                                                          |
    >   | tags             |                                                                                 |
    >   | updated_at       | 2024-01-06T05:45:22Z                                                            |
    >   | visibility       | shared                                                                          |
    >   +------------------+---------------------------------------------------------------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        flavor show \
            gaia.vm.26vcpu

    >   +----------------------------+--------------------------------------+
    >   | Field                      | Value                                |
    >   +----------------------------+--------------------------------------+
    >   | OS-FLV-DISABLED:disabled   | False                                |
    >   | OS-FLV-EXT-DATA:ephemeral  | 0                                    |
    >   | access_project_ids         | None                                 |
    >   | description                | None                                 |
    >   | disk                       | 20                                   |
    >   | id                         | f5bf7c55-d6aa-4ef7-ba91-6e15683ab557 |
    >   | name                       | gaia.vm.26vcpu                       |
    >   | os-flavor-access:is_public | True                                 |
    >   | properties                 | trait:CUSTOM_SSD_DEV='forbidden'     |
    >   | ram                        | 44032                                |
    >   | rxtx_factor                | 1.0                                  |
    >   | swap                       |                                      |
    >   | vcpus                      | 26                                   |
    >   +----------------------------+--------------------------------------+


# -----------------------------------------------------
# List the kube-system Pods in the tenant (work) cluster.
#[root@ansibler]

    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            get pods \
                --namespace kube-system
        '

    >   NAME                                                                                 READY   STATUS    RESTARTS      AGE
    >   coredns-787d4945fb-dv6px                                                             1/1     Running   0             18h
    >   coredns-787d4945fb-svl9q                                                             1/1     Running   0             18h
    >   etcd-somerville-jade-20240219-work-control-plane-ac9af912-gjv45                      1/1     Running   0             18h
    >   etcd-somerville-jade-20240219-work-control-plane-ac9af912-m4vdt                      1/1     Running   0             18h
    >   etcd-somerville-jade-20240219-work-control-plane-ac9af912-v42dq                      1/1     Running   0             18h
    >   kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-gjv45            1/1     Running   0             18h
    >   kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-m4vdt            1/1     Running   0             18h
    >   kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-v42dq            1/1     Running   0             18h
    >   kube-controller-manager-somerville-jade-20240219-work-control-plane-ac9af912-gjv45   1/1     Running   4 (18h ago)   18h
    >   kube-controller-manager-somerville-jade-20240219-work-control-plane-ac9af912-m4vdt   1/1     Running   2 (18h ago)   18h
    >   kube-controller-manager-somerville-jade-20240219-work-control-plane-ac9af912-v42dq   1/1     Running   0             18h
    >   kube-proxy-6ccvh                                                                     1/1     Running   0             18h
    >   kube-proxy-6vr7b                                                                     1/1     Running   0             18h
    >   kube-proxy-7qwtb                                                                     1/1     Running   0             18h
    >   kube-proxy-8pn9v                                                                     1/1     Running   0             18h
    >   kube-proxy-dpzg8                                                                     1/1     Running   0             18h
    >   kube-proxy-ppr9v                                                                     1/1     Running   0             18h
    >   kube-proxy-qn22t                                                                     1/1     Running   0             18h
    >   kube-proxy-rj9qh                                                                     1/1     Running   0             18h
    >   kube-proxy-vpskm                                                                     1/1     Running   0             18h
    >   kube-scheduler-somerville-jade-20240219-work-control-plane-ac9af912-gjv45            1/1     Running   4 (18h ago)   18h
    >   kube-scheduler-somerville-jade-20240219-work-control-plane-ac9af912-m4vdt            1/1     Running   2 (18h ago)   18h
    >   kube-scheduler-somerville-jade-20240219-work-control-plane-ac9af912-v42dq            1/1     Running   0             18h
    >   metrics-server-65cccfc7bb-k594p                                                      1/1     Running   0             18h


# -----------------------------------------------------
# Check the 'kube-apiserver' Pod in the 'kube-system' namespace.
#[root@ansibler]

    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            get pods \
                --namespace kube-system \
                --output json \
        | jq -r ".items[].metadata.name | select(. | startswith(\"kube-apiserver\"))"
        '

    >   kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-gjv45
    >   kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-m4vdt
    >   kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-v42dq


    ssh bootstrap -t \
        '
        source loadconfig
        for podname in $(
            kubectl \
                --kubeconfig "${workclusterconf:?}" \
                    get pods \
                        --namespace kube-system \
                        --output json \
            | jq -r ".items[].metadata.name | select(. | startswith(\"kube-apiserver\"))"
            )
        do
            echo ""
            echo "---- ---- ---- ----"
            echo "Podname [${podname}]"
            kubectl \
                --kubeconfig "${workclusterconf:?}" \
                describe pod \
                    --namespace kube-system \
                    "${podname}"
        done
        '

    >   ---- ---- ---- ----
    >   Podname [kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-gjv45]
    >   Name:                 kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-gjv45
    >   Namespace:            kube-system
    >   Priority:             2000001000
    >   Priority Class Name:  system-node-critical
    >   Node:                 somerville-jade-20240219-work-control-plane-ac9af912-gjv45/192.168.3.186
    >   Start Time:           Mon, 19 Feb 2024 17:02:41 +0000
    >   Labels:               component=kube-apiserver
    >                         tier=control-plane
    >   Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.3.186:6443
    >                         kubernetes.io/config.hash: 46c32cf496946e8498634f34e761d972
    >                         kubernetes.io/config.mirror: 46c32cf496946e8498634f34e761d972
    >                         kubernetes.io/config.seen: 2024-02-19T17:02:40.871951952Z
    >                         kubernetes.io/config.source: file
    >   Status:               Running
    >   SeccompProfile:       RuntimeDefault
    >   IP:                   192.168.3.186
    >   IPs:
    >     IP:           192.168.3.186
    >   Controlled By:  Node/somerville-jade-20240219-work-control-plane-ac9af912-gjv45
    >   Containers:
    >     kube-apiserver:
    >       Container ID:  containerd://620263b18a07caf23fd79658055a6d5ef32ca555c8c203ae054322aa7afc0adf
    >       Image:         registry.k8s.io/kube-apiserver:v1.26.7
    >       Image ID:      registry.k8s.io/kube-apiserver@sha256:c3b8fbd0418e29e8a3d49fbeebc187ffba6d0b2e437fc6c4db2cfb69b19163bf
    >       Port:          <none>
    >       Host Port:     <none>
    >       Command:
    >         kube-apiserver
    >         --advertise-address=192.168.3.186
    >         --allow-privileged=true
    >         --authorization-mode=Node,RBAC
    >         --client-ca-file=/etc/kubernetes/pki/ca.crt
    >         --cloud-provider=external
    >         --enable-admission-plugins=NodeRestriction
    >         --enable-bootstrap-token-auth=true
    >         --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    >         --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    >         --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    >         --etcd-servers=https://127.0.0.1:2379
    >         --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    >         --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    >         --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    >         --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    >         --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    >         --requestheader-allowed-names=front-proxy-client
    >         --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    >         --requestheader-extra-headers-prefix=X-Remote-Extra-
    >         --requestheader-group-headers=X-Remote-Group
    >         --requestheader-username-headers=X-Remote-User
    >         --secure-port=6443
    >         --service-account-issuer=https://kubernetes.default.svc.cluster.local
    >         --service-account-key-file=/etc/kubernetes/pki/sa.pub
    >         --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    >         --service-cluster-ip-range=172.24.0.0/13
    >         --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    >         --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    >       State:          Running
    >         Started:      Mon, 19 Feb 2024 17:01:51 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Requests:
    >         cpu:        250m
    >       Liveness:     http-get https://192.168.3.186:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    >       Readiness:    http-get https://192.168.3.186:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    >       Startup:      http-get https://192.168.3.186:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    >       Environment:  <none>
    >       Mounts:
    >         /etc/ca-certificates from etc-ca-certificates (ro)
    >         /etc/kubernetes/pki from k8s-certs (ro)
    >         /etc/ssl/certs from ca-certs (ro)
    >         /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
    >         /usr/share/ca-certificates from usr-share-ca-certificates (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     ca-certs:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/ssl/certs
    >       HostPathType:  DirectoryOrCreate
    >     etc-ca-certificates:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/ca-certificates
    >       HostPathType:  DirectoryOrCreate
    >     k8s-certs:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/kubernetes/pki
    >       HostPathType:  DirectoryOrCreate
    >     usr-local-share-ca-certificates:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /usr/local/share/ca-certificates
    >       HostPathType:  DirectoryOrCreate
    >     usr-share-ca-certificates:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /usr/share/ca-certificates
    >       HostPathType:  DirectoryOrCreate
    >   QoS Class:         Burstable
    >   Node-Selectors:    <none>
    >   Tolerations:       :NoExecute op=Exists
    >   Events:
    >     Type     Reason     Age                    From     Message
    >     ----     ------     ----                   ----     -------
    >     Warning  Unhealthy  93m (x21 over 18h)     kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500
    >     Warning  Unhealthy  4m10s (x251 over 18h)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
    >   
    >   ---- ---- ---- ----
    >   Podname [kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-m4vdt]
    >   Name:                 kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-m4vdt
    >   Namespace:            kube-system
    >   Priority:             2000001000
    >   Priority Class Name:  system-node-critical
    >   Node:                 somerville-jade-20240219-work-control-plane-ac9af912-m4vdt/192.168.3.47
    >   Start Time:           Mon, 19 Feb 2024 17:05:09 +0000
    >   Labels:               component=kube-apiserver
    >                         tier=control-plane
    >   Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.3.47:6443
    >                         kubernetes.io/config.hash: f6125d635ed29a6fe511cb611ace8915
    >                         kubernetes.io/config.mirror: f6125d635ed29a6fe511cb611ace8915
    >                         kubernetes.io/config.seen: 2024-02-19T17:05:08.276846295Z
    >                         kubernetes.io/config.source: file
    >   Status:               Running
    >   SeccompProfile:       RuntimeDefault
    >   IP:                   192.168.3.47
    >   IPs:
    >     IP:           192.168.3.47
    >   Controlled By:  Node/somerville-jade-20240219-work-control-plane-ac9af912-m4vdt
    >   Containers:
    >     kube-apiserver:
    >       Container ID:  containerd://5d8bc6b204199714638784346538cd226ebd6ae9f9321d3181fbae2e4b429733
    >       Image:         registry.k8s.io/kube-apiserver:v1.26.7
    >       Image ID:      registry.k8s.io/kube-apiserver@sha256:c3b8fbd0418e29e8a3d49fbeebc187ffba6d0b2e437fc6c4db2cfb69b19163bf
    >       Port:          <none>
    >       Host Port:     <none>
    >       Command:
    >         kube-apiserver
    >         --advertise-address=192.168.3.47
    >         --allow-privileged=true
    >         --authorization-mode=Node,RBAC
    >         --client-ca-file=/etc/kubernetes/pki/ca.crt
    >         --cloud-provider=external
    >         --enable-admission-plugins=NodeRestriction
    >         --enable-bootstrap-token-auth=true
    >         --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    >         --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    >         --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    >         --etcd-servers=https://127.0.0.1:2379
    >         --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    >         --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    >         --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    >         --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    >         --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    >         --requestheader-allowed-names=front-proxy-client
    >         --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    >         --requestheader-extra-headers-prefix=X-Remote-Extra-
    >         --requestheader-group-headers=X-Remote-Group
    >         --requestheader-username-headers=X-Remote-User
    >         --secure-port=6443
    >         --service-account-issuer=https://kubernetes.default.svc.cluster.local
    >         --service-account-key-file=/etc/kubernetes/pki/sa.pub
    >         --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    >         --service-cluster-ip-range=172.24.0.0/13
    >         --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    >         --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    >       State:          Running
    >         Started:      Mon, 19 Feb 2024 17:05:24 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Requests:
    >         cpu:        250m
    >       Liveness:     http-get https://192.168.3.47:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    >       Readiness:    http-get https://192.168.3.47:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    >       Startup:      http-get https://192.168.3.47:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    >       Environment:  <none>
    >       Mounts:
    >         /etc/ca-certificates from etc-ca-certificates (ro)
    >         /etc/kubernetes/pki from k8s-certs (ro)
    >         /etc/ssl/certs from ca-certs (ro)
    >         /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
    >         /usr/share/ca-certificates from usr-share-ca-certificates (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     ca-certs:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/ssl/certs
    >       HostPathType:  DirectoryOrCreate
    >     etc-ca-certificates:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/ca-certificates
    >       HostPathType:  DirectoryOrCreate
    >     k8s-certs:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/kubernetes/pki
    >       HostPathType:  DirectoryOrCreate
    >     usr-local-share-ca-certificates:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /usr/local/share/ca-certificates
    >       HostPathType:  DirectoryOrCreate
    >     usr-share-ca-certificates:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /usr/share/ca-certificates
    >       HostPathType:  DirectoryOrCreate
    >   QoS Class:         Burstable
    >   Node-Selectors:    <none>
    >   Tolerations:       :NoExecute op=Exists
    >   Events:
    >     Type     Reason     Age                   From     Message
    >     ----     ------     ----                  ----     -------
    >     Normal   Pulled     18h                   kubelet  Container image "registry.k8s.io/kube-apiserver:v1.26.7" already present on machine
    >     Normal   Created    18h                   kubelet  Created container kube-apiserver
    >     Normal   Started    18h                   kubelet  Started container kube-apiserver
    >     Warning  Unhealthy  18h                   kubelet  Startup probe failed: HTTP probe failed with statuscode: 403
    >     Warning  Unhealthy  18h (x4 over 18h)     kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
    >     Warning  Unhealthy  13m (x139 over 18h)   kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
    >     Warning  Unhealthy  3m17s (x21 over 18h)  kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500
    >   
    >   ---- ---- ---- ----
    >   Podname [kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-v42dq]
    >   Name:                 kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-v42dq
    >   Namespace:            kube-system
    >   Priority:             2000001000
    >   Priority Class Name:  system-node-critical
    >   Node:                 somerville-jade-20240219-work-control-plane-ac9af912-v42dq/192.168.3.171
    >   Start Time:           Mon, 19 Feb 2024 17:09:40 +0000
    >   Labels:               component=kube-apiserver
    >                         tier=control-plane
    >   Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.3.171:6443
    >                         kubernetes.io/config.hash: 94787ecd0b24bc15df444d00e1ead91e
    >                         kubernetes.io/config.mirror: 94787ecd0b24bc15df444d00e1ead91e
    >                         kubernetes.io/config.seen: 2024-02-19T17:09:35.786020973Z
    >                         kubernetes.io/config.source: file
    >   Status:               Running
    >   SeccompProfile:       RuntimeDefault
    >   IP:                   192.168.3.171
    >   IPs:
    >     IP:           192.168.3.171
    >   Controlled By:  Node/somerville-jade-20240219-work-control-plane-ac9af912-v42dq
    >   Containers:
    >     kube-apiserver:
    >       Container ID:  containerd://b6ecd2f7c06f03576508e0617ac9e8ce93b5321b1905079da9a97a015d0869c7
    >       Image:         registry.k8s.io/kube-apiserver:v1.26.7
    >       Image ID:      registry.k8s.io/kube-apiserver@sha256:c3b8fbd0418e29e8a3d49fbeebc187ffba6d0b2e437fc6c4db2cfb69b19163bf
    >       Port:          <none>
    >       Host Port:     <none>
    >       Command:
    >         kube-apiserver
    >         --advertise-address=192.168.3.171
    >         --allow-privileged=true
    >         --authorization-mode=Node,RBAC
    >         --client-ca-file=/etc/kubernetes/pki/ca.crt
    >         --cloud-provider=external
    >         --enable-admission-plugins=NodeRestriction
    >         --enable-bootstrap-token-auth=true
    >         --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    >         --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    >         --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    >         --etcd-servers=https://127.0.0.1:2379
    >         --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    >         --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    >         --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    >         --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    >         --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    >         --requestheader-allowed-names=front-proxy-client
    >         --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    >         --requestheader-extra-headers-prefix=X-Remote-Extra-
    >         --requestheader-group-headers=X-Remote-Group
    >         --requestheader-username-headers=X-Remote-User
    >         --secure-port=6443
    >         --service-account-issuer=https://kubernetes.default.svc.cluster.local
    >         --service-account-key-file=/etc/kubernetes/pki/sa.pub
    >         --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    >         --service-cluster-ip-range=172.24.0.0/13
    >         --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    >         --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    >       State:          Running
    >         Started:      Mon, 19 Feb 2024 17:09:56 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Requests:
    >         cpu:        250m
    >       Liveness:     http-get https://192.168.3.171:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    >       Readiness:    http-get https://192.168.3.171:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    >       Startup:      http-get https://192.168.3.171:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    >       Environment:  <none>
    >       Mounts:
    >         /etc/ca-certificates from etc-ca-certificates (ro)
    >         /etc/kubernetes/pki from k8s-certs (ro)
    >         /etc/ssl/certs from ca-certs (ro)
    >         /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
    >         /usr/share/ca-certificates from usr-share-ca-certificates (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     ca-certs:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/ssl/certs
    >       HostPathType:  DirectoryOrCreate
    >     etc-ca-certificates:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/ca-certificates
    >       HostPathType:  DirectoryOrCreate
    >     k8s-certs:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/kubernetes/pki
    >       HostPathType:  DirectoryOrCreate
    >     usr-local-share-ca-certificates:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /usr/local/share/ca-certificates
    >       HostPathType:  DirectoryOrCreate
    >     usr-share-ca-certificates:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /usr/share/ca-certificates
    >       HostPathType:  DirectoryOrCreate
    >   QoS Class:         Burstable
    >   Node-Selectors:    <none>
    >   Tolerations:       :NoExecute op=Exists
    >   Events:
    >     Type     Reason     Age                    From     Message
    >     ----     ------     ----                   ----     -------
    >     Normal   Pulled     18h                    kubelet  Container image "registry.k8s.io/kube-apiserver:v1.26.7" already present on machine
    >     Normal   Created    18h                    kubelet  Created container kube-apiserver
    >     Normal   Started    18h                    kubelet  Started container kube-apiserver
    >     Warning  Unhealthy  18h (x5 over 18h)      kubelet  Startup probe failed: Get "https://192.168.3.171:6443/livez": dial tcp 192.168.3.171:6443: connect: connection refused
    >     Warning  Unhealthy  18h                    kubelet  Startup probe failed: Get "https://192.168.3.171:6443/livez": net/http: TLS handshake timeout
    >     Warning  Unhealthy  18h                    kubelet  Startup probe failed: HTTP probe failed with statuscode: 403
    >     Warning  Unhealthy  27m (x28 over 18h)     kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500
    >     Warning  Unhealthy  9m40s (x144 over 18h)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500



# -----------------------------------------------------
# Slow down and read Scott's comment again.
# He actually tells us how to find out what the IP address is.
#[root@ansibler]

    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            get services \
                --namespace default
        '

    >   NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
    >   kubernetes   ClusterIP   172.24.0.1   <none>        443/TCP   19h


# -----------------------------------------------------
# Found some documentation about debugging services.
# https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/
#[root@ansibler]

    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            describe service \
                --namespace default \
                kubernetes
        '

    >   Name:              kubernetes
    >   Namespace:         default
    >   Labels:            component=apiserver
    >                      provider=kubernetes
    >   Annotations:       <none>
    >   Selector:          <none>
    >   Type:              ClusterIP
    >   IP Family Policy:  SingleStack
    >   IP Families:       IPv4
    >   IP:                172.24.0.1
    >   IPs:               172.24.0.1
    >   Port:              https  443/TCP
    >   TargetPort:        6443/TCP
    >   Endpoints:         192.168.3.171:6443,192.168.3.186:6443,192.168.3.47:6443
    >   Session Affinity:  None
    >   Events:            <none>


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            get service \
                --namespace default \
                kubernetes \
                    --output json
        '

    >   {
    >       "apiVersion": "v1",
    >       "kind": "Service",
    >       "metadata": {
    >           "creationTimestamp": "2024-02-19T17:01:57Z",
    >           "labels": {
    >               "component": "apiserver",
    >               "provider": "kubernetes"
    >           },
    >           "name": "kubernetes",
    >           "namespace": "default",
    >           "resourceVersion": "191",
    >           "uid": "7080949b-a9fc-48bf-89fd-bc9098ed2132"
    >       },
    >       "spec": {
    >           "clusterIP": "172.24.0.1",
    >           "clusterIPs": [
    >               "172.24.0.1"
    >           ],
    >           "internalTrafficPolicy": "Cluster",
    >           "ipFamilies": [
    >               "IPv4"
    >           ],
    >           "ipFamilyPolicy": "SingleStack",
    >           "ports": [
    >               {
    >                   "name": "https",
    >                   "port": 443,
    >                   "protocol": "TCP",
    >                   "targetPort": 6443
    >               }
    >           ],
    >           "sessionAffinity": "None",
    >           "type": "ClusterIP"
    >       },
    >       "status": {
    >           "loadBalancer": {}
    >       }
    >   }

