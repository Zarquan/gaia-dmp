#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Create a live deploy that we can use.
        Not sharing notebooks directory.
        Not including GaiaXPy.

        Not including secret service.
        Manual steps for user configuration.
        Manual steps for DNS configuration.

        Limited in memory on the Zeppelin node.

        Back to where we were, but on the Arcus cloud.

    Result:

        Fail, because the deploy still had a shared notebooks directory.
        Should have updated our branch from upstream to get the latest changes first.



# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --publish 3000:3000 \
        --publish 8088:8088 \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        atolmis/ansible-client:2021.08.25 \
        bash


# -----------------------------------------------------
# Set the cloud and configuration.
#[root@ansibler]

    cloudname=iris-gaia-blue
    configname=zeppelin-27.45-spark-6.27.45



# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}" \

    >   real    4m37.184s
    >   user    1m41.672s
    >   sys     0m12.653s


# -----------------------------------------------------
# Create everything, using the new config.
# Using 'test' to run the built-in tests.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}" \
            "${configname:?}" \
            'test' \
        | tee /tmp/create-all.log

    >   real    200m17.287s
    >   user    44m50.167s
    >   sys     7m26.982s


# -----------------------------------------------------
# Parse the test results as JSON.
#[root@ansibler]

    sed "
        1 d
        s/'\([0-9.]*\)'/\1/g
        s/:[[:space:]],/: '',/g
        s/'/\"/g
        " \
        '/tmp/test-result.json' \
    | jq '.'


    >   {
    >     "SetUp": {
    >       "totaltime": 49.14,
    >       "status": "SLOW",
    >       "msg": "",
    >       "valid": "TRUE"
    >     },
    >     "Mean_proper_motions_over_the_sky": {
    >       "totaltime": 47.27,
    >       "status": "SUCCESS",
    >       "msg": "",
    >       "valid": "TRUE"
    >     },
    >     "Source_counts_over_the_sky.json": {
    >       "totaltime": 16.14,
    >       "status": "SUCCESS",
    >       "msg": "",
    >       "valid": "TRUE"
    >     },
    >     "Good_astrometric_solutions_via_ML_Random_Forrest_classifier": {
    >       "totaltime": 520.11,
    >       "status": "SLOW",
    >       "msg": "",
    >       "valid": "TRUE"
    >     },
    >     "QC_cuts_dev.json": {
    >       "totaltime": 4350.43,
    >       "status": "SUCCESS",
    >       "msg": "",
    >       "valid": "TRUE"
    >     },
    >     "WD_detection_dev.json": {
    >       "totaltime": 4502.4,
    >       "status": "SLOW",
    >       "msg": "",
    >       "valid": "TRUE"
    >     }
    >   }


# -----------------------------------------------------
# Tail the Spark logs during the tests.
#[root@ansibler]


    ssh zeppelin

        pushd /home/fedora/zeppelin-0.10.0-bin-all/logs

            tail -f zeppelin-interpreter-spark-gaiauser-fedora-iris-gaia-blue-20220206-zeppelin.log ^C


    #
    # Not sure this is healthy or not ...
    #

    >   ....
    >   ....
    >    INFO [2022-02-06 14:00:07,576] ({task-result-getter-1} Logging.scala[logInfo]:57) - Finished task 22.0 in stage 979.0 (TID 1202353) in 7418 ms on worker05 (executor 40) (197/200)
    >    INFO [2022-02-06 14:00:07,576] ({task-result-getter-2} Logging.scala[logInfo]:57) - Finished task 82.0 in stage 979.0 (TID 1202413) in 7413 ms on worker05 (executor 40) (198/200)
    >    INFO [2022-02-06 14:00:07,577] ({task-result-getter-0} Logging.scala[logInfo]:57) - Finished task 52.0 in stage 979.0 (TID 1202383) in 7416 ms on worker05 (executor 40) (199/200)
    >    INFO [2022-02-06 14:00:07,577] ({task-result-getter-3} Logging.scala[logInfo]:57) - Finished task 142.0 in stage 979.0 (TID 1202473) in 7409 ms on worker05 (executor 40) (200/200)
    >    INFO [2022-02-06 14:00:07,577] ({task-result-getter-3} Logging.scala[logInfo]:57) - Removed TaskSet 979.0, whose tasks have all completed, from pool default
    >    INFO [2022-02-06 14:00:07,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - ResultStage 979 (toPandas at <stdin>:8) finished in 7.499 s
    >    INFO [2022-02-06 14:00:07,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Job 214 is finished. Cancelling potential speculative or zombie tasks for this job
    >    INFO [2022-02-06 14:00:07,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Killing all running tasks in stage 979: Stage finished
    >    INFO [2022-02-06 14:00:07,578] ({Thread-51} Logging.scala[logInfo]:57) - Job 214 finished: toPandas at <stdin>:8, took 375.467706 s
    >    WARN [2022-02-06 14:00:15,768] ({Thread-51} PooledRemoteClient.java[releaseBrokenClient]:80) - release broken client
    >    WARN [2022-02-06 14:00:15,769] ({Thread-51} PooledRemoteClient.java[releaseBrokenClient]:80) - release broken client
    >    WARN [2022-02-06 14:00:15,769] ({Thread-51} PooledRemoteClient.java[releaseBrokenClient]:80) - release broken client
    >    WARN [2022-02-06 14:00:15,770] ({Thread-51} RemoteInterpreterEventClient.java[onRemoveAngularObject]:395) - Fail to remove AngularObject
    >   java.lang.RuntimeException
    >   	at org.apache.zeppelin.interpreter.remote.PooledRemoteClient.callRemoteFunction(PooledRemoteClient.java:114)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterEventClient.callRemoteFunction(RemoteInterpreterEventClient.java:80)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterEventClient.onRemoveAngularObject(RemoteInterpreterEventClient.java:387)
    >   	at org.apache.zeppelin.display.AngularObjectRegistry.remove(AngularObjectRegistry.java:162)
    >   	at org.apache.zeppelin.display.AngularObjectRegistry.remove(AngularObjectRegistry.java:145)
    >   	at org.apache.zeppelin.interpreter.ZeppelinContext.angularUnbind(ZeppelinContext.java:909)
    >   	at org.apache.zeppelin.interpreter.ZeppelinContext.angularUnbind(ZeppelinContext.java:793)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    >   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >   	at java.lang.reflect.Method.invoke(Method.java:498)
    >   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    >   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    >   	at py4j.Gateway.invoke(Gateway.java:282)
    >   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    >   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    >   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    >   	at java.lang.Thread.run(Thread.java:748)
    >    INFO [2022-02-06 14:00:15,773] ({FIFOScheduler-interpreter_1052834727-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1644151515263_537795168 finished by scheduler interpreter_1052834727 with status FINISHED
    >    INFO [2022-02-06 14:00:16,098] ({FIFOScheduler-interpreter_1052834727-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1644151515263_308833241 started by scheduler interpreter_1052834727
    >    INFO [2022-02-06 14:00:16,102] ({FIFOScheduler-interpreter_1052834727-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1644151515263_308833241 finished by scheduler interpreter_1052834727 with status FINISHED
    >    INFO [2022-02-06 14:00:16,199] ({FIFOScheduler-interpreter_1052834727-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1644151515264_99601935 started by scheduler interpreter_1052834727
    >    INFO [2022-02-06 14:00:16,202] ({FIFOScheduler-interpreter_1052834727-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1644151515264_99601935 finished by scheduler interpreter_1052834727 with status FINISHED
    >    INFO [2022-02-06 14:01:04,387] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Requesting to kill executor(s) 6, 19
    >    INFO [2022-02-06 14:01:04,387] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Actual list of executor(s) to be killed is 6, 19
    >    INFO [2022-02-06 14:01:04,390] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Executors 6,19 removed due to idle timeout.
    >    INFO [2022-02-06 14:01:04,792] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Requesting to kill executor(s) 8
    >    INFO [2022-02-06 14:01:04,793] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Actual list of executor(s) to be killed is 8
    >    INFO [2022-02-06 14:01:04,796] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Executors 8 removed due to idle timeout.
    >   ....
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------
# Setup a SSH tunnel SOCKS proxy.
# https://www.digitalocean.com/community/tutorials/how-to-route-web-traffic-securely-without-a-vpn-using-a-socks-tunnel
# Running 'htop' on the Zeppelin node to keep the connection alive.
#[user@desktop]

    podman exec \
        --tty \
        --interactive \
        ansibler \
            bash -c \
            '
            ssh \
                -t \
                -D "3000"  \
                zeppelin \
                    "
                    htop
                    "
            '

    >   ....
    >   ....


# -----------------------------------------------------
# Login to the Zeppelin UI using FoxyProxy SOCKS proxy.
#[user@desktop]

    firefox \
        'http://zeppelin:8080/' \
        'http://master01:8088/cluster' \
        'http://monitor:3000/login' \
        &

    firefox \
        'http://monitor:3000/datasources/new' \
        'http://monitor:3000/dashboard/import' \
        'http://monitor:3000/dashboard/import' \
        &

        Create our Prometheus data source.
        http://monitor:3000/datasources/new

            URL: http://monitor:9090/
            scrape: 1s

        Import our dashboards from local disc.
        http://monitor:3000/dashboard/import

            deployments/common/grafana/20210705-02-grafana-dash.json
            deployments/common/grafana/node-exporter-v20201010-1633446087511.json

            http://monitor:3000/d/34S3C8k7z/my-first-dash&refresh=5s
            http://monitor:3000/d/xfpJB9FGz/1-node-exporter-for-prometheus-dashboard-en-v20201010?orgId=1&refresh=5s


# -----------------------------------------------------
# Check the deployment status.
#[root@ansibler]

    cat '/tmp/aglais-status.yml'

    >   aglais:
    >     status:
    >       deployment:
    >         type: hadoop-yarn
    >         conf: zeppelin-27.45-spark-6.27.45
    >         name: iris-gaia-blue-20220206
    >         date: 20220206T103959
    >     spec:
    >       openstack:
    >         cloud:
    >           base: arcus
    >           name: iris-gaia-blue


# -----------------------------------------------------
# Add the Zeppelin user accounts.
# TODO Install this fragment from a secret.
#[root@ansibler]

    ssh zeppelin

        pushd "${HOME}"
        ln -s "zeppelin-0.10.0-bin-all" "zeppelin"

            pushd "zeppelin"

                # Manual edit to add names and passwords
                vi conf/shiro.ini

                # Restart Zeppelin for the changes to take.
                bin/zeppelin-daemon.sh restart

            popd
        popd
    exit

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]


# -----------------------------------------------------
# Add the notebooks from github.
#[root@ansibler]

    ssh zeppelin

        pushd /home/fedora/zeppelin

            mv -b notebook \
               notebook-old


    >   mv: cannot move 'notebook' to 'notebook-old': Device or resource busy

    #
    # OK, this is our fault.
    # The shared notebook directory has been rolled back in the upstream version.
    # We haven't pulled the upstream changes yet.
    # So this deploy still has a shared notebook directory.
    #

    #
    # We could fix it manually, but as this is working towards a live deploy we should do this by the book.
    #


