#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Run the benchmark tests

    Result:

        Work in progress ...


# -----------------------------------------------------
# Configure out client container.
#[root@ansibler]

    dnf install git

    pip install git+https://github.com/wfau/aglais-testing@v0.2.2


# -----------------------------------------------------
# Get the IP address from the ssh config file.
# TODO Store this somewhere sensible.
#[root@ansibler]

    ipaddress=$(

        sed -n '
            /^Host zeppelin/,/^Host/ {
                /HostName/ {
                    s/^[[:space:]]*HostName[[:space:]]\(.*\)/\1/ p
                    }
                }
            ' ~/.ssh/config

        )

    echo "ipaddress [${ipaddress}]"

    >   ipaddress [128.232.222.27]


# -----------------------------------------------------
# Create a set of users
#[root@ansibler]

    testernames=(
        Rhaelhall
        Fipa
        Mythicson
        Balline
        Hiness
        Anskelisia
        Iflee
        )

    testernames=(
        Hamar
        Carclop
        )

    createarrayusers \
        ${testernames[@]} \
    | tee /tmp/testusers.json

    cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=1
        )
EOF

    python3 /tmp/testprog.py


    >   Test started [Multi User]
    >   b'Create notebook: 2H3YWFC8U\n'
    >   b'Create notebook: 2H5M5VV8Y\n'
    >   b'Create notebook: 2H594X8NB\n'
    >   b'Create notebook: 2H6KDFESA\n'
    >   Test completed! (85.68 seconds)
    >   ------------ Test Result: [PASS] ------------
    >   [{'GaiaDMPSetup': { .... }}]

    >   [
    >       {
    >       'GaiaDMPSetup': {
    >           'result': 'PASS',
    >           'outputs': {
    >               'valid': True
    >               },
    >           'time': {
    >               'result': 'FAST',
    >               'elapsed': '3.57',
    >               'expected': '45.00',
    >               'percent': '-92.06',
    >               'start': '2022-05-28T17:23:33.522687',
    >               'finish': '2022-05-28T17:23:37.096597'
    >               },
    >           'logs': ''
    >           },
    >       'Mean_proper_motions_over_the_sky': {
    >           'result': 'PASS',
    >           'outputs': {
    >               'valid': True
    >               },
    >           'time': {
    >               'result': 'SLOW',
    >               'elapsed': '55.73',
    >               'expected': '55.00',
    >               'percent': '1.33',
    >               'start': '2022-05-28T17:23:37.096811',
    >               'finish': '2022-05-28T17:24:32.826652'
    >               },
    >           'logs': ''
    >           },
    >       'Source_counts_over_the_sky.json': {
    >           'result': 'PASS',
    >           'outputs': {
    >               'valid': True
    >               },
    >           'time': {
    >               'result': 'FAST',
    >               'elapsed': '16.64',
    >               'expected': '22.00',
    >               'percent': '-24.35',
    >               'start': '2022-05-28T17:24:32.826956',
    >               'finish': '2022-05-28T17:24:49.469975'
    >               },
    >           'logs': ''
    >           },
    >       'Library_Validation.json': {
    >           'result': 'PASS',
    >           'outputs': {
    >               'valid': True
    >               },
    >           'time': {
    >               'result': 'FAST',
    >               'elapsed': '9.71',
    >               'expected': '60.00',
    >               'percent': '-83.81',
    >               'start': '2022-05-28T17:24:49.470673',
    >               'finish': '2022-05-28T17:24:59.185065'
    >               },
    >           'logs': ''
    >           }
    >       }
    >   ]


    cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=2
        )
EOF

    python3 /tmp/testprog.py



    >   Test started [Multi User]
    >   b"[Errno 2] No such file or directory: '/tmp/user2.yml'\n"
    >   b'Create notebook: 2H44G8DH3\n'
    >   ERROR:root:Expecting value: line 1 column 2 (char 1)
    >   Traceback (most recent call last):
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 126, in run_notebook
    >       json_notebook = json.loads("".join(result), strict=False)
    >     File "/usr/local/lib64/python3.10/site-packages/simplejson/__init__.py", line 542, in loads
    >       return cls(encoding=encoding, **kw).decode(s)
    >     File "/usr/local/lib64/python3.10/site-packages/simplejson/decoder.py", line 370, in decode
    >       obj, end = self.raw_decode(s)
    >     File "/usr/local/lib64/python3.10/site-packages/simplejson/decoder.py", line 400, in raw_decode
    >       return self.scan_once(s, idx=_w(s, idx).end())
    >   simplejson.errors.JSONDecodeError: Expecting value: line 1 column 2 (char 1)
    >   b"[Errno 2] No such file or directory: '/tmp/user2.yml'\n"
    >   ....
    >   ....
    >   'Create notebook: 2H4XJ46E8\n'
    >   b'Create notebook: 2H6GXEWX2\n'
    >   Test completed! (47.46 seconds)
    >   ------------ Test Result: [FAIL] ------------
    >   [{'GaiaDMPSetup': { .... }}]

    #
    # Use sed to fix the JSON output.
    # https://github.com/wfau/aglais/issues/602

    sed "
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        '/tmp/fsert534212.json' \
    | jq '.'


    >   [
    >     {
    >       "GaiaDMPSetup": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 3.65,
    >           "expected": 45.00,
    >           "percent": -91.89,
    >           "start": "2022-05-28T23:30:45.316365",
    >           "finish": "2022-05-28T23:30:48.966681"
    >         },
    >         "logs": ""
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 15.69,
    >           "expected": 55.00,
    >           "percent": -71.48,
    >           "start": "2022-05-28T23:30:48.966836",
    >           "finish": "2022-05-28T23:31:04.652350"
    >         },
    >         "logs": ""
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 21.70,
    >           "expected": 22.00,
    >           "percent": -1.36,
    >           "start": "2022-05-28T23:31:04.652754",
    >           "finish": "2022-05-28T23:31:26.354245"
    >         },
    >         "logs": ""
    >       },
    >       "Library_Validation.json": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 6.41,
    >           "expected": 60.00,
    >           "percent": -89.32,
    >           "start": "2022-05-28T23:31:26.354680",
    >           "finish": "2022-05-28T23:31:32.759701"
    >         },
    >         "logs": ""
    >       }
    >     },
    >     {
    >       "GaiaDMPSetup": {
    >         "result": "FAIL",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 1.28,
    >           "expected": 45.00,
    >           "percent": -97.16,
    >           "start": "2022-05-28T23:30:45.316417",
    >           "finish": "2022-05-28T23:30:46.592585"
    >         },
    >         "logs": ""
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "result": "FAIL",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 1.43,
    >           "expected": 55.00,
    >           "percent": -97.40,
    >           "start": "2022-05-28T23:30:46.592728",
    >           "finish": "2022-05-28T23:30:48.023202"
    >         },
    >         "logs": ""
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "result": "FAIL",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 1.21,
    >           "expected": 22.00,
    >           "percent": -94.48,
    >           "start": "2022-05-28T23:30:48.023431",
    >           "finish": "2022-05-28T23:30:49.236902"
    >         },
    >         "logs": ""
    >       },
    >       "Library_Validation.json": {
    >         "result": "FAIL",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 1.29,
    >           "expected": 60.00,
    >           "percent": -97.85,
    >           "start": "2022-05-28T23:30:49.237011",
    >           "finish": "2022-05-28T23:30:50.524897"
    >         },
    >         "logs": ""
    >       }
    >     }
    >   ]


    #
    # The first set of tests PASS.
    # The second set all FAIL.
    #

    #
    #
    # def generate_zdairi_user_configs()
    # https://github.com/wfau/aglais-testing/blob/667340fc1ae511f0fe4be1d2a76e646a6c2ba0df/aglais_benchmark/aglais_benchmark.py#L52-L71
    #
    #   Always creates one file with no number.
    #   For 3 users it will create :
    #
    #     * user.yml
    #     * user1.yml
    #     * user2.yml
    #
    # def run_notebook(
    # https://github.com/wfau/aglais-testing/blob/667340fc1ae511f0fe4be1d2a76e646a6c2ba0df/aglais_benchmark/aglais_benchmark.py#L94-L96
    #
    #   If the concurrent flag is set, looks for n user<n>.yaml files.
    #   For 3 users it will expect :
    #
    #     * user1.yml
    #     * user2.yml
    #     * user3.yml
    #

    #
    # The work around is to skip the first user, and set num_users to one less than the number in the JSON list.
    #

    testernames=(
        notused
        Lopexia
        Gonlotus
        Cheruwis
        Cheekne
        Feroona
        )

    createarrayusers \
        ${testernames[@]} \
    | tee /tmp/testusers.json \
    | jq '.users[].shirouser'


    >    jq '.users[].shirouser' /tmp/testusers.json
    >   {
    >     "name": "notused",
    >     "type": "test",
    >     "pass": "zooZ5mie1ohphobu9Ahluwamai1eex",
    >     "hash": "$shiro1$SHA-256$500000$SjXcwL0flZx2FQQpiaITGQ==$U8SqWSvBtBt29dn7tpKjE2vh3O2pDkAM4QH6RGURJOM="
    >   }
    >   {
    >     "name": "Lopexia",
    >     "type": "test",
    >     "pass": "Aiqu6japaeyae4Caekahpaay0chope",
    >     "hash": "$shiro1$SHA-256$500000$N8wZefn26nz5VN6PT7VTJA==$OuxSA1iZ9qo7MQyxnBHkr9rXTfmP9c+Bs7kHTI62cj8="
    >   }
    >   {
    >     "name": "Gonlotus",
    >     "type": "test",
    >     "pass": "phahnahs9ooqu3Uu8raikahd0Ahph3",
    >     "hash": "$shiro1$SHA-256$500000$m+2MjAeWxhCrFQlomO69Mg==$xnUm+PXq0/hWXNP5aLtHItoZKsEO/tyuqslurtHvCNs="
    >   }
    >   {
    >     "name": "Cheruwis",
    >     "type": "test",
    >     "pass": "kaph8vah4eeGei7gaib3Ahl5sodiej",
    >     "hash": "$shiro1$SHA-256$500000$7sz77HL2uYOkJsaIAP1S9Q==$XAQHWXnOt3UBtdQBInRJ7mB4+rg9g4u9EKhV/sSd/7M="
    >   }
    >   {
    >     "name": "Cheekne",
    >     "type": "test",
    >     "pass": "SeeJ6bah3vaich1Chea1ahxe3dahxi",
    >     "hash": "$shiro1$SHA-256$500000$9ym6tlBkzFMtT9mK/oI+KA==$laiJXEkw41Pk38Sm3RkHjUHh3Dtumg5clVyl8vRp6rA="
    >   }
    >   {
    >     "name": "Feroona",
    >     "type": "test",
    >     "pass": "iha7Equ2eem5Oojiechook4aa8jie7",
    >     "hash": "$shiro1$SHA-256$500000$j2pn9AAw/6uVv6Dj7YhKLA==$BKfDURrsGFrnotVxX76SLm82HKFwhcxIytpCUXtGYRc="
    >   }


    cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=5
        )
EOF

    python3 /tmp/testprog.py | tee /tmp/test-results.txt

    >   /tmp/testusers.json
    >   Test started [Multi User]
    >   b'Create notebook: 2H5JRJQAE\n'
    >   b'Create notebook: 2H6XNN9TE\n'
    >   b'Create notebook: 2H59Z6UD4\n'
    >   b'Create notebook: 2H48KNA41\n'
    >   b'Create notebook: 2H6KVHB1C\n'
    >   b'Create notebook: 2H4FXZGSB\n'
    >   b'Create notebook: 2H6YCF48A\n'
    >   b'Create notebook: 2H4F7EB18\n'
    >   b'Create notebook: 2H75B5TBD\n'
    >   b'Create notebook: 2H4QZQUWF\n'
    >   b'Create notebook: 2H3T957UG\n'
    >   b'Create notebook: 2H3Q8U7GG\n'
    >   b'Create notebook: 2H6Y1UPHJ\n'
    >   b'Create notebook: 2H6K3EJCT\n'
    >   b'Create notebook: 2H3Z5U961\n'
    >   b'Create notebook: 2H3JBP6K6\n'
    >   b'Create notebook: 2H4KRP54Z\n'
    >   b'Create notebook: 2H4E4VQHY\n'
    >   b'Create notebook: 2H3R7TY3J\n'
    >   b'Create notebook: 2H6CAWC17\n'
    >   Test completed! (84.81 seconds)
    >   ------------ Test Result: [PASS] ------------
    >   [{'GaiaDMPSetup': .... }]


    cat /tmp/user.yml

    >   zeppelin_url: http://128.232.222.27:8080
    >   zeppelin_auth: true
    >   zeppelin_user: notused
    >   zeppelin_password: zooZ5mie1ohphobu9Ahluwamai1eex


    cat /tmp/user1.yml

    >   zeppelin_url: http://128.232.222.27:8080
    >   zeppelin_auth: true
    >   zeppelin_user: Lopexia
    >   zeppelin_password: Aiqu6japaeyae4Caekahpaay0chope


    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        '/tmp/test-results.txt' \
    | jq '.' \
    | tee '/tmp/test-results.json'


[
  {
    "GaiaDMPSetup": {
      "result": "PASS",
    ....
    ....
    },
    "Mean_proper_motions_over_the_sky": {
      "result": "PASS",
    ....
    ....
    },
    "Source_counts_over_the_sky.json": {
      "result": "PASS",
    ....
    ....
    },
    "Library_Validation.json": {
      "result": "PASS",
    ....
    ....
    }
  },
  {
    "GaiaDMPSetup": {
      "result": "PASS",
    ....
    ....
    },
    "Mean_proper_motions_over_the_sky": {
      "result": "PASS",
    ....
    ....
    },
    "Source_counts_over_the_sky.json": {
      "result": "PASS",
    ....
    ....
    },
    "Library_Validation.json": {
      "result": "PASS",
    ....
    ....
    }
  },
  {
    "GaiaDMPSetup": {
      "result": "PASS",
    ....
    ....
    },
    "Mean_proper_motions_over_the_sky": {
      "result": "PASS",
    ....
    ....
    },
    "Source_counts_over_the_sky.json": {
      "result": "PASS",
    ....
    ....
    },
    "Library_Validation.json": {
      "result": "PASS",
    ....
    ....
    }
  },
  {
    "GaiaDMPSetup": {
      "result": "PASS",
    ....
    ....
    },
    "Mean_proper_motions_over_the_sky": {
      "result": "PASS",
    ....
    ....
    },
    "Source_counts_over_the_sky.json": {
      "result": "PASS",
    ....
    ....
    },
    "Library_Validation.json": {
      "result": "PASS",
    ....
    ....
    }
  },
  {
    "GaiaDMPSetup": {
      "result": "PASS",
    ....
    ....
    },
    "Mean_proper_motions_over_the_sky": {
      "result": "PASS",
    ....
    ....
    },
    "Source_counts_over_the_sky.json": {
      "result": "PASS",
    ....
    ....
    },
    "Library_Validation.json": {
      "result": "PASS",
    ....
    ....
    }
  }
]


    #
    # Filter the output down to notebook name and result.
    # No clue as to the original username ..
    #

    jq '
        .[] | keys as $x | [ $x[] as $y | {name: $y, value: .[$y].result, time: .[$y].time.elapsed } ]
        ' '/tmp/test-results.json'

    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.26
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 7.57
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 16.59
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 38.36
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.47
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.48
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 19.82
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 19.84
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.45
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.59
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 18.50
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 36.50
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.49
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 7.35
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 19.86
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 53.08
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.51
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 7.39
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 14.53
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 55.23
    >     }
    >   ]


# -----------------------------------------------------
# Going for it ....
#[root@ansibler]

    count=10

    testernames=()

    for i in $(seq $((count+1)))
    do
        testernames+=($(pwgen 12 1))
    done

    createarrayusers \
        ${testernames[@]} \
    | tee /tmp/testusers.json \
    | jq '[ .users[].shirouser.name ]'

    >   [
    >     "gah3ohL2ja8j",
    >     "aiZ6haejeeth", -
    >     "IeGhee0ia9wo", -
    >     "thoon6Aethee", -
    >     "Ye4kuseix5Ei", -
    >     "aHeeseevo2oo", -
    >     "bap0iezaaK3f", -
    >     "shu9Quoog6oi", -
    >     "cie5pae0OhXa", -
    >     "Mei8Thosebuv", -
    >     "Lethiag2uo0s"
    >   ]


    cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=10
        )
EOF

    python3 /tmp/testprog.py | tee /tmp/test-results.txt


# -----------------------------------------------------
# Seems to be stuck ....
#[root@ansibler]

    ssh zeppelin
        pushd zeppelin/logs
            tail -f zeppelin-interpreter-spark-thoon6Aethee-thoon6Aethee-fedora-iris-gaia-blue-20220528-zeppelin.log

    >   ....
    >    INFO [2022-05-29 01:29:43,526] ({FIFOScheduler-interpreter_1034317388-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0015 (state: ACCEPTED)
    >    INFO [2022-05-29 01:29:44,527] ({FIFOScheduler-interpreter_1034317388-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0015 (state: ACCEPTED)
    >    INFO [2022-05-29 01:29:45,528] ({FIFOScheduler-interpreter_1034317388-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0015 (state: ACCEPTED)
    >    INFO [2022-05-29 01:29:46,530] ({FIFOScheduler-interpreter_1034317388-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0015 (state: ACCEPTED)
    >    INFO [2022-05-29 01:29:47,530] ({FIFOScheduler-interpreter_1034317388-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0015 (state: ACCEPTED)
    >    INFO [2022-05-29 01:29:48,532] ({FIFOScheduler-interpreter_1034317388-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0015 (state: ACCEPTED)
    >   ....


# -----------------------------------------------------
# Restart zeppelin.
#[user@zeppelin]

    zeppelin-daemon.sh restart

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]


# -----------------------------------------------------
# Going for it ....
#[root@ansibler]

    count=8

    testernames=()

    for i in $(seq $((count+1)))
    do
        testernames+=($(pwgen 12 1))
    done

    createarrayusers \
        ${testernames[@]} \
    | tee /tmp/testusers.json \
    | jq '[ .users[].shirouser.name ]'

    >   [
    >     "nen4iegiNg8z",
    >     "uReequoo6phe",
    >     "jeeBui2cheeC",
    >     "il0OKeniequu",
    >     "voo2iechohZi",
    >     "ijeiNguph4UV",
    >     "UghooPohm4oh",
    >     "Uu9ongeolauz",
    >     "yeehahPhoo7E"
    >   ]


    cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=8
        )
EOF

    python3 /tmp/testprog.py | tee /tmp/test-results.txt


    #
    # Locks up again.
    #


# -----------------------------------------------------
# Restart zeppelin.
#[user@zeppelin]

    zeppelin-daemon.sh restart

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]


# -----------------------------------------------------
# Try just 4 users ..
#[root@ansibler]

        cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=4
        )
EOF

    python3 /tmp/testprog.py | tee /tmp/test-results.txt

    >   Test started [Multi User]
    >   ERROR:root:list index out of range
    >   Traceback (most recent call last):
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 114, in run_notebook
    >       notebookid = text.split(": ")[1]
    >   IndexError: list index out of range
    >   b'status_code:500\n'
    >   b'Create notebook: 2H3J3WYG2\n'
    >   b'Create notebook: 2H4GW7RMA\n'
    >   b'Create notebook: 2H6B1XH2U\n'
    >   b'Create notebook: 2H4HBBF72\n'
    >   b'Create notebook: 2H5TDQCWE\n'
    >   b'Create notebook: 2H633ASVB\n'
    >   b'Create notebook: 2H4UK7XVS\n'
    >   b'Create notebook: 2H6HFAP9K\n'
    >   b'Create notebook: 2H5PY9U4K\n'
    >   b'Create notebook: 2H66SJ75H\n'
    >   b'Create notebook: 2H3V4F94M\n'
    >   b'Create notebook: 2H5C472PW\n'
    >   b'Create notebook: 2H3QSET52\n'
    >   b'Create notebook: 2H45P57J1\n'
    >   b'Create notebook: 2H5ATDN8K\n'
    >   Test completed! (71.94 seconds)
    >   ------------ Test Result: [ERROR] ------------
    >   [{'GaiaDMPSetup': {....}}]


    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        '/tmp/test-results.txt' \
    | jq '.' \
    | tee '/tmp/test-results.json'

    >   [
    >     {
    >       "GaiaDMPSetup": {
    >         "result": "ERROR",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 5.77,
    >           "expected": 45.00,
    >           "percent": -87.18,
    >           "start": "2022-05-29T02:09:28.849238",
    >           "finish": "2022-05-29T02:09:34.616430"
    >         },
    >         "logs": "Unexpected exception: java.util.ConcurrentModificationException
    >       at java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1633)
    >       at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    >       at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    >       at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    >       at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    >       at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    >       at org.apache.zeppelin.service.JobManagerService.getNoteJobInfoByUnixTime(JobManagerService.java:90)
    >       at org.apache.zeppelin.socket.NotebookServer.broadcastUpdateNoteJobInfo(NotebookServer.java:519)
    >       at org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:2007)
    >       at org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:105)
    >       at org.apache.zeppelin.scheduler.Job.setStatus(Job.java:141)
    >       at org.apache.zeppelin.notebook.Paragraph.setStatus(Paragraph.java:398)
    >       at org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:349)
    >       at org.apache.zeppelin.notebook.Note.run(Note.java:873)
    >       at org.apache.zeppelin.service.NotebookService.runParagraph(NotebookService.java:390)
    >       at org.apache.zeppelin.rest.NotebookRestApi.runParagraph(NotebookRestApi.java:849)
    >       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    >       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    >       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >       at java.lang.reflect.Method.invoke(Method.java:498)
    >       at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
    >       at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
    >       at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
    >       at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
    >       at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
    >       at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:469)
    >       at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:391)
    >       at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:80)
    >       at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:253)
    >       at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
    >       at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
    >       at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
    >       at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
    >       at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
    >       at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
    >       at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)
    >       at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
    >       at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
    >       at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
    >       at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
    >       at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
    >       at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
    >       at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:763)
    >       at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1651)
    >       at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)
    >       at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
    >       at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
    >       at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
    >       at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
    >       at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
    >       at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
    >       at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
    >       at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
    >       at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:450)
    >       at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
    >       at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
    >       at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
    >       at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:387)
    >       at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
    >       at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
    >       at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)
    >       at org.apache.zeppelin.server.CorsFilter.doFilter(CorsFilter.java:64)
    >       at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)
    >       at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:567)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
    >       at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
    >       at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
    >       at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1610)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
    >       at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1377)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
    >       at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:507)
    >       at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1580)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
    >       at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1292)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
    >       at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
    >       at io.micrometer.core.instrument.binder.jetty.TimedHandler.handle(TimedHandler.java:120)
    >       at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    >       at org.eclipse.jetty.server.Server.handle(Server.java:501)
    >       at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
    >       at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:556)
    >       at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
    >       at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
    >       at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
    >       at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
    >       at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    >       at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
    >       at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
    >       at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
    >       at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)
    >       at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
    >       at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
    >       at java.lang.Thread.run(Thread.java:748)"
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "result": "ERROR",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 48.67,
    >           "expected": 55.00,
    >           "percent": -11.51,
    >           "start": "2022-05-29T02:09:34.616659",
    >           "finish": "2022-05-29T02:10:23.287323"
    >         },
    >         "logs": "Fail to execute line 13: df = spark.sql(query).cache()
    >   Traceback (most recent call last):
    >     File #/tmp/1653790218977-0/zeppelin_python.py#, line 158, in <module>
    >       exec(code, _zcUserQueryNameSpace)
    >     File #<stdin>#, line 13, in <module>
    >     File #/opt/spark/python/pyspark/sql/session.py#, line 723, in sql
    >       return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
    >     File #/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py#, line 1305, in __call__
    >       answer, self.gateway_client, self.target_id, self.name)
    >     File #/opt/spark/python/pyspark/sql/utils.py#, line 117, in deco
    >       raise converted from None
    >   pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 121;
    >   \"Aggregate [\"hpx_id], [\"floor((\"source_id / 140737488355328)) AS hpx_id#0, count(1) AS n#1L, \"AVG(\"pmra) AS avg_pmra#2, \"AVG(\"pmdec) AS avg_pmdec#3]
    >   +- \"UnresolvedRelation [gaia_source], [], false"
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "result": "ERROR",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 5.67,
    >           "expected": 22.00,
    >           "percent": -74.23,
    >           "start": "2022-05-29T02:10:23.287594",
    >           "finish": "2022-05-29T02:10:28.957412"
    >         },
    >         "logs": "Fail to execute line 21: df = spark.sql(#SELECT FLOOR(source_id / %d#%(divisor) + #) AS hpx_id, COUNT(*) AS n FROM gaia_source GROUP BY hpx_id#)
    >   Traceback (most recent call last):
    >     File #/tmp/1653790218977-0/zeppelin_python.py#, line 158, in <module>
    >       exec(code, _zcUserQueryNameSpace)
    >     File #<stdin>#, line 21, in <module>
    >     File #/opt/spark/python/pyspark/sql/session.py#, line 723, in sql
    >       return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
    >     File #/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py#, line 1305, in __call__
    >       answer, self.gateway_client, self.target_id, self.name)
    >     File #/opt/spark/python/pyspark/sql/utils.py#, line 117, in deco
    >       raise converted from None
    >   pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 72;
    >   \"Aggregate [\"hpx_id], [\"FLOOR((\"source_id / 140737488355328)) AS hpx_id#5, count(1) AS n#6L]
    >   +- \"UnresolvedRelation [gaia_source], [], false"
    >       },
    >       "Library_Validation.json": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 10.67,
    >           "expected": 60.00,
    >           "percent": -82.22,
    >           "start": "2022-05-29T02:10:28.957568",
    >           "finish": "2022-05-29T02:10:39.624435"
    >         },
    >         "logs": ""
    >       }
    >     },
    >     {
    >       "GaiaDMPSetup": {
    >         "result": "FAIL",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 1.64,
    >           "expected": 45.00,
    >           "percent": -96.37,
    >           "start": "2022-05-29T02:09:28.849396",
    >           "finish": "2022-05-29T02:09:30.484690"
    >         },
    >         "logs": ""
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "result": "ERROR",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 33.50,
    >           "expected": 55.00,
    >           "percent": -39.10,
    >           "start": "2022-05-29T02:09:30.484833",
    >           "finish": "2022-05-29T02:10:03.980781"
    >         },
    >         "logs": "Fail to execute line 13: df = spark.sql(query).cache()
    >   Traceback (most recent call last):
    >     File #/tmp/1653790200151-0/zeppelin_python.py#, line 158, in <module>
    >       exec(code, _zcUserQueryNameSpace)
    >     File #<stdin>#, line 13, in <module>
    >     File #/opt/spark/python/pyspark/sql/session.py#, line 723, in sql
    >       return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
    >     File #/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py#, line 1305, in __call__
    >       answer, self.gateway_client, self.target_id, self.name)
    >     File #/opt/spark/python/pyspark/sql/utils.py#, line 117, in deco
    >       raise converted from None
    >   pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 121;
    >   \"Aggregate [\"hpx_id], [\"floor((\"source_id / 140737488355328)) AS hpx_id#0, count(1) AS n#1L, \"AVG(\"pmra) AS avg_pmra#2, \"AVG(\"pmdec) AS avg_pmdec#3]
    >   +- \"UnresolvedRelation [gaia_source], [], false"
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "result": "ERROR",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 5.60,
    >           "expected": 22.00,
    >           "percent": -74.57,
    >           "start": "2022-05-29T02:10:03.980964",
    >           "finish": "2022-05-29T02:10:09.576065"
    >         },
    >         "logs": "Fail to execute line 21: df = spark.sql(#SELECT FLOOR(source_id / %d#%(divisor) + #) AS hpx_id, COUNT(*) AS n FROM gaia_source GROUP BY hpx_id#)
    >   Traceback (most recent call last):
    >     File #/tmp/1653790200151-0/zeppelin_python.py#, line 158, in <module>
    >       exec(code, _zcUserQueryNameSpace)
    >     File #<stdin>#, line 21, in <module>
    >     File #/opt/spark/python/pyspark/sql/session.py#, line 723, in sql
    >       return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
    >     File #/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py#, line 1305, in __call__
    >       answer, self.gateway_client, self.target_id, self.name)
    >     File #/opt/spark/python/pyspark/sql/utils.py#, line 117, in deco
    >       raise converted from None
    >   pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 72;
    >   \"Aggregate [\"hpx_id], [\"FLOOR((\"source_id / 140737488355328)) AS hpx_id#5, count(1) AS n#6L]
    >   +- \"UnresolvedRelation [gaia_source], [], false"
    >       },
    >       "Library_Validation.json": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 10.72,
    >           "expected": 60.00,
    >           "percent": -82.14,
    >           "start": "2022-05-29T02:10:09.576317",
    >           "finish": "2022-05-29T02:10:20.292080"
    >         },
    >         "logs": ""
    >       }
    >     },
    >     {
    >       "GaiaDMPSetup": {
    >         "result": "ERROR",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 5.76,
    >           "expected": 45.00,
    >           "percent": -87.21,
    >           "start": "2022-05-29T02:09:28.849463",
    >           "finish": "2022-05-29T02:09:34.605412"
    >         },
    >         "logs": "Unexpected exception: java.util.ConcurrentModificationException
    >       at java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1633)
    >       at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    >       at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    >       at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    >       at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    >       at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    >       at org.apache.zeppelin.service.JobManagerService.getNoteJobInfoByUnixTime(JobManagerService.java:90)
    >       at org.apache.zeppelin.socket.NotebookServer.broadcastUpdateNoteJobInfo(NotebookServer.java:519)
    >       at org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:2007)
    >       at org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:105)
    >       at org.apache.zeppelin.scheduler.Job.setStatus(Job.java:141)
    >       at org.apache.zeppelin.notebook.Paragraph.setStatus(Paragraph.java:398)
    >       at org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:349)
    >       at org.apache.zeppelin.notebook.Note.run(Note.java:873)
    >       at org.apache.zeppelin.service.NotebookService.runParagraph(NotebookService.java:390)
    >       at org.apache.zeppelin.rest.NotebookRestApi.runParagraph(NotebookRestApi.java:849)
    >       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    >       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    >       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >       at java.lang.reflect.Method.invoke(Method.java:498)
    >       at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
    >       at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
    >       at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
    >       at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
    >       at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
    >       at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:469)
    >       at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:391)
    >       at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:80)
    >       at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:253)
    >       at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
    >       at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
    >       at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
    >       at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
    >       at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
    >       at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
    >       at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)
    >       at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
    >       at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
    >       at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
    >       at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
    >       at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
    >       at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
    >       at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:763)
    >       at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1651)
    >       at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)
    >       at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
    >       at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
    >       at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
    >       at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
    >       at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
    >       at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
    >       at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
    >       at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
    >       at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:450)
    >       at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
    >       at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
    >       at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
    >       at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:387)
    >       at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
    >       at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
    >       at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)
    >       at org.apache.zeppelin.server.CorsFilter.doFilter(CorsFilter.java:64)
    >       at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)
    >       at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:567)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
    >       at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
    >       at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
    >       at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1610)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
    >       at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1377)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
    >       at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:507)
    >       at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1580)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
    >       at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1292)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
    >       at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
    >       at io.micrometer.core.instrument.binder.jetty.TimedHandler.handle(TimedHandler.java:120)
    >       at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    >       at org.eclipse.jetty.server.Server.handle(Server.java:501)
    >       at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
    >       at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:556)
    >       at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
    >       at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
    >       at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
    >       at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
    >       at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    >       at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
    >       at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
    >       at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
    >       at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)
    >       at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
    >       at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
    >       at java.lang.Thread.run(Thread.java:748)"
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "result": "ERROR",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 48.69,
    >           "expected": 55.00,
    >           "percent": -11.47,
    >           "start": "2022-05-29T02:09:34.605562",
    >           "finish": "2022-05-29T02:10:23.298509"
    >         },
    >         "logs": "Fail to execute line 13: df = spark.sql(query).cache()
    >   Traceback (most recent call last):
    >     File #/tmp/1653790218646-0/zeppelin_python.py#, line 158, in <module>
    >       exec(code, _zcUserQueryNameSpace)
    >     File #<stdin>#, line 13, in <module>
    >     File #/opt/spark/python/pyspark/sql/session.py#, line 723, in sql
    >       return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
    >     File #/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py#, line 1305, in __call__
    >       answer, self.gateway_client, self.target_id, self.name)
    >     File #/opt/spark/python/pyspark/sql/utils.py#, line 117, in deco
    >       raise converted from None
    >   pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 121;
    >   \"Aggregate [\"hpx_id], [\"floor((\"source_id / 140737488355328)) AS hpx_id#0, count(1) AS n#1L, \"AVG(\"pmra) AS avg_pmra#2, \"AVG(\"pmdec) AS avg_pmdec#3]
    >   +- \"UnresolvedRelation [gaia_source], [], false"
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "result": "ERROR",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 5.68,
    >           "expected": 22.00,
    >           "percent": -74.17,
    >           "start": "2022-05-29T02:10:23.298635",
    >           "finish": "2022-05-29T02:10:28.980349"
    >         },
    >         "logs": "Fail to execute line 21: df = spark.sql(#SELECT FLOOR(source_id / %d#%(divisor) + #) AS hpx_id, COUNT(*) AS n FROM gaia_source GROUP BY hpx_id#)
    >   Traceback (most recent call last):
    >     File #/tmp/1653790218646-0/zeppelin_python.py#, line 158, in <module>
    >       exec(code, _zcUserQueryNameSpace)
    >     File #<stdin>#, line 21, in <module>
    >     File #/opt/spark/python/pyspark/sql/session.py#, line 723, in sql
    >       return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
    >     File #/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py#, line 1305, in __call__
    >       answer, self.gateway_client, self.target_id, self.name)
    >     File #/opt/spark/python/pyspark/sql/utils.py#, line 117, in deco
    >       raise converted from None
    >   pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 72;
    >   \"Aggregate [\"hpx_id], [\"FLOOR((\"source_id / 140737488355328)) AS hpx_id#5, count(1) AS n#6L]
    >   +- \"UnresolvedRelation [gaia_source], [], false"
    >       },
    >       "Library_Validation.json": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 11.74,
    >           "expected": 60.00,
    >           "percent": -80.43,
    >           "start": "2022-05-29T02:10:28.980473",
    >           "finish": "2022-05-29T02:10:40.723101"
    >         },
    >         "logs": ""
    >       }
    >     },
    >     {
    >       "GaiaDMPSetup": {
    >         "result": "ERROR",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 5.75,
    >           "expected": 45.00,
    >           "percent": -87.21,
    >           "start": "2022-05-29T02:09:28.849526",
    >           "finish": "2022-05-29T02:09:34.604178"
    >         },
    >         "logs": "Unexpected exception: java.util.ConcurrentModificationException
    >       at java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1633)
    >       at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    >       at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    >       at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    >       at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    >       at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    >       at org.apache.zeppelin.service.JobManagerService.getNoteJobInfoByUnixTime(JobManagerService.java:90)
    >       at org.apache.zeppelin.socket.NotebookServer.broadcastUpdateNoteJobInfo(NotebookServer.java:519)
    >       at org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:2007)
    >       at org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:105)
    >       at org.apache.zeppelin.scheduler.Job.setStatus(Job.java:141)
    >       at org.apache.zeppelin.notebook.Paragraph.setStatus(Paragraph.java:398)
    >       at org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:349)
    >       at org.apache.zeppelin.notebook.Note.run(Note.java:873)
    >       at org.apache.zeppelin.service.NotebookService.runParagraph(NotebookService.java:390)
    >       at org.apache.zeppelin.rest.NotebookRestApi.runParagraph(NotebookRestApi.java:849)
    >       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    >       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    >       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >       at java.lang.reflect.Method.invoke(Method.java:498)
    >       at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
    >       at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
    >       at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
    >       at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
    >       at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
    >       at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:469)
    >       at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:391)
    >       at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:80)
    >       at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:253)
    >       at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
    >       at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
    >       at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
    >       at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
    >       at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
    >       at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
    >       at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)
    >       at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
    >       at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
    >       at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
    >       at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
    >       at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
    >       at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
    >       at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:763)
    >       at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1651)
    >       at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)
    >       at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
    >       at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
    >       at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
    >       at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
    >       at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
    >       at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
    >       at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
    >       at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
    >       at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:450)
    >       at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
    >       at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
    >       at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
    >       at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:387)
    >       at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
    >       at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
    >       at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)
    >       at org.apache.zeppelin.server.CorsFilter.doFilter(CorsFilter.java:64)
    >       at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)
    >       at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:567)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
    >       at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)
    >       at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)
    >       at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1610)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
    >       at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1377)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
    >       at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:507)
    >       at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1580)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
    >       at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1292)
    >       at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
    >       at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
    >       at io.micrometer.core.instrument.binder.jetty.TimedHandler.handle(TimedHandler.java:120)
    >       at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
    >       at org.eclipse.jetty.server.Server.handle(Server.java:501)
    >       at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
    >       at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:556)
    >       at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
    >       at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)
    >       at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
    >       at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
    >       at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    >       at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
    >       at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
    >       at java.lang.Thread.run(Thread.java:748)"
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "result": "ERROR",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 48.69,
    >           "expected": 55.00,
    >           "percent": -11.47,
    >           "start": "2022-05-29T02:09:34.604399",
    >           "finish": "2022-05-29T02:10:23.298149"
    >         },
    >         "logs": "Fail to execute line 13: df = spark.sql(query).cache()
    >   Traceback (most recent call last):
    >     File #/tmp/1653790218600-0/zeppelin_python.py#, line 158, in <module>
    >       exec(code, _zcUserQueryNameSpace)
    >     File #<stdin>#, line 13, in <module>
    >     File #/opt/spark/python/pyspark/sql/session.py#, line 723, in sql
    >       return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
    >     File #/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py#, line 1305, in __call__
    >       answer, self.gateway_client, self.target_id, self.name)
    >     File #/opt/spark/python/pyspark/sql/utils.py#, line 117, in deco
    >       raise converted from None
    >   pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 121;
    >   \"Aggregate [\"hpx_id], [\"floor((\"source_id / 140737488355328)) AS hpx_id#0, count(1) AS n#1L, \"AVG(\"pmra) AS avg_pmra#2, \"AVG(\"pmdec) AS avg_pmdec#3]
    >   +- \"UnresolvedRelation [gaia_source], [], false"
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "result": "ERROR",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 5.67,
    >           "expected": 22.00,
    >           "percent": -74.21,
    >           "start": "2022-05-29T02:10:23.298297",
    >           "finish": "2022-05-29T02:10:28.972055"
    >         },
    >         "logs": "Fail to execute line 21: df = spark.sql(#SELECT FLOOR(source_id / %d#%(divisor) + #) AS hpx_id, COUNT(*) AS n FROM gaia_source GROUP BY hpx_id#)
    >   Traceback (most recent call last):
    >     File #/tmp/1653790218600-0/zeppelin_python.py#, line 158, in <module>
    >       exec(code, _zcUserQueryNameSpace)
    >     File #<stdin>#, line 21, in <module>
    >     File #/opt/spark/python/pyspark/sql/session.py#, line 723, in sql
    >       return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
    >     File #/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py#, line 1305, in __call__
    >       answer, self.gateway_client, self.target_id, self.name)
    >     File #/opt/spark/python/pyspark/sql/utils.py#, line 117, in deco
    >       raise converted from None
    >   pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 72;
    >   \"Aggregate [\"hpx_id], [\"FLOOR((\"source_id / 140737488355328)) AS hpx_id#5, count(1) AS n#6L]
    >   +- \"UnresolvedRelation [gaia_source], [], false"
    >       },
    >       "Library_Validation.json": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 11.79,
    >           "expected": 60.00,
    >           "percent": -80.35,
    >           "start": "2022-05-29T02:10:28.972169",
    >           "finish": "2022-05-29T02:10:40.763140"
    >         },
    >         "logs": ""
    >       }
    >     }
    >   ]


# -----------------------------------------------------

    #
    # Is this a problem - there is only one value for 'hive.metastore.warehouse.dir'

    >    ....
    >    INFO [2022-05-29 02:46:00,525] ({Thread-47} Logging.scala[logInfo]:57) - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/fedora/zeppelin-0.10.0-bin-all/logs/spark-warehouse/').
    >    INFO [2022-05-29 02:46:00,525] ({Thread-47} Logging.scala[logInfo]:57) - Warehouse path is 'file:/home/fedora/zeppelin-0.10.0-bin-all/logs/spark-warehouse/'.
    >    ....

    https://docs.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_hive_metastore_configure.html
    https://spark.apache.org/docs/latest/sql-programming-guide.html


# -----------------------------------------------------
# Try just 1 user ...
#[root@ansibler]

        cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=1
        )
EOF

    python3 /tmp/testprog.py | tee /tmp/test-results.txt

    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        '/tmp/test-results.txt' \
    | jq '.' \
    | tee '/tmp/test-results.json'

    >   [
    >     {
    >       "GaiaDMPSetup": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 37.80,
    >           "expected": 45.00,
    >           "percent": -15.99,
    >           "start": "2022-05-29T02:42:12.662512",
    >           "finish": "2022-05-29T02:42:50.465931"
    >         },
    >         "logs": ""
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 31.66,
    >           "expected": 55.00,
    >           "percent": -42.44,
    >           "start": "2022-05-29T02:42:50.466131",
    >           "finish": "2022-05-29T02:43:22.123969"
    >         },
    >         "logs": ""
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 11.46,
    >           "expected": 22.00,
    >           "percent": -47.93,
    >           "start": "2022-05-29T02:43:22.124463",
    >           "finish": "2022-05-29T02:43:33.579529"
    >         },
    >         "logs": ""
    >       },
    >       "Library_Validation.json": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 9.73,
    >           "expected": 60.00,
    >           "percent": -83.78,
    >           "start": "2022-05-29T02:43:33.580204",
    >           "finish": "2022-05-29T02:43:43.309336"
    >         },
    >         "logs": ""
    >       }
    >     }
    >   ]


# -----------------------------------------------------
# Try just 2 users ...
#[root@ansibler]

        cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=2
        )
EOF

    python3 /tmp/testprog.py | tee /tmp/test-results.txt

    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        '/tmp/test-results.txt' \
    | jq '.' \
    | tee '/tmp/test-results.json'

    >   [
    >     {
    >       "GaiaDMPSetup": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 3.47,
    >           "expected": 45.00,
    >           "percent": -92.29,
    >           "start": "2022-05-29T02:45:16.899789",
    >           "finish": "2022-05-29T02:45:20.369568"
    >         },
    >         "logs": ""
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 14.79,
    >           "expected": 55.00,
    >           "percent": -73.11,
    >           "start": "2022-05-29T02:45:20.369699",
    >           "finish": "2022-05-29T02:45:35.158168"
    >         },
    >         "logs": ""
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 16.55,
    >           "expected": 22.00,
    >           "percent": -24.79,
    >           "start": "2022-05-29T02:45:35.158521",
    >           "finish": "2022-05-29T02:45:51.704844"
    >         },
    >         "logs": ""
    >       },
    >       "Library_Validation.json": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 6.46,
    >           "expected": 60.00,
    >           "percent": -89.23,
    >           "start": "2022-05-29T02:45:51.705680",
    >           "finish": "2022-05-29T02:45:58.168148"
    >         },
    >         "logs": ""
    >       }
    >     },
    >     {
    >       "GaiaDMPSetup": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "SLOW",
    >           "elapsed": 50.52,
    >           "expected": 45.00,
    >           "percent": 12.27,
    >           "start": "2022-05-29T02:45:16.899912",
    >           "finish": "2022-05-29T02:46:07.419354"
    >         },
    >         "logs": ""
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 32.47,
    >           "expected": 55.00,
    >           "percent": -40.97,
    >           "start": "2022-05-29T02:46:07.419633",
    >           "finish": "2022-05-29T02:46:39.887389"
    >         },
    >         "logs": ""
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 14.60,
    >           "expected": 22.00,
    >           "percent": -33.63,
    >           "start": "2022-05-29T02:46:39.887655",
    >           "finish": "2022-05-29T02:46:54.488725"
    >         },
    >         "logs": ""
    >       },
    >       "Library_Validation.json": {
    >         "result": "PASS",
    >         "outputs": {
    >           "valid": "True"
    >         },
    >         "time": {
    >           "result": "FAST",
    >           "elapsed": 9.63,
    >           "expected": 60.00,
    >           "percent": -83.95,
    >           "start": "2022-05-29T02:46:54.489729",
    >           "finish": "2022-05-29T02:47:04.117072"
    >         },
    >         "logs": ""
    >       }
    >     }
    >   ]


# -----------------------------------------------------
# Try 3 users ...
#[root@ansibler]

        cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=3
        )
EOF

    python3 /tmp/testprog.py | tee /tmp/test-results.txt

    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        '/tmp/test-results.txt' \
    | jq '.' \
    | tee '/tmp/test-results.json'

    jq '
        .[] | keys as $x | [ $x[] as $y | {name: $y, value: .[$y].result, time: .[$y].time.elapsed } ]
        ' '/tmp/test-results.json'

    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.70
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.84
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 15.95
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 18.72
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.68
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.50
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 18.10
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 20.72
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 49.60
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 9.51
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 76.33
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 11.23
    >     }
    >   ]


# -----------------------------------------------------
# Try 4 users ...
#[root@ansibler]

        cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=4
        )
EOF

    python3 /tmp/testprog.py | tee /tmp/test-results.txt

    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        '/tmp/test-results.txt' \
    | jq '.' \
    | tee '/tmp/test-results.json'

    jq '
        .[] | keys as $x | [ $x[] as $y | {name: $y, value: .[$y].result, time: .[$y].time.elapsed } ]
        ' '/tmp/test-results.json'

    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.81
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.40
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 17.26
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 15.58
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.83
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.45
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 16.34
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "ERROR",
    >       "time": 2.57
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.80
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.40
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 16.05
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 45.93
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 85.05
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 9.64
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 39.12
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 13.59
    >     }
    >   ]


# -----------------------------------------------------
# Try 5 users ...
#[root@ansibler]

        cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=5
        )
EOF

    python3 /tmp/testprog.py | tee /tmp/test-results.txt

    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        '/tmp/test-results.txt' \
    | jq '.' \
    | tee '/tmp/test-results.json'

    jq '
        .[] | keys as $x | [ $x[] as $y | {name: $y, value: .[$y].result, time: .[$y].time.elapsed } ]
        ' '/tmp/test-results.json'

    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.25
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 7.32
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 17.31
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 20.79
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "FAIL",
    >       "time": 1.34
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 7.33
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.88
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 41.53
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.35
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 7.37
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.91
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 85.20
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.36
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 7.40
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 17.32
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 24.95
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 110.36
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 9.38
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 32.58
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 11.39
    >     }
    >   ]





# -----------------------------------------------------
# Try 6 users ...
#[root@ansibler]

        cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=6
        )
EOF

    python3 /tmp/testprog.py | tee /tmp/test-results.txt

    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        '/tmp/test-results.txt' \
    | jq '.' \
    | tee '/tmp/test-results.json'

    jq '
        .[] | keys as $x | [ $x[] as $y | {name: $y, value: .[$y].result, time: .[$y].time.elapsed } ]
        ' '/tmp/test-results.json'

    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.44
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.55
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 18.52
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 19.76
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.41
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.42
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 18.58
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 32.27
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.45
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.33
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 5.95
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 81.44
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.46
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.48
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 16.41
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 35.40
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.10
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.40
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 16.58
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 38.64
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 108.36
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 9.50
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 39.95
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 11.37
    >     }
    >   ]


# -----------------------------------------------------
# Try 7 users ...
#[root@ansibler]

        cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=7
        )
EOF

    python3 /tmp/testprog.py | tee /tmp/test-results.txt

    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        '/tmp/test-results.txt' \
    | jq '.' \
    | tee '/tmp/test-results.json'

    jq '
        .[] | keys as $x | [ $x[] as $y | {name: $y, value: .[$y].result, time: .[$y].time.elapsed } ]
        ' '/tmp/test-results.json'


    >   Test started [Multi User]
    >   ERROR:root:list index out of range
    >   Traceback (most recent call last):
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 114, in run_notebook
    >       notebookid = text.split(": ")[1]
    >   IndexError: list index out of range
    >   ....
    >   ....

# -----------------------------------------------------

    #
    # Watching htop on zeppelin and worker nodes.
    # Huge long gaps when no one is doing anything before the test completes.
    # ???

# -----------------------------------------------------

    #
    # Errors in Zeppelin logs.
    # These don't look good :-(
    #

    >   ....
    >   ....
    >    INFO [2022-05-29 03:17:14,230] ({task-result-getter-2} Logging.scala[logInfo]:57) - Finished task 48.0 in stage 69.0 (TID 38380) in 2118 ms on worker03 (executor 99) (197/200)
    >    INFO [2022-05-29 03:17:14,301] ({task-result-getter-3} Logging.scala[logInfo]:57) - Finished task 9.0 in stage 69.0 (TID 38341) in 2194 ms on worker03 (executor 99) (198/200)
    >    INFO [2022-05-29 03:17:14,323] ({task-result-getter-0} Logging.scala[logInfo]:57) - Finished task 35.0 in stage 69.0 (TID 38367) in 2213 ms on worker03 (executor 99) (199/200)
    >    INFO [2022-05-29 03:17:14,331] ({task-result-getter-1} Logging.scala[logInfo]:57) - Finished task 61.0 in stage 69.0 (TID 38393) in 2218 ms on worker03 (executor 99) (200/200)
    >    INFO [2022-05-29 03:17:14,331] ({task-result-getter-1} Logging.scala[logInfo]:57) - Removed TaskSet 69.0, whose tasks have all completed, from pool default
    >    INFO [2022-05-29 03:17:14,332] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - ResultStage 69 (collect at <stdin>:17) finished in 2.230 s
    >    INFO [2022-05-29 03:17:14,332] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
    >    INFO [2022-05-29 03:17:14,332] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Killing all running tasks in stage 69: Stage finished
    >    INFO [2022-05-29 03:17:14,332] ({Thread-52} Logging.scala[logInfo]:57) - Job 48 finished: collect at <stdin>:17, took 14.029805 s
    >    WARN [2022-05-29 03:17:15,168] ({Thread-52} PooledRemoteClient.java[releaseBrokenClient]:80) - release broken client
    >    WARN [2022-05-29 03:17:15,170] ({Thread-52} PooledRemoteClient.java[releaseBrokenClient]:80) - release broken client
    >    WARN [2022-05-29 03:17:15,170] ({Thread-52} PooledRemoteClient.java[releaseBrokenClient]:80) - release broken client
    >    WARN [2022-05-29 03:17:15,170] ({Thread-52} RemoteInterpreterEventClient.java[onRemoveAngularObject]:395) - Fail to remove AngularObject
    >   java.lang.RuntimeException
    >   	at org.apache.zeppelin.interpreter.remote.PooledRemoteClient.callRemoteFunction(PooledRemoteClient.java:114)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterEventClient.callRemoteFunction(RemoteInterpreterEventClient.java:80)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterEventClient.onRemoveAngularObject(RemoteInterpreterEventClient.java:387)
    >   	at org.apache.zeppelin.display.AngularObjectRegistry.remove(AngularObjectRegistry.java:162)
    >   	at org.apache.zeppelin.display.AngularObjectRegistry.remove(AngularObjectRegistry.java:145)
    >   	at org.apache.zeppelin.interpreter.ZeppelinContext.angularUnbind(ZeppelinContext.java:909)
    >   	at org.apache.zeppelin.interpreter.ZeppelinContext.angularUnbind(ZeppelinContext.java:793)
    >   	at sun.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)
    >   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >   	at java.lang.reflect.Method.invoke(Method.java:498)
    >   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    >   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    >   	at py4j.Gateway.invoke(Gateway.java:282)
    >   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    >   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    >   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    >   	at java.lang.Thread.run(Thread.java:748)
    >    INFO [2022-05-29 03:17:15,179] ({FIFOScheduler-interpreter_861167659-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1653794219309_1032825628 finished by scheduler interpreter_861167659 with status FINISHED
    >    INFO [2022-05-29 03:17:16,232] ({FIFOScheduler-interpreter_861167659-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1653794219309_248449534 started by scheduler interpreter_861167659
    >    INFO [2022-05-29 03:17:16,235] ({FIFOScheduler-interpreter_861167659-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1653794219309_248449534 finished by scheduler interpreter_861167659 with status FINISHED
    >    INFO [2022-05-29 03:17:18,711] ({FIFOScheduler-interpreter_861167659-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1653794237983_1559630241 started by scheduler interpreter_861167659
    >   ....
    >   ....
    >   ....
    >   ....
    >    INFO [2022-05-29 03:17:22,666] ({FIFOScheduler-interpreter_861167659-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1653794237985_37439007 started by scheduler interpreter_861167659
    >    INFO [2022-05-29 03:17:22,669] ({FIFOScheduler-interpreter_861167659-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1653794237985_37439007 finished by scheduler interpreter_861167659 with status FINISHED
    >    INFO [2022-05-29 03:17:22,767] ({FIFOScheduler-interpreter_861167659-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1653794237985_69045688 started by scheduler interpreter_861167659
    >    INFO [2022-05-29 03:17:22,771] ({FIFOScheduler-interpreter_861167659-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1653794237985_69045688 finished by scheduler interpreter_861167659 with status FINISHED
    >    INFO [2022-05-29 03:18:14,101] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Requesting to kill executor(s) 97, 93, 102
    >    INFO [2022-05-29 03:18:14,101] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Actual list of executor(s) to be killed is 97, 93, 102
    >    INFO [2022-05-29 03:18:14,104] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Executors 97,93,102 removed due to idle timeout.
    >    INFO [2022-05-29 03:18:14,205] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Requesting to kill executor(s) 87, 92, 95, 101, 94, 100, 96
    >    INFO [2022-05-29 03:18:14,205] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Actual list of executor(s) to be killed is 87, 92, 95, 101, 94, 100, 96
    >    INFO [2022-05-29 03:18:14,207] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Executors 87,92,95,101,94,100,96 removed due to idle timeout.
    >    INFO [2022-05-29 03:18:14,308] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Requesting to kill executor(s) 98, 103
    >    INFO [2022-05-29 03:18:14,308] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Actual list of executor(s) to be killed is 98, 103
    >    INFO [2022-05-29 03:18:14,310] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Executors 98,103 removed due to idle timeout.
    >    INFO [2022-05-29 03:18:14,617] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 95.
    >    INFO [2022-05-29 03:18:14,618] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Executor lost: 95 (epoch 8)
    >    INFO [2022-05-29 03:18:14,618] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 94.
    >    INFO [2022-05-29 03:18:14,618] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Trying to remove executor 95 from BlockManagerMaster.
    >    WARN [2022-05-29 03:18:14,618] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_183 !
    >    WARN [2022-05-29 03:18:14,618] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_184 !
    >    WARN [2022-05-29 03:18:14,618] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_182 !
    >   ....
    >   ....
    >    WARN [2022-05-29 03:18:14,913] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_133 !
    >    WARN [2022-05-29 03:18:14,913] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_132 !
    >    WARN [2022-05-29 03:18:14,913] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_134 !
    >    WARN [2022-05-29 03:18:14,913] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_130 !
    >    INFO [2022-05-29 03:18:14,913] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removing block manager BlockManagerId(93, worker06, 39101, None)
    >    INFO [2022-05-29 03:18:14,913] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Executor 92 on worker06 killed by driver.
    >    INFO [2022-05-29 03:18:14,913] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Removed 93 successfully in removeExecutor
    >    INFO [2022-05-29 03:18:14,914] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Executor 93 on worker06 killed by driver.
    >    INFO [2022-05-29 03:18:15,094] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 87.
    >    INFO [2022-05-29 03:18:15,094] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Executor lost: 87 (epoch 8)
    >    INFO [2022-05-29 03:18:15,094] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 102.
    >    INFO [2022-05-29 03:18:15,094] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Trying to remove executor 87 from BlockManagerMaster.
    >    WARN [2022-05-29 03:18:15,094] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_188 !
    >    WARN [2022-05-29 03:18:15,094] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_18 !
    >    WARN [2022-05-29 03:18:15,094] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_194 !
    >    WARN [2022-05-29 03:18:15,094] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_17 !
    >    WARN [2022-05-29 03:18:15,094] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_53 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_113 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_152 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_44 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_85 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_115 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_75 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_108 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_58 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_89 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_192 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_47 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_27 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_6 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_22 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_60 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_16 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_106 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_128 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_74 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_52 !
    >    INFO [2022-05-29 03:18:15,095] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Executor 87 on worker04 killed by driver.
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_76 !
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_41 !
    >    INFO [2022-05-29 03:18:15,095] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Executor 102 on worker04 killed by driver.
    >    WARN [2022-05-29 03:18:15,095] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_142 !
    >    WARN [2022-05-29 03:18:15,096] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_67 !
    >    WARN [2022-05-29 03:18:15,096] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_61 !
    >    WARN [2022-05-29 03:18:15,096] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_83 !
    >    WARN [2022-05-29 03:18:15,096] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_21 !
    >    WARN [2022-05-29 03:18:15,096] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_50 !
    >    WARN [2022-05-29 03:18:15,096] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_79 !
    >    WARN [2022-05-29 03:18:15,096] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_11 !
    >    WARN [2022-05-29 03:18:15,096] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_147 !
    >    WARN [2022-05-29 03:18:15,096] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_54 !
    >    WARN [2022-05-29 03:18:15,096] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_187 !
    >    WARN [2022-05-29 03:18:15,096] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_39 !
    >   ....
    >   ....

# -----------------------------------------------------

    #
    # Tests never complete :-(
    #

# -----------------------------------------------------
# Restart Zepelin
#[user@zeppelin]

    zeppelin-daemon.sh restart

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]


# -----------------------------------------------------
# Try 7 users ...
#[root@ansibler]

        cat > /tmp/testprog.py << EOF
import sys
from aglais_benchmark import AglaisBenchmarker
AglaisBenchmarker(
    "/deployments/zeppelin/test/config/quick.json",
    "/tmp/testusers.json",
    "/tmp/",
    "http://128.232.222.27:8080"
    ).run(
        concurrent=True,
        users=7
        )
EOF

    python3 /tmp/testprog.py | tee /tmp/test-results.txt

    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        '/tmp/test-results.txt' \
    | jq '.' \
    | tee '/tmp/test-results.json'

    jq '
        .[] | keys as $x | [ $x[] as $y | {name: $y, value: .[$y].result, time: .[$y].time.elapsed } ]
        ' '/tmp/test-results.json'

    >   Test started [Multi User]
    >   ERROR:root:list index out of range
    >   Traceback (most recent call last):
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 114, in run_notebook
    >       notebookid = text.split(": ")[1]
    >   IndexError: list index out of range
    >   ....
    >   ....

    #
    # Error happens at the start of the tests.
    #

    #
    # Worker nodes all idle.
    #

    #
    # Everything is stuck in a queue.
    #

    >    ....
    >    INFO [2022-05-29 03:50:55,617] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    INFO [2022-05-29 03:50:56,618] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    INFO [2022-05-29 03:50:57,619] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    INFO [2022-05-29 03:50:58,620] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    ....
    >    ....
    >    INFO [2022-05-29 03:58:12,206] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    INFO [2022-05-29 03:58:13,207] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    INFO [2022-05-29 03:58:14,209] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    INFO [2022-05-29 03:58:15,210] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    ....
    >    ....
    >    INFO [2022-05-29 04:18:20,583] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    INFO [2022-05-29 04:18:21,584] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    INFO [2022-05-29 04:18:22,585] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    INFO [2022-05-29 04:18:23,586] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    ....
    >    ....
    >    .... 7 hrs later ..
    >    ....
    >    ....
    >    INFO [2022-05-29 11:18:10,321] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    INFO [2022-05-29 11:18:11,323] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    INFO [2022-05-29 11:18:12,324] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    INFO [2022-05-29 11:18:13,326] ({FIFOScheduler-interpreter_1955234851-Worker-1} Logging.scala[logInfo]:57) - Application report for application_1653751036277_0046 (state: ACCEPTED)
    >    ....
    >    ....

    #
    # Yep, it's stuck.
    #


