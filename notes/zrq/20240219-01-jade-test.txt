#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2024, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: []
#

    Target:

        Test to see if the platform is working today.

    Result:

        Nope, same issue as before.
        https://github.com/lsst-uk/somerville-operations/issues/144

        Collecting data to help diagnosis:

        * Check the Helm releases on the KinD (managment) cluster.
          https://github.com/lsst-uk/somerville-operations/issues/144#issuecomment-1905783041

        * Check the Calico CNI Pods in the tenant (work) cluster.
          https://github.com/lsst-uk/somerville-operations/issues/144#issuecomment-1910276921


# -----------------------------------------------------
# Run our local client.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    export PATH=${PATH}:${AGLAIS_CODE}/bin

    kube-client jade

    >   ....
    >   ....


# -----------------------------------------------------
# Delete and create everything.
#[root@ansibler]

    export cloudsite=somerville-jade

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    ansible-playbook \
        --inventory 'bootstrap,' \
        '/deployments/cluster-api/ansible/00-create-all.yml'

    >   ....
    >   ....
    >   PLAY RECAP **********************************************************************************************************************
    >   bootstrap                  : ok=58   changed=45   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   localhost                  : ok=35   changed=26   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


# -----------------------------------------------------
# Check the deployment configuration.
#[root@ansibler]

    cat /opt/aglais/aglais-status.yml

    >   Mon Feb 19 04:02:23 AM UTC 2024
    >   somerville-jade-20240219-bootstrap-node.novalocal
    >   aglais:
    >     ansibler:
    >       external:
    >         ipv4: 90.155.51.57
    >     deployment:
    >       date: 20240219
    >       name: somerville-jade-20240219
    >       type: cluster-api
    >     kubernetes:
    >       cluster:
    >         kind:
    >           conf: /opt/aglais/somerville-jade-20240219-kind.yml
    >           name: somerville-jade-20240219-kind
    >         work:
    >           conf: /opt/aglais/somerville-jade-20240219-work.yml
    >           name: somerville-jade-20240219-work
    >       version: 1.26.7
    >     openstack:
    >       cloud:
    >         name: somerville-jade
    >         site: somerville-jade
    >       keypair:
    >         fingerprint: 2e:84:98:98:df:70:06:0e:4c:ed:bd:d4:d6:6b:eb:16
    >         id: somerville-jade-20240219-keypair
    >         name: somerville-jade-20240219-keypair
    >       networks:
    >         bootstrap:
    >           network:
    >             id: b95514da-bb11-427a-9145-3755f9a30f18
    >             name: somerville-jade-20240219-bootstrap-network
    >           router:
    >             id: e0033643-6ff5-4741-99a3-828ccb26d7bb
    >             name: somerville-jade-20240219-bootstrap-network-router
    >           subnet:
    >             cidr: 10.10.0.0/16
    >             id: 488e9553-fdb7-491f-8c98-457ad22b01f7
    >             name: somerville-jade-20240219-bootstrap-network-subnet
    >         external:
    >           network:
    >             id: 1875828a-ccc3-419b-87fd-856aaa781492
    >             name: external
    >       project:
    >         id: be227fe0300b4ce5b03f44264df615df,
    >         name: Somerville-Gaia-Jade
    >       servers:
    >         bootstrap:
    >           float:
    >             external: 192.41.122.24
    >             id: f10cb4f1-d7e8-49fc-a30c-0d9c967983e6
    >             internal: 10.10.1.254
    >           server:
    >             address:
    >               ipv4: 10.10.1.254
    >             flavor:
    >               name: gaia.vm.2vcpu
    >             hostname: bootstrap
    >             id: 3678bc65-fb4a-482a-9942-4b9856ee7826
    >             image:
    >               id: ce533fcf-75a6-4267-a622-d0227e6940b0
    >               name: gaia-dmp-fedora-cloud-38-1.6
    >             name: somerville-jade-20240219-bootstrap-node
    >       user:
    >         id: c4aad146ab7acaf44819e90e3e67a4d0490c164fbb02d388823c1ac9f0ae2e13,
    >         name: Dave Morris


# -----------------------------------------------------
# Check the cluster status.
#[root@ansibler]

    ssh bootstrap -t \
        '
        source loadconfig
        watch \
            clusterctl \
                --kubeconfig "${kindclusterconf:?}" \
                describe cluster \
                    "${workclustername:?}"
        '

    >   NAME                                                                              READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/somerville-jade-20240219-work                                             True                                          3m5s
    >   ├─ClusterInfrastructure - OpenStackCluster/somerville-jade-20240219-work
    >   ├─ControlPlane - KubeadmControlPlane/somerville-jade-20240219-work-control-plane  True                                          3m5s
    >   │ └─3 Machines...                                                                 True                                          6m52s  See somerville-jade-20240219-work-control-plane-6fk2z, somerville-jade-20240219-work-control-plane-ccmrm, ...
    >   └─Workers
    >     └─MachineDeployment/somerville-jade-20240219-work-md-0                          False  Warning   WaitingForAvailableMachines  6m1s   Minimum availability requires 5 replicas, current 2 available
    >       ├─4 Machines...                                                               False  Error     InstanceCreateFailed         75s    See somerville-jade-20240219-work-md-0-ff5gr-6kn7j, somerville-jade-20240219-work-md-0-ff5gr-hkrms, ...
    >       └─2 Machines...                                                               True                                          11m    See somerville-jade-20240219-work-md-0-ff5gr-n6xb6, somerville-jade-20240219-work-md-0-ff5gr-w6khh


# -----------------------------------------------------
# List our machines in Openstack.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        server list

    >   +--------------------------------------+------------------------------------------------------------+--------+----------------------------------------------------------------------------+-----------------------------------+----------------+
    >   | ID                                   | Name                                                       | Status | Networks                                                                   | Image                             | Flavor         |
    >   +--------------------------------------+------------------------------------------------------------+--------+----------------------------------------------------------------------------+-----------------------------------+----------------+
    >   | aa0e964e-ada8-4fcc-802d-37ddb959dbbd | somerville-jade-20240219-work-control-plane-ac9af912-c9ldg | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.210 | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.2vcpu  |
    >   | 23018ad9-259f-4ce3-9c88-4ea07c4f9081 | somerville-jade-20240219-work-control-plane-ac9af912-xddjz | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.178 | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.2vcpu  |
    >   | 6170c69e-629a-4f5e-b767-85be78c1d5f3 | somerville-jade-20240219-work-md-0-fb50a5e8-ncwdq          | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.177 | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.26vcpu |
    >   | 7606a0a0-e23c-4e26-baca-ce14e5279b8f | somerville-jade-20240219-work-md-0-fb50a5e8-tjm67          | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.79  | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.26vcpu |
    >   | ee4669d8-dbbf-40ee-9053-133d669de390 | somerville-jade-20240219-work-control-plane-ac9af912-42bl8 | ACTIVE | k8s-clusterapi-cluster-default-somerville-jade-20240219-work=192.168.3.189 | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.2vcpu  |
    >   | 3678bc65-fb4a-482a-9942-4b9856ee7826 | somerville-jade-20240219-bootstrap-node                    | ACTIVE | somerville-jade-20240219-bootstrap-network=10.10.1.254, 192.41.122.24      | gaia-dmp-fedora-cloud-38-1.6      | gaia.vm.2vcpu  |
    >   | eeb39c9d-7a54-4e11-ae04-3677980c3581 | somerville-jade-20240218-work-md-0-5d726be3-n9dtw          | ACTIVE |                                                                            | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.26vcpu |
    >   | ba1ab201-83c7-409b-9ca5-a7ec8c43656c | somerville-jade-20240218-work-md-0-5d726be3-557t4          | ACTIVE |                                                                            | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.26vcpu |
    >   | 3fe6472d-b611-4672-8c30-e78359faf522 | somerville-jade-20240218-work-md-0-5d726be3-zhn56          | ACTIVE |                                                                            | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.26vcpu |
    >   | 6f1552e4-8b3a-4c32-967e-aa63ff2deac3 | somerville-jade-20240218-work-md-0-5d726be3-8bzxh          | ACTIVE |                                                                            | gaia-dmp-ubuntu-2204-kube-v1.26.7 | gaia.vm.26vcpu |
    >   +--------------------------------------+------------------------------------------------------------+--------+----------------------------------------------------------------------------+-----------------------------------+----------------+


# -----------------------------------------------------
# List our machines in Kubernetes.
#[root@ansibler]

    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            get machines \
                --all-namespaces
        '

    >   NAMESPACE   NAME                                                CLUSTER                         NODENAME                                                     PROVIDERID                                          PHASE          AGE     VERSION
    >   default     somerville-jade-20240219-work-control-plane-6fk2z   somerville-jade-20240219-work   somerville-jade-20240219-work-control-plane-ac9af912-xddjz   openstack:///23018ad9-259f-4ce3-9c88-4ea07c4f9081   Running        11m     v1.26.7
    >   default     somerville-jade-20240219-work-control-plane-ccmrm   somerville-jade-20240219-work   somerville-jade-20240219-work-control-plane-ac9af912-c9ldg   openstack:///aa0e964e-ada8-4fcc-802d-37ddb959dbbd   Running        7m47s   v1.26.7
    >   default     somerville-jade-20240219-work-control-plane-tw7db   somerville-jade-20240219-work   somerville-jade-20240219-work-control-plane-ac9af912-42bl8   openstack:///ee4669d8-dbbf-40ee-9053-133d669de390   Running        17m     v1.26.7
    >   default     somerville-jade-20240219-work-md-0-ff5gr-6kn7j      somerville-jade-20240219-work                                                                                                                    Provisioning   5m32s   v1.26.7
    >   default     somerville-jade-20240219-work-md-0-ff5gr-hkrms      somerville-jade-20240219-work                                                                                                                    Provisioning   5m31s   v1.26.7
    >   default     somerville-jade-20240219-work-md-0-ff5gr-hllm4      somerville-jade-20240219-work                                                                                                                    Provisioning   5m32s   v1.26.7
    >   default     somerville-jade-20240219-work-md-0-ff5gr-n6xb6      somerville-jade-20240219-work   somerville-jade-20240219-work-md-0-fb50a5e8-ncwdq            openstack:///6170c69e-629a-4f5e-b767-85be78c1d5f3   Running        19m     v1.26.7
    >   default     somerville-jade-20240219-work-md-0-ff5gr-qms66      somerville-jade-20240219-work                                                                                                                    Provisioning   5m35s   v1.26.7
    >   default     somerville-jade-20240219-work-md-0-ff5gr-w6khh      somerville-jade-20240219-work   somerville-jade-20240219-work-md-0-fb50a5e8-tjm67            openstack:///7606a0a0-e23c-4e26-baca-ce14e5279b8f   Running        19m     v1.26.7


# -----------------------------------------------------
# List our nodes in Kubernetes.
#[root@ansibler]

    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            get nodes \
                --all-namespaces
        '

    >   NAME                                                         STATUS   ROLES           AGE     VERSION
    >   somerville-jade-20240219-work-control-plane-ac9af912-42bl8   Ready    control-plane   15m     v1.26.7
    >   somerville-jade-20240219-work-control-plane-ac9af912-c9ldg   Ready    control-plane   7m14s   v1.26.7
    >   somerville-jade-20240219-work-control-plane-ac9af912-xddjz   Ready    control-plane   9m13s   v1.26.7
    >   somerville-jade-20240219-work-md-0-fb50a5e8-ncwdq            Ready    <none>          14m     v1.26.7
    >   somerville-jade-20240219-work-md-0-fb50a5e8-tjm67            Ready    <none>          14m     v1.26.7


# -----------------------------------------------------
# Get the details of our Openstack LoadBalancer.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer list

    >   +--------------------------------------+----------------------------------------------------------------------+----------------------------------+--------------+---------------------+------------------+----------+
    >   | id                                   | name                                                                 | project_id                       | vip_address  | provisioning_status | operating_status | provider |
    >   +--------------------------------------+----------------------------------------------------------------------+----------------------------------+--------------+---------------------+------------------+----------+
    >   | a228d847-d740-4205-bc7b-37223c8fabb9 | k8s-clusterapi-cluster-default-somerville-jade-20240219-work-kubeapi | be227fe0300b4ce5b03f44264df615df | 192.168.3.10 | ACTIVE              | ONLINE           | amphora  |
    >   +--------------------------------------+----------------------------------------------------------------------+----------------------------------+--------------+---------------------+------------------+----------+


    balancerid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            loadbalancer list \
                --format json \
        | jq -r '.[0].id'
        )

    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer show \
            --format json \
            "${balancerid:?}" \
    | jq '.'

    >   {
    >     "admin_state_up": true,
    >     "availability_zone": null,
    >     "created_at": "2024-02-19T03:48:29",
    >     "description": "Created by cluster-api-provider-openstack cluster default-somerville-jade-20240219-work",
    >     "flavor_id": null,
    >     "id": "a228d847-d740-4205-bc7b-37223c8fabb9",
    >     "listeners": "cb73f1c7-c8f6-43d7-b6b6-85d5803e7d1f",
    >     "name": "k8s-clusterapi-cluster-default-somerville-jade-20240219-work-kubeapi",
    >     "operating_status": "ONLINE",
    >     "pools": "a7a9a261-2c9b-43d9-a83a-de91dba6a3aa",
    >     "project_id": "be227fe0300b4ce5b03f44264df615df",
    >     "provider": "amphora",
    >     "provisioning_status": "ACTIVE",
    >     "updated_at": "2024-02-19T04:01:42",
    >     "vip_address": "192.168.3.10",
    >     "vip_network_id": "1b312e85-acf5-4a0d-9d16-5d7766346351",
    >     "vip_port_id": "040d2dfa-9f76-48be-8d4a-df671e362fc9",
    >     "vip_qos_policy_id": null,
    >     "vip_subnet_id": "6228a2d8-5365-4f90-b1f6-c5f0bbed178e",
    >     "tags": ""
    >   }


    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer listener \
            list

    >   +--------------------------------------+--------------------------------------+---------------------------------------------------------------------------+----------------------------------+----------+---------------+----------------+
    >   | id                                   | default_pool_id                      | name                                                                      | project_id                       | protocol | protocol_port | admin_state_up |
    >   +--------------------------------------+--------------------------------------+---------------------------------------------------------------------------+----------------------------------+----------+---------------+----------------+
    >   | cb73f1c7-c8f6-43d7-b6b6-85d5803e7d1f | a7a9a261-2c9b-43d9-a83a-de91dba6a3aa | k8s-clusterapi-cluster-default-somerville-jade-20240219-work-kubeapi-6443 | be227fe0300b4ce5b03f44264df615df | TCP      |          6443 | True           |
    >   +--------------------------------------+--------------------------------------+---------------------------------------------------------------------------+----------------------------------+----------+---------------+----------------+


    listenerid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            loadbalancer listener \
                list \
                --format json \
        | jq -r '.[0].id'
        )

    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer listener \
            show \
            --format json \
            "${listenerid}" \
    | jq '.'

    >   {
    >     "admin_state_up": true,
    >     "connection_limit": -1,
    >     "created_at": "2024-02-19T03:49:43",
    >     "default_pool_id": "a7a9a261-2c9b-43d9-a83a-de91dba6a3aa",
    >     "default_tls_container_ref": null,
    >     "description": "",
    >     "id": "cb73f1c7-c8f6-43d7-b6b6-85d5803e7d1f",
    >     "insert_headers": null,
    >     "l7policies": "",
    >     "loadbalancers": "a228d847-d740-4205-bc7b-37223c8fabb9",
    >     "name": "k8s-clusterapi-cluster-default-somerville-jade-20240219-work-kubeapi-6443",
    >     "operating_status": "ONLINE",
    >     "project_id": "be227fe0300b4ce5b03f44264df615df",
    >     "protocol": "TCP",
    >     "protocol_port": 6443,
    >     "provisioning_status": "ACTIVE",
    >     "sni_container_refs": [],
    >     "timeout_client_data": 50000,
    >     "timeout_member_connect": 5000,
    >     "timeout_member_data": 50000,
    >     "timeout_tcp_inspect": 0,
    >     "updated_at": "2024-02-19T04:00:30",
    >     "client_ca_tls_container_ref": null,
    >     "client_authentication": "NONE",
    >     "client_crl_container_ref": null,
    >     "allowed_cidrs": "192.168.3.0/24\n192.41.122.148/32\n192.41.122.24/32\n90.155.51.57/32",
    >     "tls_ciphers": null,
    >     "tls_versions": null,
    >     "alpn_protocols": null,
    >     "tags": ""
    >   }


    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer pool \
            list

    >   +--------------------------------------+---------------------------------------------------------------------------+----------------------------------+---------------------+----------+--------------+----------------+
    >   | id                                   | name                                                                      | project_id                       | provisioning_status | protocol | lb_algorithm | admin_state_up |
    >   +--------------------------------------+---------------------------------------------------------------------------+----------------------------------+---------------------+----------+--------------+----------------+
    >   | a7a9a261-2c9b-43d9-a83a-de91dba6a3aa | k8s-clusterapi-cluster-default-somerville-jade-20240219-work-kubeapi-6443 | be227fe0300b4ce5b03f44264df615df | ACTIVE              | TCP      | ROUND_ROBIN  | True           |
    >   +--------------------------------------+---------------------------------------------------------------------------+----------------------------------+---------------------+----------+--------------+----------------+


    poolid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            loadbalancer pool \
                list \
                --format json \
        | jq -r '.[0].id'
        )

    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer pool \
            show \
            --format json \
            "${poolid}" \
    | jq '.'

    >   {
    >     "admin_state_up": true,
    >     "created_at": "2024-02-19T03:49:50",
    >     "description": "",
    >     "healthmonitor_id": "02dc166e-6fb9-4ce9-bbee-76cee2d045f7",
    >     "id": "a7a9a261-2c9b-43d9-a83a-de91dba6a3aa",
    >     "lb_algorithm": "ROUND_ROBIN",
    >     "listeners": "cb73f1c7-c8f6-43d7-b6b6-85d5803e7d1f",
    >     "loadbalancers": "a228d847-d740-4205-bc7b-37223c8fabb9",
    >     "members": "d2c3a4f5-cb21-4b3c-912a-a4abada8a930\n161166ef-c747-4fec-8862-b33459e4e216\n98524bf5-0577-43ac-9db9-bf78a31a877a",
    >     "name": "k8s-clusterapi-cluster-default-somerville-jade-20240219-work-kubeapi-6443",
    >     "operating_status": "ONLINE",
    >     "project_id": "be227fe0300b4ce5b03f44264df615df",
    >     "protocol": "TCP",
    >     "provisioning_status": "ACTIVE",
    >     "session_persistence": null,
    >     "updated_at": "2024-02-19T04:01:42",
    >     "tls_container_ref": null,
    >     "ca_tls_container_ref": null,
    >     "crl_container_ref": null,
    >     "tls_enabled": false,
    >     "tls_ciphers": null,
    >     "tls_versions": null,
    >     "tags": "",
    >     "alpn_protocols": null
    >   }


    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer member \
            list \
            "${poolid}"

    >   +--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+---------------------+---------------+---------------+------------------+--------+
    >   | id                                   | name                                                                                                                                 | project_id                       | provisioning_status | address       | protocol_port | operating_status | weight |
    >   +--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+---------------------+---------------+---------------+------------------+--------+
    >   | 161166ef-c747-4fec-8862-b33459e4e216 | k8s-clusterapi-cluster-default-somerville-jade-20240219-work-kubeapi-6443-somerville-jade-20240219-work-control-plane-ac9af912-42bl8 | be227fe0300b4ce5b03f44264df615df | ACTIVE              | 192.168.3.189 |          6443 | ONLINE           |      1 |
    >   | d2c3a4f5-cb21-4b3c-912a-a4abada8a930 | k8s-clusterapi-cluster-default-somerville-jade-20240219-work-kubeapi-6443-somerville-jade-20240219-work-control-plane-ac9af912-xddjz | be227fe0300b4ce5b03f44264df615df | ACTIVE              | 192.168.3.178 |          6443 | ONLINE           |      1 |
    >   | 98524bf5-0577-43ac-9db9-bf78a31a877a | k8s-clusterapi-cluster-default-somerville-jade-20240219-work-kubeapi-6443-somerville-jade-20240219-work-control-plane-ac9af912-c9ldg | be227fe0300b4ce5b03f44264df615df | ACTIVE              | 192.168.3.210 |          6443 | ONLINE           |      1 |
    >   +--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+----------------------------------+---------------------+---------------+---------------+------------------+--------+


    for memberid in $(
        openstack \
            --os-cloud "${cloudname:?}" \
            loadbalancer member \
                list \
                "${poolid}" \
                --format json \
        | jq -r '.[].id'
        )
        do
            echo ""
            echo "Member [${memberid}]"
            openstack \
                --os-cloud "${cloudname:?}" \
                loadbalancer member \
                    show \
                    "${poolid}" \
                    "${memberid}" \
                    --format json \
            | jq '.'
        done

    >   Member [161166ef-c747-4fec-8862-b33459e4e216]
    >   {
    >     "address": "192.168.3.189",
    >     "admin_state_up": true,
    >     "created_at": "2024-02-19T03:50:12",
    >     "id": "161166ef-c747-4fec-8862-b33459e4e216",
    >     "name": "k8s-clusterapi-cluster-default-somerville-jade-20240219-work-kubeapi-6443-somerville-jade-20240219-work-control-plane-ac9af912-42bl8",
    >     "operating_status": "ONLINE",
    >     "project_id": "be227fe0300b4ce5b03f44264df615df",
    >     "protocol_port": 6443,
    >     "provisioning_status": "ACTIVE",
    >     "subnet_id": null,
    >     "updated_at": "2024-02-19T03:52:12",
    >     "weight": 1,
    >     "monitor_port": null,
    >     "monitor_address": null,
    >     "backup": false,
    >     "tags": ""
    >   }
    >   
    >   Member [d2c3a4f5-cb21-4b3c-912a-a4abada8a930]
    >   {
    >     "address": "192.168.3.178",
    >     "admin_state_up": true,
    >     "created_at": "2024-02-19T03:56:45",
    >     "id": "d2c3a4f5-cb21-4b3c-912a-a4abada8a930",
    >     "name": "k8s-clusterapi-cluster-default-somerville-jade-20240219-work-kubeapi-6443-somerville-jade-20240219-work-control-plane-ac9af912-xddjz",
    >     "operating_status": "ONLINE",
    >     "project_id": "be227fe0300b4ce5b03f44264df615df",
    >     "protocol_port": 6443,
    >     "provisioning_status": "ACTIVE",
    >     "subnet_id": null,
    >     "updated_at": "2024-02-19T04:00:32",
    >     "weight": 1,
    >     "monitor_port": null,
    >     "monitor_address": null,
    >     "backup": false,
    >     "tags": ""
    >   }
    >   
    >   Member [98524bf5-0577-43ac-9db9-bf78a31a877a]
    >   {
    >     "address": "192.168.3.210",
    >     "admin_state_up": true,
    >     "created_at": "2024-02-19T04:00:26",
    >     "id": "98524bf5-0577-43ac-9db9-bf78a31a877a",
    >     "name": "k8s-clusterapi-cluster-default-somerville-jade-20240219-work-kubeapi-6443-somerville-jade-20240219-work-control-plane-ac9af912-c9ldg",
    >     "operating_status": "ONLINE",
    >     "project_id": "be227fe0300b4ce5b03f44264df615df",
    >     "protocol_port": 6443,
    >     "provisioning_status": "ACTIVE",
    >     "subnet_id": null,
    >     "updated_at": "2024-02-19T04:01:42",
    >     "weight": 1,
    >     "monitor_port": null,
    >     "monitor_address": null,
    >     "backup": false,
    >     "tags": ""
    >   }


# -----------------------------------------------------
# Check the Helm releases on the KinD managment cluster.
# https://github.com/lsst-uk/somerville-operations/issues/144#issuecomment-1905783041
#[root@ansibler]

    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            get helmrelease -A
        '

    >   NAMESPACE   NAME                                                      CLUSTER                         BOOTSTRAP   TARGET NAMESPACE         RELEASE NAME                PHASE      REVISION   CHART NAME                           CHART VERSION   AGE
    >   default     somerville-jade-20240219-work-ccm-openstack               somerville-jade-20240219-work   true        openstack-system         ccm-openstack               Deployed   1          openstack-cloud-controller-manager   1.3.0           32m
    >   default     somerville-jade-20240219-work-cni-calico                  somerville-jade-20240219-work   true        tigera-operator          cni-calico                  Deployed   1          tigera-operator                      v3.26.0         32m
    >   default     somerville-jade-20240219-work-csi-cinder                  somerville-jade-20240219-work   true        openstack-system         csi-cinder                  Deployed   1          openstack-cinder-csi                 2.2.0           32m
    >   default     somerville-jade-20240219-work-kubernetes-dashboard        somerville-jade-20240219-work   true        kubernetes-dashboard     kubernetes-dashboard        Deployed   1          kubernetes-dashboard                 5.10.0          32m
    >   default     somerville-jade-20240219-work-mellanox-network-operator   somerville-jade-20240219-work   true        network-operator         mellanox-network-operator   Deployed   1          network-operator                     1.3.0           32m
    >   default     somerville-jade-20240219-work-metrics-server              somerville-jade-20240219-work   true        kube-system              metrics-server              Deployed   1          metrics-server                       3.8.2           32m
    >   default     somerville-jade-20240219-work-node-feature-discovery      somerville-jade-20240219-work   true        node-feature-discovery   node-feature-discovery      Deployed   1          node-feature-discovery               0.11.2          32m
    >   default     somerville-jade-20240219-work-nvidia-gpu-operator         somerville-jade-20240219-work   true        gpu-operator             nvidia-gpu-operator         Deployed   1          gpu-operator                         v1.11.1         32m


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            describe helmrelease \
                "somerville-jade-20240219-work-ccm-openstack"
        '

    >   Name:         somerville-jade-20240219-work-ccm-openstack
    >   Namespace:    default
    >   Labels:       addons.stackhpc.com/cluster=somerville-jade-20240219-work
    >                 addons.stackhpc.com/release-name=ccm-openstack
    >                 addons.stackhpc.com/release-namespace=openstack-system
    >                 app.kubernetes.io/managed-by=Helm
    >                 capi.stackhpc.com/cluster=somerville-jade-20240219-work
    >                 capi.stackhpc.com/component=ccm-openstack
    >                 capi.stackhpc.com/managed-by=Helm
    >                 helm.sh/chart=addons-0.1.0
    >   Annotations:  addons.stackhpc.com/kopf-managed: yes
    >                 addons.stackhpc.com/last-handled-configuration:
    >                   {"spec":{"bootstrap":true,"chart":{"name":"openstack-cloud-controller-manager","repo":"https://kubernetes.github.io/cloud-provider-opensta...
    >                 meta.helm.sh/release-name: somerville-jade-20240219-work
    >                 meta.helm.sh/release-namespace: default
    >   API Version:  addons.stackhpc.com/v1alpha1
    >   Kind:         HelmRelease
    >   Metadata:
    >     Creation Timestamp:  2024-02-19T03:48:16Z
    >     Finalizers:
    >       addons.stackhpc.com/finalizer
    >     Generation:  1
    >     Owner References:
    >       API Version:           cluster.x-k8s.io/v1beta1
    >       Block Owner Deletion:  true
    >       Controller:            false
    >       Kind:                  Cluster
    >       Name:                  somerville-jade-20240219-work
    >       UID:                   15628401-979d-4230-9b03-90a2922df59d
    >     Resource Version:        3302
    >     UID:                     24fa555d-aab9-44fc-8b7d-7a1c2218e91e
    >   Spec:
    >     Bootstrap:  true
    >     Chart:
    >       Name:            openstack-cloud-controller-manager
    >       Repo:            https://kubernetes.github.io/cloud-provider-openstack
    >       Version:         1.3.0
    >     Cluster Name:      somerville-jade-20240219-work
    >     Release Name:      ccm-openstack
    >     Target Namespace:  openstack-system
    >     Values Sources:
    >       Secret:
    >         Key:   defaults
    >         Name:  somerville-jade-20240219-work-ccm-openstack-config
    >       Secret:
    >         Key:   overrides
    >         Name:  somerville-jade-20240219-work-ccm-openstack-config
    >   Status:
    >     Phase:  Deployed
    >     Resources:
    >       API Version:  v1
    >       Kind:         ServiceAccount
    >       Name:         openstack-cloud-controller-manager
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRole
    >       Name:         system:openstack-cloud-controller-manager
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRoleBinding
    >       Name:         system:openstack-cloud-controller-manager
    >       API Version:  apps/v1
    >       Kind:         DaemonSet
    >       Name:         openstack-cloud-controller-manager
    >     Revision:       1
    >   Events:
    >     Type    Reason   Age   From  Message
    >     ----    ------   ----  ----  -------
    >     Error   Logging  40m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  40m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  40m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  39m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  39m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  39m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  38m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  38m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  38m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  38m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  37m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  37m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  37m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  37m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  36m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  36m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Normal  Logging  36m   kopf  Handler 'handle_addon_updated' succeeded.
    >     Normal  Logging  36m   kopf  Creation is processed: 1 succeeded; 0 failed.


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            describe helmrelease \
                "somerville-jade-20240219-work-cni-calico"
        '

    >   Name:         somerville-jade-20240219-work-cni-calico
    >   Namespace:    default
    >   Labels:       addons.stackhpc.com/cluster=somerville-jade-20240219-work
    >                 addons.stackhpc.com/release-name=cni-calico
    >                 addons.stackhpc.com/release-namespace=tigera-operator
    >                 app.kubernetes.io/managed-by=Helm
    >                 capi.stackhpc.com/cluster=somerville-jade-20240219-work
    >                 capi.stackhpc.com/component=cni-calico
    >                 capi.stackhpc.com/managed-by=Helm
    >                 helm.sh/chart=addons-0.1.0
    >   Annotations:  addons.stackhpc.com/kopf-managed: yes
    >                 addons.stackhpc.com/last-handled-configuration:
    >                   {"spec":{"bootstrap":true,"chart":{"name":"tigera-operator","repo":"https://projectcalico.docs.tigera.io/charts","version":"v3.26.0"},"clu...
    >                 meta.helm.sh/release-name: somerville-jade-20240219-work
    >                 meta.helm.sh/release-namespace: default
    >   API Version:  addons.stackhpc.com/v1alpha1
    >   Kind:         HelmRelease
    >   Metadata:
    >     Creation Timestamp:  2024-02-19T03:48:16Z
    >     Finalizers:
    >       addons.stackhpc.com/finalizer
    >     Generation:  1
    >     Owner References:
    >       API Version:           cluster.x-k8s.io/v1beta1
    >       Block Owner Deletion:  true
    >       Controller:            false
    >       Kind:                  Cluster
    >       Name:                  somerville-jade-20240219-work
    >       UID:                   15628401-979d-4230-9b03-90a2922df59d
    >     Resource Version:        3610
    >     UID:                     b579293d-5b78-4bc2-b41b-241132af0cfd
    >   Spec:
    >     Bootstrap:  true
    >     Chart:
    >       Name:            tigera-operator
    >       Repo:            https://projectcalico.docs.tigera.io/charts
    >       Version:         v3.26.0
    >     Cluster Name:      somerville-jade-20240219-work
    >     Release Name:      cni-calico
    >     Target Namespace:  tigera-operator
    >     Values Sources:
    >       Secret:
    >         Key:   defaults
    >         Name:  somerville-jade-20240219-work-cni-calico-config
    >       Secret:
    >         Key:   overrides
    >         Name:  somerville-jade-20240219-work-cni-calico-config
    >   Status:
    >     Phase:  Deployed
    >     Resources:
    >       API Version:  v1
    >       Kind:         ServiceAccount
    >       Name:         tigera-operator
    >       Namespace:    tigera-operator
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRole
    >       Name:         tigera-operator
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRoleBinding
    >       Name:         tigera-operator
    >       API Version:  apps/v1
    >       Kind:         Deployment
    >       Name:         tigera-operator
    >       Namespace:    tigera-operator
    >       API Version:  operator.tigera.io/v1
    >       Kind:         APIServer
    >       Name:         default
    >       API Version:  operator.tigera.io/v1
    >       Kind:         Installation
    >       Name:         default
    >     Revision:       1
    >   Events:
    >     Type    Reason   Age   From  Message
    >     ----    ------   ----  ----  -------
    >     Error   Logging  38m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  38m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  38m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  37m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  37m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  37m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  37m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  36m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  36m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  36m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  36m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  35m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  35m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  35m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  35m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  34m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Normal  Logging  33m   kopf  Handler 'handle_addon_updated' succeeded.
    >     Normal  Logging  33m   kopf  Creation is processed: 1 succeeded; 0 failed.


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            describe helmrelease \
                "somerville-jade-20240219-work-csi-cinder"
        '

    >   Name:         somerville-jade-20240219-work-csi-cinder
    >   Namespace:    default
    >   Labels:       addons.stackhpc.com/cluster=somerville-jade-20240219-work
    >                 addons.stackhpc.com/release-name=csi-cinder
    >                 addons.stackhpc.com/release-namespace=openstack-system
    >                 app.kubernetes.io/managed-by=Helm
    >                 capi.stackhpc.com/cluster=somerville-jade-20240219-work
    >                 capi.stackhpc.com/component=csi-cinder
    >                 capi.stackhpc.com/managed-by=Helm
    >                 helm.sh/chart=addons-0.1.0
    >   Annotations:  addons.stackhpc.com/kopf-managed: yes
    >                 addons.stackhpc.com/last-handled-configuration:
    >                   {"spec":{"bootstrap":true,"chart":{"name":"openstack-cinder-csi","repo":"https://kubernetes.github.io/cloud-provider-openstack","version":...
    >                 meta.helm.sh/release-name: somerville-jade-20240219-work
    >                 meta.helm.sh/release-namespace: default
    >   API Version:  addons.stackhpc.com/v1alpha1
    >   Kind:         HelmRelease
    >   Metadata:
    >     Creation Timestamp:  2024-02-19T03:48:16Z
    >     Finalizers:
    >       addons.stackhpc.com/finalizer
    >     Generation:  1
    >     Owner References:
    >       API Version:           cluster.x-k8s.io/v1beta1
    >       Block Owner Deletion:  true
    >       Controller:            false
    >       Kind:                  Cluster
    >       Name:                  somerville-jade-20240219-work
    >       UID:                   15628401-979d-4230-9b03-90a2922df59d
    >     Resource Version:        4238
    >     UID:                     61d76f6f-17c3-410c-9117-de5660c13543
    >   Spec:
    >     Bootstrap:  true
    >     Chart:
    >       Name:            openstack-cinder-csi
    >       Repo:            https://kubernetes.github.io/cloud-provider-openstack
    >       Version:         2.2.0
    >     Cluster Name:      somerville-jade-20240219-work
    >     Release Name:      csi-cinder
    >     Target Namespace:  openstack-system
    >     Values Sources:
    >       Secret:
    >         Key:   defaults
    >         Name:  somerville-jade-20240219-work-csi-cinder-config
    >       Secret:
    >         Key:   overrides
    >         Name:  somerville-jade-20240219-work-csi-cinder-config
    >   Status:
    >     Notes:  Use the following storageClass csi-cinder-sc-retain and csi-cinder-sc-delete only for RWO volumes.
    >     Phase:  Deployed
    >     Resources:
    >       API Version:  v1
    >       Kind:         ServiceAccount
    >       Name:         csi-cinder-controller-sa
    >       Namespace:    openstack-system
    >       API Version:  v1
    >       Kind:         ServiceAccount
    >       Name:         csi-cinder-node-sa
    >       Namespace:    openstack-system
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRole
    >       Name:         csi-attacher-role
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRole
    >       Name:         csi-provisioner-role
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRole
    >       Name:         csi-snapshotter-role
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRole
    >       Name:         csi-resizer-role
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRole
    >       Name:         csi-nodeplugin-role
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRoleBinding
    >       Name:         csi-attacher-binding
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRoleBinding
    >       Name:         csi-provisioner-binding
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRoleBinding
    >       Name:         csi-snapshotter-binding
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRoleBinding
    >       Name:         csi-resizer-binding
    >       API Version:  rbac.authorization.k8s.io/v1
    >       Kind:         ClusterRoleBinding
    >       Name:         csi-nodeplugin-binding
    >       API Version:  apps/v1
    >       Kind:         DaemonSet
    >       Name:         openstack-cinder-csi-nodeplugin
    >       Namespace:    openstack-system
    >       API Version:  apps/v1
    >       Kind:         Deployment
    >       Name:         openstack-cinder-csi-controllerplugin
    >       Namespace:    openstack-system
    >       API Version:  storage.k8s.io/v1
    >       Kind:         CSIDriver
    >       Name:         cinder.csi.openstack.org
    >     Revision:       1
    >   Events:
    >     Type    Reason   Age   From  Message
    >     ----    ------   ----  ----  -------
    >     Error   Logging  36m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  36m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  36m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  36m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  35m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  35m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  35m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  35m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  34m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  34m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  34m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  34m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  33m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  33m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Error   Logging  33m   kopf  Handler 'handle_addon_updated' failed temporarily: cluster 'somerville-jade-20240219-work' is not ready
    >     Normal  Logging  28m   kopf  Handler 'handle_addon_updated' succeeded.
    >     Normal  Logging  28m   kopf  Creation is processed: 1 succeeded; 0 failed.


# -----------------------------------------------------
# List the Pods in the tenant (work) cluster.
#[root@ansibler]

    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            get pods \
                --all-namespaces
        '

    >   NAMESPACE                NAME                                                                                 READY   STATUS             RESTARTS        AGE
    >   calico-apiserver         calico-apiserver-55f54f87fb-gfdvc                                                    1/1     Running            0               19m
    >   calico-apiserver         calico-apiserver-55f54f87fb-vt8lf                                                    1/1     Running            0               19m
    >   calico-system            calico-kube-controllers-58f5b66895-xw995                                             1/1     Running            0               23m
    >   calico-system            calico-node-b8x47                                                                    1/1     Running            0               23m
    >   calico-system            calico-node-l6pk6                                                                    1/1     Running            0               23m
    >   calico-system            calico-node-ld6v5                                                                    1/1     Running            0               23m
    >   calico-system            calico-node-r9kwb                                                                    1/1     Running            0               19m
    >   calico-system            calico-node-rkdbq                                                                    1/1     Running            0               17m
    >   calico-system            calico-typha-96774b784-d6wfz                                                         1/1     Running            0               17m
    >   calico-system            calico-typha-96774b784-kzsht                                                         1/1     Running            0               23m
    >   calico-system            calico-typha-96774b784-rm5ps                                                         1/1     Running            0               23m
    >   calico-system            csi-node-driver-cs2df                                                                2/2     Running            0               17m
    >   calico-system            csi-node-driver-jq4v9                                                                2/2     Running            0               23m
    >   calico-system            csi-node-driver-l9mpb                                                                2/2     Running            0               23m
    >   calico-system            csi-node-driver-m9hsc                                                                2/2     Running            0               19m
    >   calico-system            csi-node-driver-rxntx                                                                2/2     Running            0               23m
    >   gpu-operator             gpu-operator-6c8649c88c-rdxmb                                                        1/1     Running            2 (18m ago)     25m
    >   kube-system              coredns-787d4945fb-f75hg                                                             1/1     Running            0               25m
    >   kube-system              coredns-787d4945fb-rzvgz                                                             1/1     Running            0               25m
    >   kube-system              etcd-somerville-jade-20240219-work-control-plane-ac9af912-42bl8                      1/1     Running            0               25m
    >   kube-system              etcd-somerville-jade-20240219-work-control-plane-ac9af912-c9ldg                      1/1     Running            0               16m
    >   kube-system              etcd-somerville-jade-20240219-work-control-plane-ac9af912-xddjz                      1/1     Running            0               18m
    >   kube-system              kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-42bl8            1/1     Running            0               25m
    >   kube-system              kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-c9ldg            1/1     Running            0               16m
    >   kube-system              kube-apiserver-somerville-jade-20240219-work-control-plane-ac9af912-xddjz            1/1     Running            1 (18m ago)     18m
    >   kube-system              kube-controller-manager-somerville-jade-20240219-work-control-plane-ac9af912-42bl8   1/1     Running            1 (18m ago)     25m
    >   kube-system              kube-controller-manager-somerville-jade-20240219-work-control-plane-ac9af912-c9ldg   1/1     Running            0               16m
    >   kube-system              kube-controller-manager-somerville-jade-20240219-work-control-plane-ac9af912-xddjz   1/1     Running            0               18m
    >   kube-system              kube-proxy-5j4qs                                                                     1/1     Running            0               24m
    >   kube-system              kube-proxy-k24zq                                                                     1/1     Running            0               25m
    >   kube-system              kube-proxy-q7sjm                                                                     1/1     Running            0               18m
    >   kube-system              kube-proxy-rt6mw                                                                     1/1     Running            0               24m
    >   kube-system              kube-proxy-sxd58                                                                     1/1     Running            0               17m
    >   kube-system              kube-scheduler-somerville-jade-20240219-work-control-plane-ac9af912-42bl8            1/1     Running            1 (18m ago)     25m
    >   kube-system              kube-scheduler-somerville-jade-20240219-work-control-plane-ac9af912-c9ldg            1/1     Running            0               16m
    >   kube-system              kube-scheduler-somerville-jade-20240219-work-control-plane-ac9af912-xddjz            1/1     Running            0               18m
    >   kube-system              metrics-server-65cccfc7bb-8cvqn                                                      1/1     Running            0               25m
    >   kubernetes-dashboard     kubernetes-dashboard-85d67585b8-85xs9                                                2/2     Running            0               25m
    >   network-operator         mellanox-network-operator-5f7b6b766c-tfm49                                           0/1     CrashLoopBackOff   8 (4m31s ago)   24m
    >   node-feature-discovery   node-feature-discovery-master-75c9d78d5f-rf2wk                                       1/1     Running            0               25m
    >   node-feature-discovery   node-feature-discovery-worker-2bvc4                                                  1/1     Running            0               24m
    >   node-feature-discovery   node-feature-discovery-worker-7cml4                                                  1/1     Running            0               19m
    >   node-feature-discovery   node-feature-discovery-worker-95x6x                                                  1/1     Running            0               17m
    >   node-feature-discovery   node-feature-discovery-worker-krzhs                                                  1/1     Running            2 (20m ago)     24m
    >   node-feature-discovery   node-feature-discovery-worker-mz827                                                  1/1     Running            0               25m
    >   openstack-system         openstack-cinder-csi-controllerplugin-7f768f855d-ts9rd                               6/6     Running            4 (18m ago)     25m
    >   openstack-system         openstack-cinder-csi-nodeplugin-cvs9v                                                3/3     Running            0               25m
    >   openstack-system         openstack-cinder-csi-nodeplugin-fkc4s                                                3/3     Running            0               24m
    >   openstack-system         openstack-cinder-csi-nodeplugin-mrggv                                                3/3     Running            0               18m
    >   openstack-system         openstack-cinder-csi-nodeplugin-rxv2n                                                3/3     Running            0               24m
    >   openstack-system         openstack-cinder-csi-nodeplugin-wpkj5                                                3/3     Running            0               17m
    >   openstack-system         openstack-cloud-controller-manager-lmhjc                                             1/1     Running            1 (18m ago)     22m
    >   openstack-system         openstack-cloud-controller-manager-m9f47                                             1/1     Running            0               16m
    >   openstack-system         openstack-cloud-controller-manager-rzqtk                                             1/1     Running            0               16m
    >   tigera-operator          tigera-operator-7d4cfffc6-mwbxc                                                      1/1     Running            2 (18m ago)     24m


# -----------------------------------------------------
# Check the Calico CNI Pods in the tenant (work) cluster.
# https://github.com/lsst-uk/somerville-operations/issues/144#issuecomment-1910276921

#[root@ansibler]


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            describe pod \
                --namespace calico-apiserver \
                calico-apiserver-55f54f87fb-gfdvc
        '

    >   Name:             calico-apiserver-55f54f87fb-gfdvc
    >   Namespace:        calico-apiserver
    >   Priority:         0
    >   Service Account:  calico-apiserver
    >   Node:             somerville-jade-20240219-work-md-0-fb50a5e8-ncwdq/192.168.3.177
    >   Start Time:       Mon, 19 Feb 2024 03:58:27 +0000
    >   Labels:           apiserver=true
    >                     app.kubernetes.io/name=calico-apiserver
    >                     k8s-app=calico-apiserver
    >                     pod-template-hash=55f54f87fb
    >   Annotations:      cni.projectcalico.org/containerID: 53448aaad87bcc3fc9cb2ed77083dfaa2f5303552be0e97f852bc6b9cdf0c0ca
    >                     cni.projectcalico.org/podIP: 172.21.231.7/32
    >                     cni.projectcalico.org/podIPs: 172.21.231.7/32
    >                     hash.operator.tigera.io/calico-apiserver-certs: af48e2e6ba5d7e74aa06e90948ef23aef32022b3
    >   Status:           Running
    >   IP:               172.21.231.7
    >   IPs:
    >     IP:           172.21.231.7
    >   Controlled By:  ReplicaSet/calico-apiserver-55f54f87fb
    >   Containers:
    >     calico-apiserver:
    >       Container ID:    containerd://292b2026c3b44cbc7e114c8bf34f58d99707068b3bd278b990f25690d4f5999b
    >       Image:           docker.io/calico/apiserver:v3.26.0
    >       Image ID:        docker.io/calico/apiserver@sha256:7cb5f719499163c172de25d55f1b3fc9bb1b6ea7ad8a1c8259e3eb3ac74890fc
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       Args:
    >         --secure-port=5443
    >         --tls-private-key-file=/calico-apiserver-certs/tls.key
    >         --tls-cert-file=/calico-apiserver-certs/tls.crt
    >       State:          Running
    >         Started:      Mon, 19 Feb 2024 03:58:38 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Liveness:       http-get https://:5443/version delay=90s timeout=1s period=10s #success=1 #failure=3
    >       Readiness:      exec [/code/filecheck] delay=5s timeout=1s period=10s #success=1 #failure=5
    >       Environment:
    >         DATASTORE_TYPE:           kubernetes
    >         KUBERNETES_SERVICE_HOST:  172.24.0.1
    >         KUBERNETES_SERVICE_PORT:  443
    >         MULTI_INTERFACE_MODE:     none
    >       Mounts:
    >         /calico-apiserver-certs from calico-apiserver-certs (ro)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jz5w7 (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     calico-apiserver-certs:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  calico-apiserver-certs
    >       Optional:    false
    >     kube-api-access-jz5w7:
    >       Type:                    Projected (a volume that contains injected data from multiple sources)
    >       TokenExpirationSeconds:  3607
    >       ConfigMapName:           kube-root-ca.crt
    >       ConfigMapOptional:       <nil>
    >       DownwardAPI:             true
    >   QoS Class:                   BestEffort
    >   Node-Selectors:              kubernetes.io/os=linux
    >   Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
    >                                node-role.kubernetes.io/master:NoSchedule
    >                                node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
    >                                node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
    >   Events:
    >     Type     Reason       Age                From               Message
    >     ----     ------       ----               ----               -------
    >     Normal   Scheduled    33m                default-scheduler  Successfully assigned calico-apiserver/calico-apiserver-55f54f87fb-gfdvc to somerville-jade-20240219-work-md-0-fb50a5e8-ncwdq
    >     Warning  FailedMount  33m (x3 over 33m)  kubelet            MountVolume.SetUp failed for volume "calico-apiserver-certs" : secret "calico-apiserver-certs" not found
    >     Normal   Pulling      33m                kubelet            Pulling image "docker.io/calico/apiserver:v3.26.0"
    >     Normal   Pulled       33m                kubelet            Successfully pulled image "docker.io/calico/apiserver:v3.26.0" in 6.294179904s (6.294198749s including waiting)
    >     Normal   Created      33m                kubelet            Created container calico-apiserver
    >     Normal   Started      33m                kubelet            Started container calico-apiserver


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            describe pod \
                --namespace calico-apiserver \
                calico-apiserver-55f54f87fb-vt8lf
        '

    >   Name:             calico-apiserver-55f54f87fb-vt8lf
    >   Namespace:        calico-apiserver
    >   Priority:         0
    >   Service Account:  calico-apiserver
    >   Node:             somerville-jade-20240219-work-md-0-fb50a5e8-tjm67/192.168.3.79
    >   Start Time:       Mon, 19 Feb 2024 03:58:27 +0000
    >   Labels:           apiserver=true
    >                     app.kubernetes.io/name=calico-apiserver
    >                     k8s-app=calico-apiserver
    >                     pod-template-hash=55f54f87fb
    >   Annotations:      cni.projectcalico.org/containerID: 0f0af15272142b832119f9e157ef40b80625c53b1334d8d15e8cefcb47687135
    >                     cni.projectcalico.org/podIP: 172.20.177.67/32
    >                     cni.projectcalico.org/podIPs: 172.20.177.67/32
    >                     hash.operator.tigera.io/calico-apiserver-certs: af48e2e6ba5d7e74aa06e90948ef23aef32022b3
    >   Status:           Running
    >   IP:               172.20.177.67
    >   IPs:
    >     IP:           172.20.177.67
    >   Controlled By:  ReplicaSet/calico-apiserver-55f54f87fb
    >   Containers:
    >     calico-apiserver:
    >       Container ID:    containerd://616264e2b694bbc49473fcff259fb7364dded94383746e3ee3e66f00fc012b66
    >       Image:           docker.io/calico/apiserver:v3.26.0
    >       Image ID:        docker.io/calico/apiserver@sha256:7cb5f719499163c172de25d55f1b3fc9bb1b6ea7ad8a1c8259e3eb3ac74890fc
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       Args:
    >         --secure-port=5443
    >         --tls-private-key-file=/calico-apiserver-certs/tls.key
    >         --tls-cert-file=/calico-apiserver-certs/tls.crt
    >       State:          Running
    >         Started:      Mon, 19 Feb 2024 03:59:03 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Liveness:       http-get https://:5443/version delay=90s timeout=1s period=10s #success=1 #failure=3
    >       Readiness:      exec [/code/filecheck] delay=5s timeout=1s period=10s #success=1 #failure=5
    >       Environment:
    >         DATASTORE_TYPE:           kubernetes
    >         KUBERNETES_SERVICE_HOST:  172.24.0.1
    >         KUBERNETES_SERVICE_PORT:  443
    >         MULTI_INTERFACE_MODE:     none
    >       Mounts:
    >         /calico-apiserver-certs from calico-apiserver-certs (ro)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x74lc (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     calico-apiserver-certs:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  calico-apiserver-certs
    >       Optional:    false
    >     kube-api-access-x74lc:
    >       Type:                    Projected (a volume that contains injected data from multiple sources)
    >       TokenExpirationSeconds:  3607
    >       ConfigMapName:           kube-root-ca.crt
    >       ConfigMapOptional:       <nil>
    >       DownwardAPI:             true
    >   QoS Class:                   BestEffort
    >   Node-Selectors:              kubernetes.io/os=linux
    >   Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
    >                                node-role.kubernetes.io/master:NoSchedule
    >                                node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
    >                                node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
    >   Events:
    >     Type     Reason       Age                From               Message
    >     ----     ------       ----               ----               -------
    >     Normal   Scheduled    34m                default-scheduler  Successfully assigned calico-apiserver/calico-apiserver-55f54f87fb-vt8lf to somerville-jade-20240219-work-md-0-fb50a5e8-tjm67
    >     Warning  FailedMount  34m (x3 over 34m)  kubelet            MountVolume.SetUp failed for volume "calico-apiserver-certs" : secret "calico-apiserver-certs" not found
    >     Normal   Pulling      34m                kubelet            Pulling image "docker.io/calico/apiserver:v3.26.0"
    >     Normal   Started      33m                kubelet            Started container calico-apiserver
    >     Warning  Unhealthy    33m (x3 over 33m)  kubelet            Readiness probe failed:


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            describe pod \
                --namespace calico-system \
                calico-kube-controllers-58f5b66895-xw995
        '

    >   Name:                 calico-kube-controllers-58f5b66895-xw995
    >   Namespace:            calico-system
    >   Priority:             2000000000
    >   Priority Class Name:  system-cluster-critical
    >   Service Account:      calico-kube-controllers
    >   Node:                 somerville-jade-20240219-work-control-plane-ac9af912-42bl8/192.168.3.189
    >   Start Time:           Mon, 19 Feb 2024 03:56:18 +0000
    >   Labels:               app.kubernetes.io/name=calico-kube-controllers
    >                         k8s-app=calico-kube-controllers
    >                         pod-template-hash=58f5b66895
    >   Annotations:          cni.projectcalico.org/containerID: 204467b7ba0c1c486ffca746f63f9ef080e2c86ed83c37b670acedbdf7aa1356
    >                         cni.projectcalico.org/podIP: 172.16.197.196/32
    >                         cni.projectcalico.org/podIPs: 172.16.197.196/32
    >                         hash.operator.tigera.io/system: bb4746872201725da2dea19756c475aa67d9c1e9
    >                         hash.operator.tigera.io/tigera-ca-private: 0a5ef91460a81d15d1fd01d33998d6cb3b790c56
    >   Status:               Running
    >   IP:                   172.16.197.196
    >   IPs:
    >     IP:           172.16.197.196
    >   Controlled By:  ReplicaSet/calico-kube-controllers-58f5b66895
    >   Containers:
    >     calico-kube-controllers:
    >       Container ID:    containerd://2fb8f29fb4632259f92a79c7b1066a89005dcb74de3140d6c5bd88da9675120a
    >       Image:           docker.io/calico/kube-controllers:v3.26.0
    >       Image ID:        docker.io/calico/kube-controllers@sha256:a097a09cd7523e6a2ad5b6f7566e68ece5827a77fcbc631d6c34d6d092db4aaa
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       State:           Running
    >         Started:       Mon, 19 Feb 2024 03:57:32 +0000
    >       Ready:           True
    >       Restart Count:   0
    >       Liveness:        exec [/usr/bin/check-status -l] delay=10s timeout=10s period=10s #success=1 #failure=6
    >       Readiness:       exec [/usr/bin/check-status -r] delay=0s timeout=10s period=10s #success=1 #failure=3
    >       Environment:
    >         KUBE_CONTROLLERS_CONFIG_NAME:  default
    >         DATASTORE_TYPE:                kubernetes
    >         ENABLED_CONTROLLERS:           node
    >         FIPS_MODE_ENABLED:             false
    >         KUBERNETES_SERVICE_HOST:       172.24.0.1
    >         KUBERNETES_SERVICE_PORT:       443
    >         CA_CRT_PATH:                   /etc/pki/tls/certs/tigera-ca-bundle.crt
    >       Mounts:
    >         /etc/pki/tls/cert.pem from tigera-ca-bundle (ro,path="ca-bundle.crt")
    >         /etc/pki/tls/certs from tigera-ca-bundle (ro)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nc2t7 (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     tigera-ca-bundle:
    >       Type:      ConfigMap (a volume populated by a ConfigMap)
    >       Name:      tigera-ca-bundle
    >       Optional:  false
    >     kube-api-access-nc2t7:
    >       Type:                    Projected (a volume that contains injected data from multiple sources)
    >       TokenExpirationSeconds:  3607
    >       ConfigMapName:           kube-root-ca.crt
    >       ConfigMapOptional:       <nil>
    >       DownwardAPI:             true
    >   QoS Class:                   BestEffort
    >   Node-Selectors:              kubernetes.io/os=linux
    >   Tolerations:                 CriticalAddonsOnly op=Exists
    >                                node-role.kubernetes.io/control-plane:NoSchedule
    >                                node-role.kubernetes.io/master:NoSchedule
    >                                node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
    >                                node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
    >   Events:
    >     Type     Reason            Age                From               Message
    >     ----     ------            ----               ----               -------
    >     Warning  FailedScheduling  39m                default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node.cloudprovider.kubernetes.io/uninitialized: true}, 2 node(s) had untolerated taint {node.cluster.x-k8s.io/uninitialized: }. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..
    >     Normal   Scheduled         37m                default-scheduler  Successfully assigned calico-system/calico-kube-controllers-58f5b66895-xw995 to somerville-jade-20240219-work-control-plane-ac9af912-42bl8
    >     Normal   Pulling           37m                kubelet            Pulling image "docker.io/calico/kube-controllers:v3.26.0"
    >     Normal   Pulled            36m                kubelet            Successfully pulled image "docker.io/calico/kube-controllers:v3.26.0" in 40.421112946s (58.37187685s including waiting)
    >     Normal   Created           36m                kubelet            Created container calico-kube-controllers
    >     Normal   Started           36m                kubelet            Started container calico-kube-controllers
    >     Warning  Unhealthy         34m (x2 over 34m)  kubelet            Readiness probe failed: Error verifying datastore: Get "https://172.24.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default": context deadline exceeded; Error reaching apiserver: Get "https://172.24.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default": context deadline exceeded with http status code: 500
    >     Warning  Unhealthy         34m (x2 over 34m)  kubelet            Liveness probe failed: Error verifying datastore: Get "https://172.24.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default": context deadline exceeded; Error reaching apiserver: Get "https://172.24.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default": context deadline exceeded with http status code: 500


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            describe pod \
                --namespace calico-system \
                calico-node-b8x47
        '

    >   Name:                 calico-node-b8x47
    >   Namespace:            calico-system
    >   Priority:             2000001000
    >   Priority Class Name:  system-node-critical
    >   Service Account:      calico-node
    >   Node:                 somerville-jade-20240219-work-control-plane-ac9af912-42bl8/192.168.3.189
    >   Start Time:           Mon, 19 Feb 2024 03:54:12 +0000
    >   Labels:               app.kubernetes.io/name=calico-node
    >                         controller-revision-hash=7db8f76d57
    >                         k8s-app=calico-node
    >                         pod-template-generation=1
    >   Annotations:          hash.operator.tigera.io/cni-config: 7c1526fa50c76a4b3650efe703ed353846f576d4
    >                         hash.operator.tigera.io/system: bb4746872201725da2dea19756c475aa67d9c1e9
    >                         hash.operator.tigera.io/tigera-ca-private: 0a5ef91460a81d15d1fd01d33998d6cb3b790c56
    >   Status:               Running
    >   IP:                   192.168.3.189
    >   IPs:
    >     IP:           192.168.3.189
    >   Controlled By:  DaemonSet/calico-node
    >   Init Containers:
    >     flexvol-driver:
    >       Container ID:    containerd://20fbef32ab5df5be7f4442c27dd3131181c02a3c7797aa5b10a7b7919123c8b5
    >       Image:           docker.io/calico/pod2daemon-flexvol:v3.26.0
    >       Image ID:        docker.io/calico/pod2daemon-flexvol@sha256:f8a737c47bfef9f070c50f02332d931cb3d19d544f7b1867fee0a99cb0626356
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       State:           Terminated
    >         Reason:        Completed
    >         Exit Code:     0
    >         Started:       Mon, 19 Feb 2024 03:54:48 +0000
    >         Finished:      Mon, 19 Feb 2024 03:54:48 +0000
    >       Ready:           True
    >       Restart Count:   0
    >       Environment:     <none>
    >       Mounts:
    >         /host/driver from flexvol-driver-host (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlsjl (ro)
    >     install-cni:
    >       Container ID:    containerd://8419f30fe293e7dbbb42792de9784649b9576eaf0d654cf79abbd153374daf45
    >       Image:           docker.io/calico/cni:v3.26.0
    >       Image ID:        docker.io/calico/cni@sha256:c7c80d82dc4f85ac4d7f2345c940bc7f818bbea03c2043df89923c032d8ee7fc
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       Command:
    >         /opt/cni/bin/install
    >       State:          Terminated
    >         Reason:       Completed
    >         Exit Code:    0
    >         Started:      Mon, 19 Feb 2024 03:55:12 +0000
    >         Finished:     Mon, 19 Feb 2024 03:55:32 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Environment:
    >         CNI_CONF_NAME:            10-calico.conflist
    >         SLEEP:                    false
    >         CNI_NET_DIR:              /etc/cni/net.d
    >         CNI_NETWORK_CONFIG:       <set to the key 'config' of config map 'cni-config'>  Optional: false
    >         KUBERNETES_SERVICE_HOST:  172.24.0.1
    >         KUBERNETES_SERVICE_PORT:  443
    >       Mounts:
    >         /host/etc/cni/net.d from cni-net-dir (rw)
    >         /host/opt/cni/bin from cni-bin-dir (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlsjl (ro)
    >   Containers:
    >     calico-node:
    >       Container ID:    containerd://3c0fff2c7a015fb55aec3fdb231222b7918ae4f10bb18bcc9f9ba9487b932bd8
    >       Image:           docker.io/calico/node:v3.26.0
    >       Image ID:        docker.io/calico/node@sha256:5086f1ef0287886811fea4d545a4bbb75d9345367b1b1ad1aa4447af2ecbc4ea
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       State:           Running
    >         Started:       Mon, 19 Feb 2024 03:55:53 +0000
    >       Ready:           True
    >       Restart Count:   0
    >       Liveness:        http-get http://localhost:9099/liveness delay=0s timeout=10s period=10s #success=1 #failure=3
    >       Readiness:       exec [/bin/calico-node -felix-ready] delay=0s timeout=5s period=10s #success=1 #failure=3
    >       Environment:
    >         DATASTORE_TYPE:                      kubernetes
    >         WAIT_FOR_DATASTORE:                  true
    >         CLUSTER_TYPE:                        k8s,operator
    >         CALICO_DISABLE_FILE_LOGGING:         false
    >         FELIX_DEFAULTENDPOINTTOHOSTACTION:   ACCEPT
    >         FELIX_HEALTHENABLED:                 true
    >         FELIX_HEALTHPORT:                    9099
    >         NODENAME:                             (v1:spec.nodeName)
    >         NAMESPACE:                           calico-system (v1:metadata.namespace)
    >         FELIX_TYPHAK8SNAMESPACE:             calico-system
    >         FELIX_TYPHAK8SSERVICENAME:           calico-typha
    >         FELIX_TYPHACAFILE:                   /etc/pki/tls/certs/tigera-ca-bundle.crt
    >         FELIX_TYPHACERTFILE:                 /node-certs/tls.crt
    >         FELIX_TYPHAKEYFILE:                  /node-certs/tls.key
    >         FIPS_MODE_ENABLED:                   false
    >         FELIX_TYPHACN:                       typha-server
    >         CALICO_MANAGE_CNI:                   true
    >         CALICO_IPV4POOL_CIDR:                172.16.0.0/13
    >         CALICO_IPV4POOL_VXLAN:               Always
    >         CALICO_IPV4POOL_BLOCK_SIZE:          26
    >         CALICO_IPV4POOL_NODE_SELECTOR:       all()
    >         CALICO_IPV4POOL_DISABLE_BGP_EXPORT:  false
    >         CALICO_NETWORKING_BACKEND:           vxlan
    >         IP:                                  autodetect
    >         IP_AUTODETECTION_METHOD:             kubernetes-internal-ip
    >         IP6:                                 none
    >         FELIX_IPV6SUPPORT:                   false
    >         KUBERNETES_SERVICE_HOST:             172.24.0.1
    >         KUBERNETES_SERVICE_PORT:             443
    >       Mounts:
    >         /etc/pki/tls/cert.pem from tigera-ca-bundle (ro,path="ca-bundle.crt")
    >         /etc/pki/tls/certs from tigera-ca-bundle (ro)
    >         /host/etc/cni/net.d from cni-net-dir (rw)
    >         /lib/modules from lib-modules (ro)
    >         /node-certs from node-certs (ro)
    >         /run/xtables.lock from xtables-lock (rw)
    >         /var/lib/calico from var-lib-calico (rw)
    >         /var/log/calico/cni from cni-log-dir (rw)
    >         /var/run/calico from var-run-calico (rw)
    >         /var/run/nodeagent from policysync (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlsjl (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     lib-modules:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /lib/modules
    >       HostPathType:
    >     xtables-lock:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /run/xtables.lock
    >       HostPathType:  FileOrCreate
    >     policysync:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/run/nodeagent
    >       HostPathType:  DirectoryOrCreate
    >     tigera-ca-bundle:
    >       Type:      ConfigMap (a volume populated by a ConfigMap)
    >       Name:      tigera-ca-bundle
    >       Optional:  false
    >     node-certs:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  node-certs
    >       Optional:    false
    >     var-run-calico:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/run/calico
    >       HostPathType:
    >     var-lib-calico:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/calico
    >       HostPathType:
    >     cni-bin-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /opt/cni/bin
    >       HostPathType:
    >     cni-net-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/cni/net.d
    >       HostPathType:
    >     cni-log-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/log/calico/cni
    >       HostPathType:
    >     flexvol-driver-host:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
    >       HostPathType:  DirectoryOrCreate
    >     kube-api-access-rlsjl:
    >       Type:                    Projected (a volume that contains injected data from multiple sources)
    >       TokenExpirationSeconds:  3607
    >       ConfigMapName:           kube-root-ca.crt
    >       ConfigMapOptional:       <nil>
    >       DownwardAPI:             true
    >   QoS Class:                   BestEffort
    >   Node-Selectors:              kubernetes.io/os=linux
    >   Tolerations:                 :NoSchedule op=Exists
    >                                :NoExecute op=Exists
    >                                CriticalAddonsOnly op=Exists
    >                                node.kubernetes.io/disk-pressure:NoSchedule op=Exists
    >                                node.kubernetes.io/memory-pressure:NoSchedule op=Exists
    >                                node.kubernetes.io/network-unavailable:NoSchedule op=Exists
    >                                node.kubernetes.io/not-ready:NoExecute op=Exists
    >                                node.kubernetes.io/pid-pressure:NoSchedule op=Exists
    >                                node.kubernetes.io/unreachable:NoExecute op=Exists
    >                                node.kubernetes.io/unschedulable:NoSchedule op=Exists
    >   Events:
    >     Type     Reason     Age   From               Message
    >     ----     ------     ----  ----               -------
    >     Normal   Scheduled  42m   default-scheduler  Successfully assigned calico-system/calico-node-b8x47 to somerville-jade-20240219-work-control-plane-ac9af912-42bl8
    >     Normal   Pulling    41m   kubelet            Pulling image "docker.io/calico/pod2daemon-flexvol:v3.26.0"
    >     Normal   Pulled     41m   kubelet            Successfully pulled image "docker.io/calico/pod2daemon-flexvol:v3.26.0" in 32.00622891s (32.00626557s including waiting)
    >     Normal   Created    41m   kubelet            Created container flexvol-driver
    >     Normal   Started    41m   kubelet            Started container flexvol-driver
    >     Normal   Pulling    41m   kubelet            Pulling image "docker.io/calico/cni:v3.26.0"
    >     Normal   Pulled     41m   kubelet            Successfully pulled image "docker.io/calico/cni:v3.26.0" in 19.35115177s (19.351158102s including waiting)
    >     Normal   Created    41m   kubelet            Created container install-cni
    >     Normal   Started    41m   kubelet            Started container install-cni
    >     Normal   Pulling    40m   kubelet            Pulling image "docker.io/calico/node:v3.26.0"
    >     Normal   Pulled     40m   kubelet            Successfully pulled image "docker.io/calico/node:v3.26.0" in 17.790260733s (17.790406434s including waiting)
    >     Normal   Created    40m   kubelet            Created container calico-node
    >     Normal   Started    40m   kubelet            Started container calico-node
    >     Warning  Unhealthy  40m   kubelet            Readiness probe failed: calico/node is not ready: felix is not ready: Get "http://localhost:9099/readiness": dial tcp [::1]:9099: connect: connection refused
    >   W0219 03:55:54.134184      15 feature_gate.go:241] Setting GA feature gate ServiceInternalTrafficPolicy=true. It will be removed in a future release.
    >     Warning  Unhealthy  40m  kubelet  Readiness probe failed: calico/node is not ready: felix is not ready: Get "http://localhost:9099/readiness": dial tcp [::1]:9099: connect: connection refused
    >   W0219 03:55:55.046074      34 feature_gate.go:241] Setting GA feature gate ServiceInternalTrafficPolicy=true. It will be removed in a future release.
    >     Warning  Unhealthy  40m  kubelet  Liveness probe failed: Get "http://localhost:9099/liveness": dial tcp [::1]:9099: connect: connection refused
    >     Warning  Unhealthy  40m  kubelet  Readiness probe failed: calico/node is not ready: felix is not ready: Get "http://localhost:9099/readiness": dial tcp [::1]:9099: connect: connection refused
    >   W0219 03:55:55.422654      46 feature_gate.go:241] Setting GA feature gate ServiceInternalTrafficPolicy=true. It will be removed in a future release.


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            describe pod \
                --namespace calico-system \
                calico-node-l6pk6
        '

    >   Name:                 calico-node-l6pk6
    >   Namespace:            calico-system
    >   Priority:             2000001000
    >   Priority Class Name:  system-node-critical
    >   Service Account:      calico-node
    >   Node:                 somerville-jade-20240219-work-md-0-fb50a5e8-tjm67/192.168.3.79
    >   Start Time:           Mon, 19 Feb 2024 03:54:12 +0000
    >   Labels:               app.kubernetes.io/name=calico-node
    >                         controller-revision-hash=7db8f76d57
    >                         k8s-app=calico-node
    >                         pod-template-generation=1
    >   Annotations:          hash.operator.tigera.io/cni-config: 7c1526fa50c76a4b3650efe703ed353846f576d4
    >                         hash.operator.tigera.io/system: bb4746872201725da2dea19756c475aa67d9c1e9
    >                         hash.operator.tigera.io/tigera-ca-private: 0a5ef91460a81d15d1fd01d33998d6cb3b790c56
    >   Status:               Running
    >   IP:                   192.168.3.79
    >   IPs:
    >     IP:           192.168.3.79
    >   Controlled By:  DaemonSet/calico-node
    >   Init Containers:
    >     flexvol-driver:
    >       Container ID:    containerd://9102c1839d2b201e7c37b1e8e813b9083ab8fdf66cfe57c1de7006cd873f040d
    >       Image:           docker.io/calico/pod2daemon-flexvol:v3.26.0
    >       Image ID:        docker.io/calico/pod2daemon-flexvol@sha256:f8a737c47bfef9f070c50f02332d931cb3d19d544f7b1867fee0a99cb0626356
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       State:           Terminated
    >         Reason:        Completed
    >         Exit Code:     0
    >         Started:       Mon, 19 Feb 2024 03:56:25 +0000
    >         Finished:      Mon, 19 Feb 2024 03:56:25 +0000
    >       Ready:           True
    >       Restart Count:   0
    >       Environment:     <none>
    >       Mounts:
    >         /host/driver from flexvol-driver-host (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-88vlh (ro)
    >     install-cni:
    >       Container ID:    containerd://3a868cbfceef1bb553689f3148bce39c2a95f9a1ed5ec7e02cc0016a05f8ddad
    >       Image:           docker.io/calico/cni:v3.26.0
    >       Image ID:        docker.io/calico/cni@sha256:c7c80d82dc4f85ac4d7f2345c940bc7f818bbea03c2043df89923c032d8ee7fc
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       Command:
    >         /opt/cni/bin/install
    >       State:          Terminated
    >         Reason:       Completed
    >         Exit Code:    0
    >         Started:      Mon, 19 Feb 2024 03:57:02 +0000
    >         Finished:     Mon, 19 Feb 2024 03:57:24 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Environment:
    >         CNI_CONF_NAME:            10-calico.conflist
    >         SLEEP:                    false
    >         CNI_NET_DIR:              /etc/cni/net.d
    >         CNI_NETWORK_CONFIG:       <set to the key 'config' of config map 'cni-config'>  Optional: false
    >         KUBERNETES_SERVICE_HOST:  172.24.0.1
    >         KUBERNETES_SERVICE_PORT:  443
    >       Mounts:
    >         /host/etc/cni/net.d from cni-net-dir (rw)
    >         /host/opt/cni/bin from cni-bin-dir (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-88vlh (ro)
    >   Containers:
    >     calico-node:
    >       Container ID:    containerd://a55e9e255083c37bad926729f41b282c4a54816020819fd7eeb2256ae86aafaf
    >       Image:           docker.io/calico/node:v3.26.0
    >       Image ID:        docker.io/calico/node@sha256:5086f1ef0287886811fea4d545a4bbb75d9345367b1b1ad1aa4447af2ecbc4ea
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       State:           Running
    >         Started:       Mon, 19 Feb 2024 03:57:51 +0000
    >       Ready:           True
    >       Restart Count:   0
    >       Liveness:        http-get http://localhost:9099/liveness delay=0s timeout=10s period=10s #success=1 #failure=3
    >       Readiness:       exec [/bin/calico-node -felix-ready] delay=0s timeout=5s period=10s #success=1 #failure=3
    >       Environment:
    >         DATASTORE_TYPE:                      kubernetes
    >         WAIT_FOR_DATASTORE:                  true
    >         CLUSTER_TYPE:                        k8s,operator
    >         CALICO_DISABLE_FILE_LOGGING:         false
    >         FELIX_DEFAULTENDPOINTTOHOSTACTION:   ACCEPT
    >         FELIX_HEALTHENABLED:                 true
    >         FELIX_HEALTHPORT:                    9099
    >         NODENAME:                             (v1:spec.nodeName)
    >         NAMESPACE:                           calico-system (v1:metadata.namespace)
    >         FELIX_TYPHAK8SNAMESPACE:             calico-system
    >         FELIX_TYPHAK8SSERVICENAME:           calico-typha
    >         FELIX_TYPHACAFILE:                   /etc/pki/tls/certs/tigera-ca-bundle.crt
    >         FELIX_TYPHACERTFILE:                 /node-certs/tls.crt
    >         FELIX_TYPHAKEYFILE:                  /node-certs/tls.key
    >         FIPS_MODE_ENABLED:                   false
    >         FELIX_TYPHACN:                       typha-server
    >         CALICO_MANAGE_CNI:                   true
    >         CALICO_IPV4POOL_CIDR:                172.16.0.0/13
    >         CALICO_IPV4POOL_VXLAN:               Always
    >         CALICO_IPV4POOL_BLOCK_SIZE:          26
    >         CALICO_IPV4POOL_NODE_SELECTOR:       all()
    >         CALICO_IPV4POOL_DISABLE_BGP_EXPORT:  false
    >         CALICO_NETWORKING_BACKEND:           vxlan
    >         IP:                                  autodetect
    >         IP_AUTODETECTION_METHOD:             kubernetes-internal-ip
    >         IP6:                                 none
    >         FELIX_IPV6SUPPORT:                   false
    >         KUBERNETES_SERVICE_HOST:             172.24.0.1
    >         KUBERNETES_SERVICE_PORT:             443
    >       Mounts:
    >         /etc/pki/tls/cert.pem from tigera-ca-bundle (ro,path="ca-bundle.crt")
    >         /etc/pki/tls/certs from tigera-ca-bundle (ro)
    >         /host/etc/cni/net.d from cni-net-dir (rw)
    >         /lib/modules from lib-modules (ro)
    >         /node-certs from node-certs (ro)
    >         /run/xtables.lock from xtables-lock (rw)
    >         /var/lib/calico from var-lib-calico (rw)
    >         /var/log/calico/cni from cni-log-dir (rw)
    >         /var/run/calico from var-run-calico (rw)
    >         /var/run/nodeagent from policysync (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-88vlh (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     lib-modules:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /lib/modules
    >       HostPathType:
    >     xtables-lock:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /run/xtables.lock
    >       HostPathType:  FileOrCreate
    >     policysync:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/run/nodeagent
    >       HostPathType:  DirectoryOrCreate
    >     tigera-ca-bundle:
    >       Type:      ConfigMap (a volume populated by a ConfigMap)
    >       Name:      tigera-ca-bundle
    >       Optional:  false
    >     node-certs:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  node-certs
    >       Optional:    false
    >     var-run-calico:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/run/calico
    >       HostPathType:
    >     var-lib-calico:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/calico
    >       HostPathType:
    >     cni-bin-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /opt/cni/bin
    >       HostPathType:
    >     cni-net-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/cni/net.d
    >       HostPathType:
    >     cni-log-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/log/calico/cni
    >       HostPathType:
    >     flexvol-driver-host:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
    >       HostPathType:  DirectoryOrCreate
    >     kube-api-access-88vlh:
    >       Type:                    Projected (a volume that contains injected data from multiple sources)
    >       TokenExpirationSeconds:  3607
    >       ConfigMapName:           kube-root-ca.crt
    >       ConfigMapOptional:       <nil>
    >       DownwardAPI:             true
    >   QoS Class:                   BestEffort
    >   Node-Selectors:              kubernetes.io/os=linux
    >   Tolerations:                 :NoSchedule op=Exists
    >                                :NoExecute op=Exists
    >                                CriticalAddonsOnly op=Exists
    >                                node.kubernetes.io/disk-pressure:NoSchedule op=Exists
    >                                node.kubernetes.io/memory-pressure:NoSchedule op=Exists
    >                                node.kubernetes.io/network-unavailable:NoSchedule op=Exists
    >                                node.kubernetes.io/not-ready:NoExecute op=Exists
    >                                node.kubernetes.io/pid-pressure:NoSchedule op=Exists
    >                                node.kubernetes.io/unreachable:NoExecute op=Exists
    >                                node.kubernetes.io/unschedulable:NoSchedule op=Exists
    >   Events:
    >     Type     Reason     Age   From               Message
    >     ----     ------     ----  ----               -------
    >     Normal   Scheduled  44m   default-scheduler  Successfully assigned calico-system/calico-node-l6pk6 to somerville-jade-20240219-work-md-0-fb50a5e8-tjm67
    >     Normal   Pulling    44m   kubelet            Pulling image "docker.io/calico/pod2daemon-flexvol:v3.26.0"
    >     Normal   Pulled     42m   kubelet            Successfully pulled image "docker.io/calico/pod2daemon-flexvol:v3.26.0" in 59.726291223s (2m6.388537578s including waiting)
    >     Normal   Created    42m   kubelet            Created container flexvol-driver
    >     Normal   Started    42m   kubelet            Started container flexvol-driver
    >     Normal   Pulling    42m   kubelet            Pulling image "docker.io/calico/cni:v3.26.0"
    >     Normal   Pulled     41m   kubelet            Successfully pulled image "docker.io/calico/cni:v3.26.0" in 25.843487971s (32.573775437s including waiting)
    >     Normal   Created    41m   kubelet            Created container install-cni
    >     Normal   Started    41m   kubelet            Started container install-cni
    >     Normal   Pulling    41m   kubelet            Pulling image "docker.io/calico/node:v3.26.0"
    >     Normal   Pulled     40m   kubelet            Successfully pulled image "docker.io/calico/node:v3.26.0" in 16.371388206s (25.907222259s including waiting)
    >     Normal   Created    40m   kubelet            Created container calico-node
    >     Normal   Started    40m   kubelet            Started container calico-node
    >     Warning  Unhealthy  40m   kubelet            Readiness probe failed: calico/node is not ready: felix is not ready: Get "http://localhost:9099/readiness": dial tcp [::1]:9099: connect: connection refused
    >   W0219 03:57:52.385491      51 feature_gate.go:241] Setting GA feature gate ServiceInternalTrafficPolicy=true. It will be removed in a future release.
    >     Warning  Unhealthy  40m  kubelet  Readiness probe failed: calico/node is not ready: felix is not ready: Get "http://localhost:9099/readiness": dial tcp [::1]:9099: connect: connection refused
    >   W0219 03:57:53.389536      75 feature_gate.go:241] Setting GA feature gate ServiceInternalTrafficPolicy=true. It will be removed in a future release.
    >     Warning  Unhealthy  40m  kubelet  Liveness probe failed: Get "http://localhost:9099/liveness": dial tcp [::1]:9099: connect: connection refused
    >     Warning  Unhealthy  40m  kubelet  Readiness probe failed: calico/node is not ready: felix is not ready: readiness probe reporting 503
    >   W0219 03:57:55.483055     191 feature_gate.go:241] Setting GA feature gate ServiceInternalTrafficPolicy=true. It will be removed in a future release.


    # Skipped ...
    # calico-system calico-node-ld6v5
    # calico-system calico-node-r9kwb
    # calico-system calico-node-rkdbq


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            describe pod \
                --namespace calico-system \
                calico-typha-96774b784-d6wfz
        '

    >   Name:                 calico-typha-96774b784-d6wfz
    >   Namespace:            calico-system
    >   Priority:             2000000000
    >   Priority Class Name:  system-cluster-critical
    >   Service Account:      calico-typha
    >   Node:                 somerville-jade-20240219-work-control-plane-ac9af912-c9ldg/192.168.3.210
    >   Start Time:           Mon, 19 Feb 2024 04:01:05 +0000
    >   Labels:               app.kubernetes.io/name=calico-typha
    >                         k8s-app=calico-typha
    >                         pod-template-hash=96774b784
    >   Annotations:          hash.operator.tigera.io/system: bb4746872201725da2dea19756c475aa67d9c1e9
    >                         hash.operator.tigera.io/tigera-ca-private: 0a5ef91460a81d15d1fd01d33998d6cb3b790c56
    >                         hash.operator.tigera.io/typha-certs: 510e2f51ab36184fa8953a67a8a7b3ebdeee4c39
    >   Status:               Running
    >   IP:                   192.168.3.210
    >   IPs:
    >     IP:           192.168.3.210
    >   Controlled By:  ReplicaSet/calico-typha-96774b784
    >   Containers:
    >     calico-typha:
    >       Container ID:    containerd://5250de8ee209251718368729f19c8d40311bff13a0b6937f5b70c0de0bdeff7c
    >       Image:           docker.io/calico/typha:v3.26.0
    >       Image ID:        docker.io/calico/typha@sha256:32328a796f86ee18660e77b22a494369b07abe749a0f184b65d922e17beaeee1
    >       Port:            5473/TCP
    >       Host Port:       5473/TCP
    >       SeccompProfile:  RuntimeDefault
    >       State:           Running
    >         Started:       Mon, 19 Feb 2024 04:01:16 +0000
    >       Ready:           True
    >       Restart Count:   0
    >       Liveness:        http-get http://localhost:9098/liveness delay=0s timeout=10s period=10s #success=1 #failure=3
    >       Readiness:       http-get http://localhost:9098/readiness delay=0s timeout=10s period=10s #success=1 #failure=3
    >       Environment:
    >         TYPHA_LOGSEVERITYSCREEN:          info
    >         TYPHA_LOGFILEPATH:                none
    >         TYPHA_LOGSEVERITYSYS:             none
    >         TYPHA_CONNECTIONREBALANCINGMODE:  kubernetes
    >         TYPHA_DATASTORETYPE:              kubernetes
    >         TYPHA_HEALTHENABLED:              true
    >         TYPHA_HEALTHPORT:                 9098
    >         TYPHA_K8SNAMESPACE:               calico-system
    >         TYPHA_CAFILE:                     /etc/pki/tls/certs/tigera-ca-bundle.crt
    >         TYPHA_SERVERCERTFILE:             /typha-certs/tls.crt
    >         TYPHA_SERVERKEYFILE:              /typha-certs/tls.key
    >         TYPHA_FIPSMODEENABLED:            false
    >         TYPHA_SHUTDOWNTIMEOUTSECS:        300
    >         TYPHA_CLIENTCN:                   typha-client
    >         KUBERNETES_SERVICE_HOST:          172.24.0.1
    >         KUBERNETES_SERVICE_PORT:          443
    >       Mounts:
    >         /etc/pki/tls/cert.pem from tigera-ca-bundle (ro,path="ca-bundle.crt")
    >         /etc/pki/tls/certs from tigera-ca-bundle (ro)
    >         /typha-certs from typha-certs (ro)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kn4w9 (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     tigera-ca-bundle:
    >       Type:      ConfigMap (a volume populated by a ConfigMap)
    >       Name:      tigera-ca-bundle
    >       Optional:  false
    >     typha-certs:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  typha-certs
    >       Optional:    false
    >     kube-api-access-kn4w9:
    >       Type:                    Projected (a volume that contains injected data from multiple sources)
    >       TokenExpirationSeconds:  3607
    >       ConfigMapName:           kube-root-ca.crt
    >       ConfigMapOptional:       <nil>
    >       DownwardAPI:             true
    >   QoS Class:                   BestEffort
    >   Node-Selectors:              kubernetes.io/os=linux
    >   Tolerations:                 :NoSchedule op=Exists
    >                                :NoExecute op=Exists
    >                                CriticalAddonsOnly op=Exists
    >   Events:
    >     Type     Reason     Age   From               Message
    >     ----     ------     ----  ----               -------
    >     Normal   Scheduled  39m   default-scheduler  Successfully assigned calico-system/calico-typha-96774b784-d6wfz to somerville-jade-20240219-work-control-plane-ac9af912-c9ldg
    >     Normal   Pulling    39m   kubelet            Pulling image "docker.io/calico/typha:v3.26.0"
    >     Normal   Pulled     39m   kubelet            Successfully pulled image "docker.io/calico/typha:v3.26.0" in 3.47259572s (8.486792185s including waiting)
    >     Normal   Created    39m   kubelet            Created container calico-typha
    >     Normal   Started    39m   kubelet            Started container calico-typha
    >     Warning  Unhealthy  39m   kubelet            Readiness probe failed: HTTP probe failed with statuscode: 503


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            describe pod \
                --namespace calico-system \
                calico-typha-96774b784-kzsht
        '

    >   Name:                 calico-typha-96774b784-kzsht
    >   Namespace:            calico-system
    >   Priority:             2000000000
    >   Priority Class Name:  system-cluster-critical
    >   Service Account:      calico-typha
    >   Node:                 somerville-jade-20240219-work-md-0-fb50a5e8-ncwdq/192.168.3.177
    >   Start Time:           Mon, 19 Feb 2024 03:54:19 +0000
    >   Labels:               app.kubernetes.io/name=calico-typha
    >                         k8s-app=calico-typha
    >                         pod-template-hash=96774b784
    >   Annotations:          hash.operator.tigera.io/system: bb4746872201725da2dea19756c475aa67d9c1e9
    >                         hash.operator.tigera.io/tigera-ca-private: 0a5ef91460a81d15d1fd01d33998d6cb3b790c56
    >                         hash.operator.tigera.io/typha-certs: 510e2f51ab36184fa8953a67a8a7b3ebdeee4c39
    >   Status:               Running
    >   IP:                   192.168.3.177
    >   IPs:
    >     IP:           192.168.3.177
    >   Controlled By:  ReplicaSet/calico-typha-96774b784
    >   Containers:
    >     calico-typha:
    >       Container ID:    containerd://4039dd4f2801f2345c39dca4725035f53e0ab8daa2046b5478c85d932c55b08b
    >       Image:           docker.io/calico/typha:v3.26.0
    >       Image ID:        docker.io/calico/typha@sha256:32328a796f86ee18660e77b22a494369b07abe749a0f184b65d922e17beaeee1
    >       Port:            5473/TCP
    >       Host Port:       5473/TCP
    >       SeccompProfile:  RuntimeDefault
    >       State:           Running
    >         Started:       Mon, 19 Feb 2024 03:54:23 +0000
    >       Ready:           True
    >       Restart Count:   0
    >       Liveness:        http-get http://localhost:9098/liveness delay=0s timeout=10s period=10s #success=1 #failure=3
    >       Readiness:       http-get http://localhost:9098/readiness delay=0s timeout=10s period=10s #success=1 #failure=3
    >       Environment:
    >         TYPHA_LOGSEVERITYSCREEN:          info
    >         TYPHA_LOGFILEPATH:                none
    >         TYPHA_LOGSEVERITYSYS:             none
    >         TYPHA_CONNECTIONREBALANCINGMODE:  kubernetes
    >         TYPHA_DATASTORETYPE:              kubernetes
    >         TYPHA_HEALTHENABLED:              true
    >         TYPHA_HEALTHPORT:                 9098
    >         TYPHA_K8SNAMESPACE:               calico-system
    >         TYPHA_CAFILE:                     /etc/pki/tls/certs/tigera-ca-bundle.crt
    >         TYPHA_SERVERCERTFILE:             /typha-certs/tls.crt
    >         TYPHA_SERVERKEYFILE:              /typha-certs/tls.key
    >         TYPHA_FIPSMODEENABLED:            false
    >         TYPHA_SHUTDOWNTIMEOUTSECS:        300
    >         TYPHA_CLIENTCN:                   typha-client
    >         KUBERNETES_SERVICE_HOST:          172.24.0.1
    >         KUBERNETES_SERVICE_PORT:          443
    >       Mounts:
    >         /etc/pki/tls/cert.pem from tigera-ca-bundle (ro,path="ca-bundle.crt")
    >         /etc/pki/tls/certs from tigera-ca-bundle (ro)
    >         /typha-certs from typha-certs (ro)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-grpxb (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     tigera-ca-bundle:
    >       Type:      ConfigMap (a volume populated by a ConfigMap)
    >       Name:      tigera-ca-bundle
    >       Optional:  false
    >     typha-certs:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  typha-certs
    >       Optional:    false
    >     kube-api-access-grpxb:
    >       Type:                    Projected (a volume that contains injected data from multiple sources)
    >       TokenExpirationSeconds:  3607
    >       ConfigMapName:           kube-root-ca.crt
    >       ConfigMapOptional:       <nil>
    >       DownwardAPI:             true
    >   QoS Class:                   BestEffort
    >   Node-Selectors:              kubernetes.io/os=linux
    >   Tolerations:                 :NoSchedule op=Exists
    >                                :NoExecute op=Exists
    >                                CriticalAddonsOnly op=Exists
    >   Events:
    >     Type     Reason     Age                From               Message
    >     ----     ------     ----               ----               -------
    >     Normal   Scheduled  48m                default-scheduler  Successfully assigned calico-system/calico-typha-96774b784-kzsht to somerville-jade-20240219-work-md-0-fb50a5e8-ncwdq
    >     Normal   Pulling    48m                kubelet            Pulling image "docker.io/calico/typha:v3.26.0"
    >     Normal   Pulled     48m                kubelet            Successfully pulled image "docker.io/calico/typha:v3.26.0" in 3.57713725s (3.577235729s including waiting)
    >     Normal   Created    48m                kubelet            Created container calico-typha
    >     Normal   Started    48m                kubelet            Started container calico-typha
    >     Warning  Unhealthy  48m (x2 over 48m)  kubelet            Readiness probe failed: Get "http://localhost:9098/readiness": dial tcp [::1]:9098: connect: connection refused


    # Skipped ...
    # calico-system calico-typha-96774b784-rm5ps


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            describe pod \
                --namespace calico-system \
                csi-node-driver-cs2df
        '

    >   Name:                 csi-node-driver-cs2df
    >   Namespace:            calico-system
    >   Priority:             2000001000
    >   Priority Class Name:  system-node-critical
    >   Service Account:      default
    >   Node:                 somerville-jade-20240219-work-control-plane-ac9af912-c9ldg/192.168.3.210
    >   Start Time:           Mon, 19 Feb 2024 04:01:05 +0000
    >   Labels:               app.kubernetes.io/name=csi-node-driver
    >                         controller-revision-hash=746cb5f497
    >                         k8s-app=csi-node-driver
    >                         name=csi-node-driver
    >                         pod-template-generation=1
    >   Annotations:          cni.projectcalico.org/containerID: ce60484c0ba791123dfef8fe92110724cb722ed330c4c4920c73e5f67b640f6d
    >                         cni.projectcalico.org/podIP: 172.16.120.1/32
    >                         cni.projectcalico.org/podIPs: 172.16.120.1/32
    >   Status:               Running
    >   IP:                   172.16.120.1
    >   IPs:
    >     IP:           172.16.120.1
    >   Controlled By:  DaemonSet/csi-node-driver
    >   Containers:
    >     calico-csi:
    >       Container ID:    containerd://286af6ecbe0ef7522989d84dd5f74444465f6c2ca7a288ccdbfa43508ac0c76b
    >       Image:           docker.io/calico/csi:v3.26.0
    >       Image ID:        docker.io/calico/csi@sha256:b078c61d3c8a1395bdb6a441c1318e21dfa4597ed18a5706495c49d515330d64
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       Args:
    >         --nodeid=$(KUBE_NODE_NAME)
    >         --loglevel=$(LOG_LEVEL)
    >       State:          Running
    >         Started:      Mon, 19 Feb 2024 04:01:39 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Environment:
    >         LOG_LEVEL:       warn
    >         KUBE_NODE_NAME:   (v1:spec.nodeName)
    >       Mounts:
    >         /csi from socket-dir (rw)
    >         /etc/calico from etccalico (rw)
    >         /var/lib/kubelet from kubelet-dir (rw)
    >         /var/run from varrun (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gn7hd (ro)
    >     csi-node-driver-registrar:
    >       Container ID:    containerd://ae70615f0e384b5f523b4c75be84e81a25a06fa3c3ea457e6ca5c8679bdbf82e
    >       Image:           docker.io/calico/node-driver-registrar:v3.26.0
    >       Image ID:        docker.io/calico/node-driver-registrar@sha256:af9458100d80e83308d27bde8f65ab1cc647c96094134082f863c38929afc3c0
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       Args:
    >         --v=5
    >         --csi-address=$(ADDRESS)
    >         --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
    >       State:          Running
    >         Started:      Mon, 19 Feb 2024 04:01:42 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Environment:
    >         ADDRESS:               /csi/csi.sock
    >         DRIVER_REG_SOCK_PATH:  /var/lib/kubelet/plugins/csi.tigera.io/csi.sock
    >         KUBE_NODE_NAME:         (v1:spec.nodeName)
    >       Mounts:
    >         /csi from socket-dir (rw)
    >         /registration from registration-dir (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gn7hd (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     varrun:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/run
    >       HostPathType:
    >     etccalico:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/calico
    >       HostPathType:
    >     kubelet-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet
    >       HostPathType:  Directory
    >     socket-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet/plugins/csi.tigera.io
    >       HostPathType:  DirectoryOrCreate
    >     registration-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet/plugins_registry
    >       HostPathType:  Directory
    >     kube-api-access-gn7hd:
    >       Type:                    Projected (a volume that contains injected data from multiple sources)
    >       TokenExpirationSeconds:  3607
    >       ConfigMapName:           kube-root-ca.crt
    >       ConfigMapOptional:       <nil>
    >       DownwardAPI:             true
    >   QoS Class:                   BestEffort
    >   Node-Selectors:              kubernetes.io/os=linux
    >   Tolerations:                 :NoSchedule op=Exists
    >                                :NoExecute op=Exists
    >                                CriticalAddonsOnly op=Exists
    >                                node.kubernetes.io/disk-pressure:NoSchedule op=Exists
    >                                node.kubernetes.io/memory-pressure:NoSchedule op=Exists
    >                                node.kubernetes.io/not-ready:NoExecute op=Exists
    >                                node.kubernetes.io/pid-pressure:NoSchedule op=Exists
    >                                node.kubernetes.io/unreachable:NoExecute op=Exists
    >                                node.kubernetes.io/unschedulable:NoSchedule op=Exists
    >   Events:
    >     Type     Reason                  Age                 From               Message
    >     ----     ------                  ----                ----               -------
    >     Normal   Scheduled               43m                 default-scheduler  Successfully assigned calico-system/csi-node-driver-cs2df to somerville-jade-20240219-work-control-plane-ac9af912-c9ldg
    >     Warning  NetworkNotReady         43m (x10 over 43m)  kubelet            network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized
    >     Warning  FailedCreatePodSandBox  42m                 kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "026a94d69be2b8c14d0c3ad184b0f798df711461d79f91b7f660d71b499627a1": plugin type="calico" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/
    >     Normal   SandboxChanged          42m (x2 over 42m)   kubelet            Pod sandbox changed, it will be killed and re-created.
    >     Normal   Pulling                 42m                 kubelet            Pulling image "docker.io/calico/csi:v3.26.0"
    >     Normal   Pulled                  42m                 kubelet            Successfully pulled image "docker.io/calico/csi:v3.26.0" in 2.640023638s (2.640030301s including waiting)
    >     Normal   Created                 42m                 kubelet            Created container calico-csi
    >     Normal   Started                 42m                 kubelet            Started container calico-csi
    >     Normal   Pulling                 42m                 kubelet            Pulling image "docker.io/calico/node-driver-registrar:v3.26.0"
    >     Normal   Pulled                  42m                 kubelet            Successfully pulled image "docker.io/calico/node-driver-registrar:v3.26.0" in 2.472652574s (2.472766659s including waiting)
    >     Normal   Created                 42m                 kubelet            Created container csi-node-driver-registrar
    >     Normal   Started                 42m                 kubelet            Started container csi-node-driver-registrar


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${workclusterconf:?}" \
            describe pod \
                --namespace calico-system \
                csi-node-driver-jq4v9
        '

    >   Name:                 csi-node-driver-jq4v9
    >   Namespace:            calico-system
    >   Priority:             2000001000
    >   Priority Class Name:  system-node-critical
    >   Service Account:      default
    >   Node:                 somerville-jade-20240219-work-md-0-fb50a5e8-tjm67/192.168.3.79
    >   Start Time:           Mon, 19 Feb 2024 03:54:15 +0000
    >   Labels:               app.kubernetes.io/name=csi-node-driver
    >                         controller-revision-hash=746cb5f497
    >                         k8s-app=csi-node-driver
    >                         name=csi-node-driver
    >                         pod-template-generation=1
    >   Annotations:          cni.projectcalico.org/containerID: e785939432c8723b0dcdfd135ddc28b8929dce8d58cead8ceaff4438bc636a87
    >                         cni.projectcalico.org/podIP: 172.20.177.65/32
    >                         cni.projectcalico.org/podIPs: 172.20.177.65/32
    >   Status:               Running
    >   IP:                   172.20.177.65
    >   IPs:
    >     IP:           172.20.177.65
    >   Controlled By:  DaemonSet/csi-node-driver
    >   Containers:
    >     calico-csi:
    >       Container ID:    containerd://267e75db6b8c5221f9a76be1b04c02ba6153bc5092fc37c4ff16865713f62b95
    >       Image:           docker.io/calico/csi:v3.26.0
    >       Image ID:        docker.io/calico/csi@sha256:b078c61d3c8a1395bdb6a441c1318e21dfa4597ed18a5706495c49d515330d64
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       Args:
    >         --nodeid=$(KUBE_NODE_NAME)
    >         --loglevel=$(LOG_LEVEL)
    >       State:          Running
    >         Started:      Mon, 19 Feb 2024 03:58:38 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Environment:
    >         LOG_LEVEL:       warn
    >         KUBE_NODE_NAME:   (v1:spec.nodeName)
    >       Mounts:
    >         /csi from socket-dir (rw)
    >         /etc/calico from etccalico (rw)
    >         /var/lib/kubelet from kubelet-dir (rw)
    >         /var/run from varrun (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-77jrj (ro)
    >     csi-node-driver-registrar:
    >       Container ID:    containerd://12017a399bf3e477c9efb420fffbc03814ec577a4f6654d875ce5315f4d01f30
    >       Image:           docker.io/calico/node-driver-registrar:v3.26.0
    >       Image ID:        docker.io/calico/node-driver-registrar@sha256:af9458100d80e83308d27bde8f65ab1cc647c96094134082f863c38929afc3c0
    >       Port:            <none>
    >       Host Port:       <none>
    >       SeccompProfile:  RuntimeDefault
    >       Args:
    >         --v=5
    >         --csi-address=$(ADDRESS)
    >         --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)
    >       State:          Running
    >         Started:      Mon, 19 Feb 2024 03:59:09 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Environment:
    >         ADDRESS:               /csi/csi.sock
    >         DRIVER_REG_SOCK_PATH:  /var/lib/kubelet/plugins/csi.tigera.io/csi.sock
    >         KUBE_NODE_NAME:         (v1:spec.nodeName)
    >       Mounts:
    >         /csi from socket-dir (rw)
    >         /registration from registration-dir (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-77jrj (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     varrun:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/run
    >       HostPathType:
    >     etccalico:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /etc/calico
    >       HostPathType:
    >     kubelet-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet
    >       HostPathType:  Directory
    >     socket-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet/plugins/csi.tigera.io
    >       HostPathType:  DirectoryOrCreate
    >     registration-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet/plugins_registry
    >       HostPathType:  Directory
    >     kube-api-access-77jrj:
    >       Type:                    Projected (a volume that contains injected data from multiple sources)
    >       TokenExpirationSeconds:  3607
    >       ConfigMapName:           kube-root-ca.crt
    >       ConfigMapOptional:       <nil>
    >       DownwardAPI:             true
    >   QoS Class:                   BestEffort
    >   Node-Selectors:              kubernetes.io/os=linux
    >   Tolerations:                 :NoSchedule op=Exists
    >                                :NoExecute op=Exists
    >                                CriticalAddonsOnly op=Exists
    >                                node.kubernetes.io/disk-pressure:NoSchedule op=Exists
    >                                node.kubernetes.io/memory-pressure:NoSchedule op=Exists
    >                                node.kubernetes.io/not-ready:NoExecute op=Exists
    >                                node.kubernetes.io/pid-pressure:NoSchedule op=Exists
    >                                node.kubernetes.io/unreachable:NoExecute op=Exists
    >                                node.kubernetes.io/unschedulable:NoSchedule op=Exists
    >   Events:
    >     Type     Reason           Age                 From               Message
    >     ----     ------           ----                ----               -------
    >     Normal   Scheduled        51m                 default-scheduler  Successfully assigned calico-system/csi-node-driver-jq4v9 to somerville-jade-20240219-work-md-0-fb50a5e8-tjm67
    >     Warning  NetworkNotReady  50m (x25 over 51m)  kubelet            network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized
    >     Normal   Pulled           46m                 kubelet            Successfully pulled image "docker.io/calico/node-driver-registrar:v3.26.0" in 4.75851331s (29.739786245s including waiting)


    # Skipped ...
    # calico-system csi-node-driver-l9mpb
    # calico-system csi-node-driver-m9hsc
    # calico-system csi-node-driver-rxntx

