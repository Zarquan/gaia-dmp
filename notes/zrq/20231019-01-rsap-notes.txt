#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: []
#

    Target:

        General notes ...

    Result:

        Work in progress ...

# -----------------------------------------------------
# Somerville 2024 plans
# https://github.com/lsst-uk/somerville-operations/wiki/Resource-Planning-Notes-for-2024

    Gaia DMP
    End of 2024 (24 user DR3 system at Somerville) :

        700 cores, 1TB RAM,
        500TB Ceph volume/or Object store (with preferred Ceph SSD for ~10% of that).
        Interest in Ironic.
        Local SSDs for high-use spark tables, up to 15TB of ephemeral SSD per HV.


# -----------------------------------------------------
# 2024/25 plans

    What we order now will not be available until Jan 2025.


    We want to be able to test mock data in 2024.
    Needs space to store it.
    Make the data tables more flexible.
    Load some from S3, some from CephFS and some from DAS.

    Modify gaia-dmp-setup to use separate source addresses for each table.
    Table sources loaded from a JSON/YAML config file.
    Change the config file and reload gaia-dmp-setup to update sources.

    Full DR3 dataset =   5 TiB
    Full DR4 dataset = 500 TiB

    With this much storage, people will be happier to provide S3 Object rather than CephFS.
    Graded levels of storage: slow, medium, fast, ultra-fast.

    Request in Dec 2022, still not available yet.
    Request in Dec 2023, available Jan 2025. <-- this is what we will launch DR4 with.
    Request in Dec 2024, available Jan 2026. <-- this won't be available in time.

    Cambridge
        Live DR4 service
              700 cores
            1 TiB RAM
        Full DR4 dataset, 500 TiB
          500 TiB Object Swift
           50 TiB CephFS Manila
            5 TiB Block  Cinder
           16 TiB DAS Ephemeral

        Dev DR4 service
              700 cores
            1 TiB RAM
        Full DR4 dataset, 500 TiB (needed to re-generate table indexing)
          500 TiB Object Swift
           50 TiB CephFS Manila - Nigel used 30 TiB for indexing DR3
            5 TiB Block  Cinder
           16 TiB DAS Ephemeral

        Total for Cambridge
             1400 cores
            2 TiB RAM
         1000 TiB Object Swift
          100 TiB CephFS Manila
           10 TiB Block  Cinder
           32 TiB DAS Ephemeral

    Somerville
        Live DR4 service
              700 cores
            1 TiB RAM
        Full DR4 dataset, 500 TiB
          500 TiB Object Swift
           50 TiB CephFS Manila
            5 TiB Block  Cinder
           16 TiB DAS Ephemeral

        Dev DR4 service
              700 cores
            1 TiB RAM
        Shared DR4 dataset
            5 TiB Object Swift
            5 TiB CephFS Manila
            5 TiB Block  Cinder
           16 TiB DAS Ephemeral

        Total for Somerville
             1400 cores
            2 TiB RAM
          505 TiB Object Swift
           55 TiB CephFS Manila
           10 TiB Block  Cinder
           32 TiB DAS Ephemeral


    The main site is Cambridge.
    2024/2025 The main site will run the existing DR3 live system, capable
    of supporting for 24 concurrent users in a workshop scenario.

    In addition, work in 2024/2025 will involve creating 500 TiB mock DR4
    data sets and developing analysis code to test that our services scale to match.

    At the end of 2025 we need a live service (700 vcpu, 1 TiB RAM, 500 TiB)
    and a replica for development and testing (700 vcpu, 1 TiB RAM, 500 TiB)
    with enough space to re-partition a new copy of the dataset.

    There is nothing site specific about the Cambridge deployment.
    In theory it could be relocated to RAL with minimal changes.
    (*) If RAL are willing/able to pin hypervisor allocation or guarantee headroom
    to support repeated create/delete cycles.


    The development site will be Somerville.
    Somerville are better placed to experiment with hypervisor hardware options.
    Large fish in a small pond, rather than small fish in a large pond.
    2024/2025 will involve development of the K8s deployment and resource booking system.
    At the end of 2025 we need to have enough resources to be able to support a
    second live service (700 vcpu, 1 TiB RAM, 500 TiB).

    We need a copy of the data at each site that we run a live service.
    We can access the data remotely, but it is way too slow.
    We could wrap it all up in Rucio, but we would still need the same space.
    Rucio might be a good way to manage (and backup) the large 500 TiB blobs,
    but it doesn't magically remove the need for 500 TiB at each site.

    The second dev system at Somerville is not critical to have.
    In theory, we could run without this, splitting the dev system
    at Cambridge into smaller chunks, or using the Somerville main system
    for development. However, cutting the second dev system would only save 700 cores.

    Our deadline for the next Gaia data release is to have a live system capable
    of handling the 500 TiB DR4 dataset in Q4 2025.
    Resources requested in the 24/25 RSAP round will not be operational in time
    to meet this deadline.
    As a result, we are having to plan ahead 2 years, requesting sufficient
    resources in the current 23/24 RSAP round to handle the expected peak load
    which will occur when the next Gaia DR4 is released in December 2025.

        Total for 2023/24 request, operational in 2025
             2800 cores
            4 TiB RAM
         1505 TiB Object Swift
          155 TiB CephFS Manila
           20 TiB Block  Cinder
           54 TiB DAS Ephemeral

    The interactive nature of our service means that our load pattern is extremely variable.
    There will be significant periods when there are only a few people using the system.
    Weekends and university holidays in particular see very low use.
    On the other hand we experience high peaks of interest triggerd by new data releases,
    published papers, conference talks and workshops.

    Due to the static nature of the allocation process, we have to quote figures based on
    the resources needed to handle the expected peaks.
    There will be significant periods when the system is using a fraction of these
    resources.
    Unfortunatley at the moment there is no mechanism for re-allocationg unused resources
    back into the system.
    If the Openstack resource reservation module Blazar becomes available on the IRIS Openstack
    platforms then we can give much better estimates for the peak and background requirements.

    Technical notes


        IRIS accounting portal
        https://accounting.iris.ac.uk/

            I think this view shows our allocation (800) vs our use (600).
            https://accounting.iris.ac.uk/?orgId=1&var-Project=All&var-Site=All&var-VO=gaia&var-VOGroup=All&var-Source=All&var-LineSeries=Just%20Allocation

            If it is just counting allocated VMs, then idle test VMs will show as in-use.
            The fairly constant 600 cores maps to 3 deployments, 2 dev and one live.

        Current deployment at Cambridge

            Pinned to xx hypervisors in order to make resource allocation predictable.

            Without the pinning, resource allocation was unpredictable.
            Early on in the project we encoutered problems deleting and creating sets of resources.
            We could delete a set of 10 virtual machines and then have problems creating a new set because
            some of the resources were allocated to background batch processes in the interval between delete and create.


            Details of hypervisors:
                standard
                high-mem

            Details of the flavors:
                Project has been allocated a set of 'high memory' flavors to make use of high-memory hypervisors.


        Direct attached storage

            Based on the metrics that we have gathered during our development the platform
            disc IO is a limiting factor.




        data managment - Rucio ?






