#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Try to find out more about the limits on concurrent users.
        Based on a clean deployment using 20220613-01-blue-deploy.txt.

    Result:

        Work in progress ...

        TODO move from quick to complex test sets
        TODO move from 4 to 8 concurrent users


# -----------------------------------------------------
# Create our benchmark script.
#[root@ansibler]

    cat > /tmp/run-benchmark.py << 'EOF'
#!/bin/python3
import sys
from aglais_benchmark import AglaisBenchmarker

try:

    opts = [opt for opt in sys.argv[1:] if opt.startswith("-")]
    args = [arg for arg in sys.argv[1:] if not arg.startswith("-")]

    endpoint = args[0]
    testconfig = args[1]
    userlist = args[2]
    usercount = int(args[3])
    delaystart = int(args[4])
    delaynotebook = int(args[5])

except IndexError:

    raise SystemExit(f"Usage: {sys.argv[0]} <Zepelin endpoint> <test config> <list of users> <number of users>")

print(
"""
{{
\"config\": {{
    \"endpoint\":   \"{}\",
    \"testconfig\": \"{}\",
    \"userlist\":   \"{}\",
    \"usercount\":  \"{}\",
    \"delaystart\":  \"{}\",
    \"delaynotebook\":  \"{}\"
    }},
\"output\": {{
""".format(
        endpoint,
        testconfig,
        userlist,
        usercount,
        delaystart,
        delaynotebook
        )
    )

print(
    "---start---"
    )
AglaisBenchmarker(
    testconfig,
    userlist,
    "/tmp/",
    endpoint
    ).run(
        concurrent=True,
        users=usercount,
        delay_start=delaystart,
        delay_notebook=delaynotebook
        )
print(
    "---end---"
    )
print(
"""
    }
}
"""
    )
EOF

    chmod 'a+x' /tmp/run-benchmark.py


# -----------------------------------------------------
# Create our filter function.
# https://github.com/wfau/aglais/issues/602
#[root@ansibler]

    filter-results()
        {
        local testname=${1:?'testname required'}
        sed "
            /^--*start--*/,/^--*end--*/ {
                /^--*start/,/^--* Test Result/ {
                    /Test Result/ ! {
                        d
                        }
                    /Test Result/ {
                        s/^.*Test Result: \[\(.*\)\].*$/'testcode': '\1',/
                        a \"threads\":
                        }
                    }
                s/\"/'/g
                s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
                s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
                s/:[[:space:]]*\([,}]\),/: ''\1/g
                s/'/\"/g
                }
            /^--*end--*/ {
                d
                }
            " \
            "/tmp/results/${testname:?}.txt" \
        | tee "/tmp/results/${testname:?}.json" \
        | jq  '
              .output.threads[] | keys as $x | [ $x[] as $y | {name: $y, value: .[$y].result, time: .[$y].time.elapsed , start: .[$y].time.start, finish: .[$y].time.finish } ]
              '
        }


# -----------------------------------------------------
# Create our test-loop function.
#[root@ansibler]

    test-loop()
        {
        local loopcount=${1:?'loopcount required'}
        local usercount=${2:?'usercount required'}

        rm -f /tmp/results/*

        echo "["

        local comma=''
        for i in $(seq 0 $((loopcount - 1)))
        do
            echo "${comma}" ; comma=','

            testname="multi-user-$(printf "%02d" ${usercount})-$(printf "%02d" ${i})"

cat << EOF
    {
    "iteration": ${i},
    "testname": "${testname}",
    "threads":
EOF

            /tmp/run-benchmark.py \
                "${endpoint:?}" \
                "${testconfig:?}" \
                "${testusers:?}" \
                "${usercount:?}" \
                "${delaystart:?}" \
                "${delaynotebook:?}" \
            > "/tmp/results/${testname:?}.txt"

            filter-results "${testname:?}"

            echo "}"

        done

        echo "]"

        }


# -----------------------------------------------------
# Test with 1 user doing 1 loop.
#[root@ansibler]

    test-loop 1 1 \
    | tee /tmp/test-loop.json

    jq '.' /tmp/test-loop.json

--START--
[
  {
    "iteration": 0,
    "testname": "multi-user-01-00",
    "threads": [
      {
        "name": "GaiaDMPSetup",
        "value": "PASS",
        "time": 36.04,
        "start": "2022-06-14T12:02:17.073449",
        "finish": "2022-06-14T12:02:53.115954"
      },
      {
        "name": "Library_Validation.json",
        "value": "PASS",
        "time": 10.14,
        "start": "2022-06-14T12:04:19.243331",
        "finish": "2022-06-14T12:04:29.385640"
      },
      {
        "name": "Mean_proper_motions_over_the_sky",
        "value": "PASS",
        "time": 65.06,
        "start": "2022-06-14T12:02:54.117116",
        "finish": "2022-06-14T12:03:59.174176"
      },
      {
        "name": "Source_counts_over_the_sky.json",
        "value": "PASS",
        "time": 18.07,
        "start": "2022-06-14T12:04:00.175377",
        "finish": "2022-06-14T12:04:18.241466"
      }
    ]
  }
]
--END--


    grep 'Result:' /tmp/results/*.txt

--START--
------------ Test Result: [PASS] ------------
--END--


# -----------------------------------------------------
# Test with 2 users doing 1 loop (wrong).
#[root@ansibler]

    test-loop 2 1 \
    | tee /tmp/test-loop.json

    jq '.' /tmp/test-loop.json


--START--
[

    {
    "iteration": 0,
    "testname": "multi-user-01-00",
    "threads":
[
  {
    "name": "GaiaDMPSetup",
    "value": "PASS",
    "time": 36.63,
    "start": "2022-06-14T12:06:46.929067",
    "finish": "2022-06-14T12:07:23.560850"
  },
  {
    "name": "Library_Validation.json",
    "value": "PASS",
    "time": 9.68,
    "start": "2022-06-14T12:08:53.924361",
    "finish": "2022-06-14T12:09:03.607976"
  },
  {
    "name": "Mean_proper_motions_over_the_sky",
    "value": "PASS",
    "time": 65.66,
    "start": "2022-06-14T12:07:24.561381",
    "finish": "2022-06-14T12:08:30.217678"
  },
  {
    "name": "Source_counts_over_the_sky.json",
    "value": "PASS",
    "time": 21.70,
    "start": "2022-06-14T12:08:31.218326",
    "finish": "2022-06-14T12:08:52.923187"
  }
]
}
,
    {
    "iteration": 1,
    "testname": "multi-user-01-01",
    "threads":
parse error: Invalid numeric literal at line 15, column 638
}
]
parse error: Unmatched '}' at line 43, column 1
--END--


    grep 'Result:' /tmp/results/*.txt

--START--
/tmp/results/multi-user-01-00.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-01-01.txt:------------ Test Result: [FAIL] ------------
--END--

    #
    # Failing the second test is back.
    # Go figure.
    #


# -----------------------------------------------------
# Test with 3 users doing 1 loop (wrong).
#[root@ansibler]

    test-loop 3 1 \
    | tee /tmp/test-loop.json

    jq '.' /tmp/test-loop.json

--START--
[

    {
    "iteration": 0,
    "testname": "multi-user-01-00",
    "threads":
[
  {
    "name": "GaiaDMPSetup",
    "value": "PASS",
    "time": 34.87,
    "start": "2022-06-14T12:16:20.059583",
    "finish": "2022-06-14T12:16:54.933593"
  },
  {
    "name": "Library_Validation.json",
    "value": "PASS",
    "time": 10.20,
    "start": "2022-06-14T12:18:31.505472",
    "finish": "2022-06-14T12:18:41.706929"
  },
  {
    "name": "Mean_proper_motions_over_the_sky",
    "value": "PASS",
    "time": 72.21,
    "start": "2022-06-14T12:16:55.934349",
    "finish": "2022-06-14T12:18:08.149132"
  },
  {
    "name": "Source_counts_over_the_sky.json",
    "value": "PASS",
    "time": 21.35,
    "start": "2022-06-14T12:18:09.150417",
    "finish": "2022-06-14T12:18:30.503974"
  }
]
}
,
    {
    "iteration": 1,
    "testname": "multi-user-01-01",
    "threads":
parse error: Invalid numeric literal at line 15, column 7462
}
,
    {
    "iteration": 2,
    "testname": "multi-user-01-02",
    "threads":
parse error: Invalid numeric literal at line 15, column 638
}
]
parse error: Unmatched '}' at line 43, column 1
--END--



    grep 'Result:' /tmp/results/*.txt

--START--
/tmp/results/multi-user-01-00.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-01-01.txt:------------ Test Result: [ERROR] ------------
/tmp/results/multi-user-01-02.txt:------------ Test Result: [FAIL] ------------
--END--

    #
    # What is the difference between an ERROR and a FAIL ?
    #

    vi /tmp/results/multi-user-01-01.txt


--START--
....
Test started [Multi User]
Test completed! (83.97 seconds)
------------ Test Result: [ERROR] ------------
    [
        {
        'GaiaDMPSetup': {
            'result': 'ERROR',
            'outputs': {
                'valid': True
                },
            'time': {
                'result': 'FAST',
                'elapsed': '33.45',
                'expected': '45.00',
                'percent': '-25.67',
                'start': '2022-06-14T12:18:44.237768',
                'finish': '2022-06-14T12:19:17.685659'
                },
            'logs': '
                org.apache.zeppelin.interpreter.InterpreterException: java.io.IOException:
                    Interpreter Process creation is time out in 30 seconds
                    You can increase timeout threshold via setting zeppelin.interpreter.connect.timeout of this interpreter.
                    Interpreter download command:
                        /etc/alternatives/jre/bin/java
                            -Dfile.encoding=UTF-8
                            -Dlog4j.configuration=file:///home/fedora/zeppelin/conf/log4j.properties
                            -Dlog4j.configurationFile=file:///home/fedora/zeppelin/conf/log4j2.properties
                            -Dzeppelin.log.file=/home/fedora/zeppelin/logs/zeppelin-interpreter-spark-Carclop-Carclop-fedora-iris-gaia-blue-20220613-zeppelin.log
                            -cp :/home/fedora/zeppelin/interpreter/spark/*
                                :/home/fedora/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar
                                :/home/fedora/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar
                                :/opt/hadoop/etc/hadoop org.apache.zeppelin.interpreter.remote.RemoteInterpreterDownloader
                            10.10.2.210 35643
                            spark
                            /home/fedora/zeppelin/local-repo/spark

                    [INFO] Interpreter launch command:
                        /opt/spark/bin/spark-submit
                            --class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer
                            --driver-class-path
                                :/home/fedora/zeppelin/local-repo/spark/*
                                :/home/fedora/zeppelin/interpreter/spark/*
                                :/home/fedora/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar
                                :/home/fedora/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar
                                :/opt/hadoop/etc/hadoop
                            --driver-java-options
                                -Dfile.encoding=UTF-8
                                -Dlog4j.configuration=file:///home/fedora/zeppelin/conf/log4j.properties
                                -Dlog4j.configurationFile=file:///home/fedora/zeppelin/conf/log4j2.properties
                                -Dzeppelin.log.file=/home/fedora/zeppelin/logs/zeppelin-interpreter-spark-Carclop-Carclop-fedora-iris-gaia-blue-20220613-zeppelin.log
                            --proxy-user Carclop
                            --conf spark.yarn.dist.archives=/opt/spark/R/lib/sparkr.zip#sparkr
                            --conf spark.submit.deployMode=client
                            --conf spark.webui.yarn.useProxy=false
                            --conf spark.yarn.isPython=true
                            --conf spark.app.name=spark-Carclop
                            --conf spark.master=yarn
                            /home/fedora/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar
                                10.10.2.210 35643
                                spark-Carclop :

                    SLF4J: Class path contains multiple SLF4J bindings.
                    SLF4J: Found binding in [jar:file:/home/fedora/zeppelin-0.10.0-bin-all/interpreter/spark/spark-interpreter-0.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
                    SLF4J: Found binding in [jar:file:/opt/spark-3.1.2-bin-hadoop3.2/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
                    SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
                    SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

                        at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:129)
                        at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:271)
                        at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:440)
                        at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:71)
                        at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
                        at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)
                        at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:182)
                        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
                        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
                        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
                        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
                        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
                        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
                        at java.lang.Thread.run(Thread.java:748)

                    Caused by: java.io.IOException: Interpreter Process creation is time out in 30 seconds
                        You can increase timeout threshold via setting zeppelin.interpreter.connect.timeout of this interpreter.
                        Interpreter download command: ....
                        [INFO] Interpreter launch command: ....

                    SLF4J: Class path contains multiple SLF4J bindings.
                    SLF4J: Found binding in [jar:file:/home/fedora/zeppelin-0.10.0-bin-all/interpreter/spark/spark-interpreter-0.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
                    SLF4J: Found binding in [jar:file:/opt/spark-3.1.2-bin-hadoop3.2/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
                    SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
                    SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

                        at org.apache.zeppelin.interpreter.remote.ExecRemoteInterpreterProcess.start(ExecRemoteInterpreterProcess.java:93)
                        at org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:68)
                        at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:104)
                        at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:154)
                        at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:126)
                        ... 13 more
                '
            },
        'Mean_proper_motions_over_the_sky': {
            'result': 'ERROR',
            'outputs': {
                'valid': True
                },
            'time': {
                'result': 'FAST',
                'elapsed': '29.76',
                'expected': '55.00',
                'percent': '-45.89',
                'start': '2022-06-14T12:19:18.687135',
                'finish': '2022-06-14T12:19:48.447845'
                },
            'logs': '
                Fail to execute line 13: df = spark.sql(query).cache()
                ....
                pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 121;
                \'Aggregate [\'hpx_id], [\'floor((\'source_id / 140737488355328)) AS hpx_id#0, count(1) AS n#1L, \'AVG(\'pmra) AS avg_pmra#2, \'AVG(\'pmdec) AS avg_pmdec#3]
                +- \'UnresolvedRelation [gaia_source], [], false
                '
            },
        'Source_counts_over_the_sky.json': {
            'result': 'ERROR',
            'outputs': {
                'valid': True
                },
            'time': {
                'result': 'FAST',
                'elapsed': '5.74',
                'expected': '22.00',
                'percent': '-73.90',
                'start': '2022-06-14T12:19:49.448736',
                'finish': '2022-06-14T12:19:55.190901'
                },
            'logs': '
                Fail to execute line 21: df = spark.sql("SELECT FLOOR(source_id / %d"%(divisor) + ") AS hpx_id, COUNT(*) AS n FROM gaia_source GROUP BY hpx_id")
                ....
                pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 72;
                \'Aggregate [\'hpx_id], [\'FLOOR((\'source_id / 140737488355328)) AS hpx_id#5, count(1) AS n#6L]
                +- \'UnresolvedRelation [gaia_source], [], false
                '
            },
        'Library_Validation.json': {
            'result': 'PASS',
            'outputs': {
                'valid': True
                },
            'time': {
                'result': 'FAST',
                'elapsed': '10.99',
                'expected': '60.00',
                'percent': '-81.68',
                'start': '2022-06-14T12:19:56.191334',
                'finish': '2022-06-14T12:20:07.180597'
                },
            'logs': ''
            }
        }
    ]
....
--END--

    #
    # I'm guessing that GaiaDMPSetup ran first, got half way through creating the schema and failed.
    # This left the top level schema partially defined, so the test in the subsequent notebooks skipped the schema creation.
    #


    vi /tmp/results/multi-user-01-02.txt


--START--
....
Test started [Multi User]
Test completed! (56.67 seconds)
------------ Test Result: [FAIL] ------------
    [
        {
        'GaiaDMPSetup': {
            'result': 'FAIL',
            'outputs': {
                'valid': True
                },
            'time': {
                'result': 'FAST',
                'elapsed': '4.61',
                'expected': '45.00',
                'percent': '-89.76',
                'start': '2022-06-14T12:20:08.716961',
                'finish': '2022-06-14T12:20:13.325401'
                },
            'logs': ''
            },
        'Mean_proper_motions_over_the_sky': {
            'result': 'ERROR',
            'outputs': {
                'valid': True
                },
            'time': {
                'result': 'FAST',
                'elapsed': '30.62',
                'expected': '55.00',
                'percent': '-44.33',
                'start': '2022-06-14T12:20:14.326399',
                'finish': '2022-06-14T12:20:44.947081'
                },
            'logs': '
                Fail to execute line 13: df = spark.sql(query).cache()
                ....
                pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 121;
                \'Aggregate [\'hpx_id], [\'floor((\'source_id / 140737488355328)) AS hpx_id#0, count(1) AS n#1L, \'AVG(\'pmra) AS avg_pmra#2, \'AVG(\'pmdec) AS avg_pmdec#3]
                +- \'UnresolvedRelation [gaia_source], [], false
                '
            },
        'Source_counts_over_the_sky.json': {
            'result': 'ERROR',
            'outputs': {
                'valid': True
                },
            'time': {
                'result': 'FAST',
                'elapsed': '5.66',
                'expected': '22.00',
                'percent': '-74.25',
                'start': '2022-06-14T12:20:45.948445',
                'finish': '2022-06-14T12:20:51.613161'
                },
            'logs': '
                Fail to execute line 21: df = spark.sql("SELECT FLOOR(source_id / %d"%(divisor) + ") AS hpx_id, COUNT(*) AS n FROM gaia_source GROUP BY hpx_id")
                ....
                pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 72;
                \'Aggregate [\'hpx_id], [\'FLOOR((\'source_id / 140737488355328)) AS hpx_id#5, count(1) AS n#6L]
                +- \'UnresolvedRelation [gaia_source], [], false
                '
            },
        'Library_Validation.json': {
            'result': 'PASS',
            'outputs': {
                'valid': True
                },
            'time': {
                'result': 'FAST',
                'elapsed': '11.76',
                'expected': '60.00',
                'percent': '-80.41',
                'start': '2022-06-14T12:20:52.614339',
                'finish': '2022-06-14T12:21:04.370259'
                },
            'logs': ''
            }
        }
    ]
....
--END--

    #
    # In this one, GaiaDMPSetup failed (no details).
    # This left the top level schema partially defined, so the test in the subsequent notebooks skipped the schema creation.
    #


# -----------------------------------------------------
# Test again with 3 users doing 1 loop (wrong).
#[root@ansibler]

    test-loop 3 1 \
    | tee /tmp/test-loop.json

    jq '.' /tmp/test-loop.json

    grep 'Result:' /tmp/results/*.txt

--START--
/tmp/results/multi-user-01-00.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-01-01.txt:------------ Test Result: [ERROR] ------------
/tmp/results/multi-user-01-02.txt:------------ Test Result: [ERROR] ------------
--END--

    #
    # Slightly different results.
    # One pass and two errors ..
    #


# -----------------------------------------------------
# Test again with 3 users doing 1 loop (wrong).
#[root@ansibler]

    test-loop 3 1 \
    | tee /tmp/test-loop.json

    jq '.' /tmp/test-loop.json

    grep 'Result:' /tmp/results/*.txt

--START--
/tmp/results/multi-user-01-00.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-01-01.txt:------------ Test Result: [ERROR] ------------
/tmp/results/multi-user-01-02.txt:------------ Test Result: [FAIL] ------------
--END--

    #
    # Slightly different results.
    # A pass an error and a fail ..
    #

    #
    # A guess ... this started after I had killed a test run using Ctrl-C.
    # Perhaps it has left a mis-configured interpreter active ?
    # See what we can find out via the REST API.
    #


# -----------------------------------------------------
# Get the user login details.
#[root@ansibler]

    less /tmp/results/multi-user-01-01.txt

--START--
{
"config": {
    "endpoint":   "http://zeppelin:8080",
    "testconfig": "/deployments/zeppelin/test/config/quick.json",
    "userlist":   "/tmp/testusers-02.json",
    "usercount":  "1",
    "delaystart":  "1",
    "delaynotebook":  "1"
    },
....
....
--END--


    jq '.users[].shirouser | {name, pass}' /tmp/testusers-02.json

--START--
{
  "name": "Hamar",
  "pass": "bu2hohmohthiesuNg1deiy5IeshaeD"
}
{
  "name": "Carclop",
  "pass": "vae4thae4kae9phohb8eefohBaf4Ee"
}
{
  "name": "Halda",
  "pass": "ohcho9oechai1caiQuieNgeey2ush7"
}
{
  "name": "Jaden",
  "pass": "oMeivie5uingo6kai1eisaiK2ciawo"
}
{
  "name": "Mavaca",
  "pass": "xa9Dohhei8aihahkauMaxah8Ahjiey"
}
{
  "name": "Franilley",
  "pass": "eiz4ik3meen4queeyei4Eedaec1ahv"
}
{
  "name": "Masonania",
  "pass": "booGee2ohthae4Chiingu8ziu5eech"
}
{
  "name": "Webbbron",
  "pass": "ahchohk1neiTah1eiyaiwaekohwaiN"
}
{
  "name": "Granwaler",
  "pass": "Su1ie7akaethae6eic0ien5wiChaeC"
}
--END--


# -----------------------------------------------------
# Select the username and password for the second test user.
#[root@ansibler]

    # Note - JSON array is zero indexed, but AglaisBenchmarker skips the first entry.
    # So use [2] to get the second test user.

    jq '.users[2].shirouser | {name, pass}' /tmp/testusers-02.json

--START--
{
  "name": "Halda",
  "pass": "ohcho9oechai1caiQuieNgeey2ush7"
}
--END--


    testername=$(
        jq -r '.users[2].shirouser.name' /tmp/testusers-02.json
        )
    testerpass=$(
        jq -r '.users[2].shirouser.pass' /tmp/testusers-02.json
        )


# -----------------------------------------------------
# Login to Zeppelin as the test user.
#[root@ansibler]

    source '/deployments/zeppelin/bin/zeppelin-rest-tools.sh'

    zeppelinurl='http://zeppelin:8080'
    zepcookies=/tmp/${testername:?}.cookies

    zeplogin "${testername:?}" "${testerpass:?}" \
    | jq '.'

--START--
{
  "status": "OK",
  "message": "",
  "body": {
    "principal": "Halda",
    "ticket": "5d168ff0-a9cd-48e9-9b04-064219d8f4a9",
    "roles": "[\"user\"]"
  }
}
--END--


# -----------------------------------------------------
# List the registered interpreters.
#[root@ansibler]

    curl \
        --silent \
        --cookie "${zepcookies:?}" \
        "${zeppelinurl:?}/api/interpreter" \
    | tee /tmp/interpreter-list.json \
    | jq '.'

--START--
{
  "status": "OK",
  "message": "",
  "body": {
    "python": {
      "id": "python",
      "name": "python",
      "group": "python",
      "properties": {
        ....
        },
      "status": "READY",
      "interpreterGroup": [
        ....
        ],
      "dependencies": [],
      "option": {
        "remote": true,
        "port": -1,
        "isExistingProcess": false,
        "setPermission": false,
        "isUserImpersonate": false
      }
    },
    "spark": {
      "id": "spark",
      "name": "spark",
      "group": "spark",
      "properties": {
        ....
        },
      "status": "READY",
      "interpreterGroup": [
        ....
        ],
      "dependencies": [],
      "option": {
        "remote": true,
        "port": -1,
        "isExistingProcess": false,
        "setPermission": false,
        "isUserImpersonate": false
      }
    },
    "sh": {
      "id": "sh",
      "name": "sh",
      "group": "sh",
      "properties": {
        ....
        },
      "status": "READY",
      "interpreterGroup": [
        ....
        ],
      "dependencies": [],
      "option": {
        "remote": true,
        "port": -1,
        "isExistingProcess": false,
        "setPermission": false,
        "isUserImpersonate": false
      }
    },
    "md": {
      "id": "md",
      "name": "md",
      "group": "md",
      "properties": {
        ....
        },
      "status": "READY",
      "interpreterGroup": [
        ....
        ],
      "dependencies": [],
      "option": {
        "remote": true,
        "port": -1,
        "isExistingProcess": false,
        "setPermission": false,
        "isUserImpersonate": false
      }
    }
  }
}
--END--

    #
    # These interpreters all have 'isUserImpersonate' set to 'false'.
    # Should they ?
    #

    #
    # According to the Zeppelin documentation, restarting an interpreter is done on a per notebook basis.
    # https://zeppelin.apache.org/docs/0.10.0/usage/rest_api/interpreter.html#restart-an-interpreter

    PUT
    ${zeppelinurl:?}/api/interpreter/setting/restart/[interpreter ID]

        {
        "noteId": "2AVQJVC8N"
        }

    #
    # .. but if the tests deleted the notebooks after each test run,
    # then we won't have an interpreter attached to a notebook.
    #
    # Is the interpreter per user account or per notebook ?
    # Interpreter settings page says:
    # http://zeppelin:8080/#/interpreter
    #
    # spark:
    #   The interpreter will be instantiated [per user] in [isolated] process.
    #
    # Checked the UI, we do have one notebook in /tmp.
    # name  : 4XM670LBGE.json
    # ident : 2H6GREYK3
    #

    #
    # Run the test to confirm errors
    # Restart the interpreter
    # Run the test to confirm errors
    #

# -----------------------------------------------------
# Test baseline with 3 users doing 1 loop (wrong).
#[root@ansibler]

    test-loop 3 1 \
    | tee /tmp/test-loop.json

    jq '.' /tmp/test-loop.json

    grep 'Result:' /tmp/results/*.txt

--START--
/tmp/results/multi-user-01-00.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-01-01.txt:------------ Test Result: [ERROR] ------------
/tmp/results/multi-user-01-02.txt:------------ Test Result: [ERROR] ------------
--END--


# -----------------------------------------------------
# Try restarting our pyspark interpreter.
# https://zeppelin.apache.org/docs/0.10.0/usage/rest_api/interpreter.html#restart-an-interpreter
#[root@ansibler]

    interpreter=spark

    curl \
        --silent \
        --cookie "${zepcookies:?}" \
        --header "Content-Type: application/json" \
        --request PUT \
        "${zeppelinurl:?}/api/interpreter/setting/restart/${interpreter}"


--START--
{"status":"OK","message":"","body":{"id":"spark","name":"spark","group":"spark","properties":{"SPARK_HOME":{"name":"SPARK_HOME","value":"/opt/spark","type":"string","description":"Location of spark distribution"},"spark.master":{"name":"spark.master","value":"yarn","type":"string","description":"Spark master uri. local | yarn-client | yarn-cluster | spark master address of standalone mode, ex) spark://master_host:7077"},"spark.submit.deployMode":{"name":"spark.submit.deployMode","value":"client","type":"string","description":"The deploy mode of Spark driver program, either \"client\" or \"cluster\", Which means to launch driver program locally (\"client\") or remotely (\"cluster\") on one of the nodes inside the cluster."},"spark.app.name":{"name":"spark.app.name","value":"","type":"string","description":"The name of spark application."},"spark.driver.cores":{"name":"spark.driver.cores","value":"","type":"number","description":"Number of cores to use for the driver process, only in cluster mode."},"spark.driver.memory":{"name":"spark.driver.memory","value":"","type":"string","description":"Amount of memory to use for the driver process, i.e. where SparkContext is initialized, in the same format as JVM memory strings with a size unit suffix (\"k\", \"m\", \"g\" or \"t\") (e.g. 512m, 2g)."},"spark.executor.cores":{"name":"spark.executor.cores","value":"","type":"number","description":"The number of cores to use on each executor"},"spark.executor.memory":{"name":"spark.executor.memory","value":"","type":"string","description":"Executor memory per worker instance. ex) 512m, 32g"},"spark.executor.instances":{"name":"spark.executor.instances","value":"","type":"number","description":"The number of executors for static allocation."},"spark.files":{"name":"spark.files","value":"","type":"string","description":"Comma-separated list of files to be placed in the working directory of each executor. Globs are allowed."},"spark.jars":{"name":"spark.jars","value":"","type":"string","description":"Comma-separated list of jars to include on the driver and executor classpaths. Globs are allowed."},"spark.jars.packages":{"name":"spark.jars.packages","value":"","type":"string","description":"Comma-separated list of Maven coordinates of jars to include on the driver and executor classpaths. The coordinates should be groupId:artifactId:version. If spark.jars.ivySettings is given artifacts will be resolved according to the configuration in the file, otherwise artifacts will be searched for in the local maven repo, then maven central and finally any additional remote repositories given by the command-line option --repositories."},"zeppelin.spark.useHiveContext":{"name":"zeppelin.spark.useHiveContext","value":true,"type":"checkbox","description":"Use HiveContext instead of SQLContext if it is true. Enable hive for SparkSession."},"zeppelin.spark.run.asLoginUser":{"name":"zeppelin.spark.run.asLoginUser","value":false,"type":"checkbox","description":"Whether run spark job as the zeppelin login user, it is only applied when running spark job in hadoop yarn cluster and shiro is enabled"},"zeppelin.spark.printREPLOutput":{"name":"zeppelin.spark.printREPLOutput","value":true,"type":"checkbox","description":"Print REPL output"},"zeppelin.spark.maxResult":{"name":"zeppelin.spark.maxResult","value":"1000","type":"number","description":"Max number of result to display."},"zeppelin.spark.enableSupportedVersionCheck":{"name":"zeppelin.spark.enableSupportedVersionCheck","value":true,"type":"checkbox","description":"Whether checking supported spark version. Developer only setting, not for production use"},"zeppelin.spark.uiWebUrl":{"name":"zeppelin.spark.uiWebUrl","value":"","type":"string","description":"Override Spark UI default URL. In Kubernetes mode, value can be Jinja template string with 3 template variables \u0027PORT\u0027, \u0027SERVICE_NAME\u0027 and \u0027SERVICE_DOMAIN\u0027. (ex: http://{{PORT}}-{{SERVICE_NAME}}.{{SERVICE_DOMAIN}})"},"zeppelin.spark.ui.hidden":{"name":"zeppelin.spark.ui.hidden","value":false,"type":"checkbox","description":"Whether hide spark ui in zeppelin ui"},"spark.webui.yarn.useProxy":{"name":"spark.webui.yarn.useProxy","value":false,"type":"checkbox","description":"whether use yarn proxy url as spark weburl, e.g. http://localhost:8088/proxy/application_1583396598068_0004"},"zeppelin.spark.scala.color":{"name":"zeppelin.spark.scala.color","value":true,"type":"checkbox","description":"Whether enable color output of spark scala interpreter"},"zeppelin.spark.deprecatedMsg.show":{"name":"zeppelin.spark.deprecatedMsg.show","value":true,"type":"checkbox","description":"Whether show the spark deprecated message, spark 2.2 and before are deprecated. Zeppelin will display warning message by default"},"zeppelin.spark.concurrentSQL":{"name":"zeppelin.spark.concurrentSQL","value":true,"type":"checkbox","description":"Execute multiple SQL concurrently if set true."},"zeppelin.spark.concurrentSQL.max":{"name":"zeppelin.spark.concurrentSQL.max","value":"10","type":"number","description":"Max number of SQL concurrently executed"},"zeppelin.spark.sql.stacktrace":{"name":"zeppelin.spark.sql.stacktrace","value":true,"type":"checkbox","description":"Show full exception stacktrace for SQL queries if set to true."},"zeppelin.spark.sql.interpolation":{"name":"zeppelin.spark.sql.interpolation","value":false,"type":"checkbox","description":"Enable ZeppelinContext variable interpolation into spark sql"},"PYSPARK_PYTHON":{"name":"PYSPARK_PYTHON","value":"python","type":"string","description":"Python binary executable to use for PySpark in both driver and workers (default is python2.7 if available, otherwise python). Property `spark.pyspark.python` take precedence if it is set"},"PYSPARK_DRIVER_PYTHON":{"name":"PYSPARK_DRIVER_PYTHON","value":"python","type":"string","description":"Python binary executable to use for PySpark in driver only (default is `PYSPARK_PYTHON`). Property `spark.pyspark.driver.python` take precedence if it is set"},"zeppelin.pyspark.useIPython":{"name":"zeppelin.pyspark.useIPython","value":false,"type":"checkbox","description":"Whether use IPython when it is available"},"zeppelin.R.knitr":{"name":"zeppelin.R.knitr","value":true,"type":"checkbox","description":"Whether use knitr or not"},"zeppelin.R.cmd":{"name":"zeppelin.R.cmd","value":"R","type":"string","description":"R binary executable path"},"zeppelin.R.image.width":{"name":"zeppelin.R.image.width","value":"100%","type":"number","description":"Image width of R plotting"},"zeppelin.R.render.options":{"name":"zeppelin.R.render.options","value":"out.format \u003d \u0027html\u0027, comment \u003d NA, echo \u003d FALSE, results \u003d \u0027asis\u0027, message \u003d F, warning \u003d F, fig.retina \u003d 2","type":"textarea","description":""},"zeppelin.R.shiny.portRange":{"name":"zeppelin.R.shiny.portRange","value":":","type":"string","description":"Shiny app would launch a web app at some port, this property is to specify the portRange via format \u0027\u003cstart\u003e:\u003cend\u003e\u0027, e.g. \u00275000:5001\u0027. By default it is \u0027:\u0027 which means any port"},"zeppelin.kotlin.shortenTypes":{"name":"zeppelin.kotlin.shortenTypes","value":true,"type":"checkbox","description":"Show short types instead of full, e.g. List\u003cString\u003e or kotlin.collections.List\u003ckotlin.String\u003e"}},"status":"READY","interpreterGroup":[{"name":"spark","class":"org.apache.zeppelin.spark.SparkInterpreter","defaultInterpreter":true,"editor":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},{"name":"sql","class":"org.apache.zeppelin.spark.SparkSqlInterpreter","defaultInterpreter":false,"editor":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},{"name":"pyspark","class":"org.apache.zeppelin.spark.PySparkInterpreter","defaultInterpreter":false,"editor":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},{"name":"ipyspark","class":"org.apache.zeppelin.spark.IPySparkInterpreter","defaultInterpreter":false,"editor":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},{"name":"r","class":"org.apache.zeppelin.spark.SparkRInterpreter","defaultInterpreter":false,"editor":{"language":"r","editOnDblClick":false,"completionSupport":false,"completionKey":"TAB"}},{"name":"ir","class":"org.apache.zeppelin.spark.SparkIRInterpreter","defaultInterpreter":false,"editor":{"language":"r","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},{"name":"shiny","class":"org.apache.zeppelin.spark.SparkShinyInterpreter","defaultInterpreter":false,"editor":{"language":"r","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"}},{"name":"kotlin","class":"org.apache.zeppelin.spark.KotlinSparkInterpreter","defaultInterpreter":false,"editor":{"language":"kotlin","editOnDblClick":false,"completionKey":"TAB","completionSupport":false}}],"dependencies":[],"option":{"remote":true,"port":-1,"perNote":"shared","perUser":"isolated","isExistingProcess":false,"setPermission":false,"owners":[],"isUserImpersonate":true}}}[root@ansibler /]#
--END--

    #
    # Re-starting the spark interpreter by user 'Halda', closing interpreters for 'spark-Halda' and 'spark-anonymous', and but logs show it is also closing interpreter for 'spark-Mavaca' too.
    # ... but what about the third user 'Carclop' ?
    # actually, Mavaca is the 5th user (not used in these tests), so perhaps that was a clean up.
    #
    # At this point I was thinking the test was running 3 users doing 1 loop, which was wrong.
    # So the fixes I was trying wouldn't have worked anyway.
    #


--START--
 ....
 INFO [2022-06-15 02:25:22,413] ({qtp686466458-214460} InterpreterRestApi.java[restartSetting]:199) - Restart interpreterSetting spark, msg=
 INFO [2022-06-15 02:25:22,414] ({qtp686466458-214460} InterpreterSetting.java[close]:535) - Close InterpreterSetting: spark
 INFO [2022-06-15 02:25:22,418] ({spark-close} ManagedInterpreterGroup.java[close]:91) - Close InterpreterGroup: spark-anonymous
 INFO [2022-06-15 02:25:22,418] ({spark-close} ManagedInterpreterGroup.java[close]:91) - Close InterpreterGroup: spark-Halda
 INFO [2022-06-15 02:25:22,418] ({spark-close} ManagedInterpreterGroup.java[close]:102) - Close Session: shared_session for interpreter setting: spark
 INFO [2022-06-15 02:25:22,418] ({spark-close} ManagedInterpreterGroup.java[close]:91) - Close InterpreterGroup: spark-Mavaca
 ....
 INFO [2022-06-15 02:25:22,419] ({RemoteInterpreter-close} SchedulerFactory.java[removeScheduler]:110) - Remove scheduler: RemoteInterpreter-spark-anonymous-shared_session
 INFO [2022-06-15 02:25:22,419] ({RemoteInterpreter-close} SchedulerFactory.java[removeScheduler]:110) - Remove scheduler: RemoteInterpreter-spark-Halda-shared_session
 ....
 INFO [2022-06-15 02:25:22,420] ({ConfInterpreter-close} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_1769331079
 ....
 ....
 INFO [2022-06-15 02:25:22,426] ({RemoteInterpreter-close} SchedulerFactory.java[removeScheduler]:110) - Remove scheduler: RemoteInterpreter-spark-Halda-shared_session
 INFO [2022-06-15 02:25:22,426] ({RemoteInterpreter-close} SchedulerFactory.java[removeScheduler]:110) - Remove scheduler: RemoteInterpreter-spark-Mavaca-shared_session
 INFO [2022-06-15 02:25:22,427] ({RemoteInterpreter-close} SchedulerFactory.java[removeScheduler]:110) - Remove scheduler: RemoteInterpreter-spark-anonymous-shared_session
 ....
 INFO [2022-06-15 02:25:22,427] ({spark-close} ManagedInterpreterGroup.java[close]:106) - Remove this InterpreterGroup: spark-Halda as all the sessions are closed
 ....
 INFO [2022-06-15 02:25:22,427] ({spark-close} ManagedInterpreterGroup.java[close]:106) - Remove this InterpreterGroup: spark-anonymous as all the sessions are closed
 ....
 INFO [2022-06-15 02:25:22,430] ({spark-close} ManagedInterpreterGroup.java[close]:106) - Remove this InterpreterGroup: spark-Mavaca as all the sessions are closed
 ....
 INFO [2022-06-15 02:25:22,430] ({spark-close} RemoteInterpreterManagedProcess.java[stop]:80) - Stop interpreter process for interpreter group: spark-Mavaca
 INFO [2022-06-15 02:25:22,437] ({pool-7-thread-773} RemoteInterpreterEventServer.java[unRegisterInterpreterProcess]:190) - Unregister interpreter process: spark-Mavaca
 WARN [2022-06-15 02:25:22,437] ({pool-7-thread-773} RemoteInterpreterEventServer.java[unRegisterInterpreterProcess]:194) - Unable to unregister interpreter process because no such interpreterGroup: spark-Mavaca
 WARN [2022-06-15 02:25:22,662] ({Exec Default Executor} ExecRemoteInterpreterProcess.java[onProcessComplete]:226) - Process is exited with exit value 0
 INFO [2022-06-15 02:25:22,663] ({Exec Default Executor} ProcessLauncher.java[transition]:109) - Process state is transitioned to COMPLETED
 INFO [2022-06-15 02:25:24,942] ({spark-close} ExecRemoteInterpreterProcess.java[stop]:136) - Remote exec process of interpreter group: spark-Mavaca is terminated
....
--END--

    #
    # Spark interpreter restart called by user 'Halda', closing interpreters for 'spark-anonymous', 'spark-Halda' and 'spark-Mavaca'.
    #


# -----------------------------------------------------
# Test with 3 users doing 1 loop (wrong).
#[root@ansibler]

    test-loop 3 1 \
    | tee /tmp/test-loop.json

    jq '.' /tmp/test-loop.json

    grep 'Result:' /tmp/results/*.txt

--START--
/tmp/results/multi-user-01-00.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-01-01.txt:------------ Test Result: [ERROR] ------------
/tmp/results/multi-user-01-02.txt:------------ Test Result: [ERROR] ------------
--END--

    #
    # Still failing ..
    #

--START--
....
 WARN [2022-06-15 02:30:43,448] ({pool-7-thread-835} RemoteInterpreterEventServer.java[registerInterpreterProcess]:172) - Unable to register interpreter process, because no such interpreterGroup: spark-Carclop
 WARN [2022-06-15 02:31:10,155] ({SchedulerFactory17} ExecRemoteInterpreterProcess.java[waitForReady]:199) - Ready timeout reached
 WARN [2022-06-15 02:31:10,155] ({SchedulerFactory17} ProcessLauncher.java[onTimeout]:113) - Process launch is time out.
 WARN [2022-06-15 02:31:10,158] ({SchedulerFactory17} NotebookServer.java[onStatusChange]:1986) -
    Job paragraph_1655260239378_183842340 is finished,
    status: ERROR,
    exception: null,
    result: %text org.apache.zeppelin.interpreter.InterpreterException:
        java.io.IOException:
            Interpreter Process creation is time out in 30 seconds
            You can increase timeout threshold via setting zeppelin.interpreter.connect.timeout of this interpreter.
Interpreter download command:
    /etc/alternatives/jre/bin/java
        -Dfile.encoding=UTF-8
        -Dlog4j.configuration=file:///home/fedora/zeppelin/conf/log4j.properties
        -Dlog4j.configurationFile=file:///home/fedora/zeppelin/conf/log4j2.properties
        -Dzeppelin.log.file=/home/fedora/zeppelin/logs/zeppelin-interpreter-spark-Carclop-Carclop-fedora-iris-gaia-blue-20220613-zeppelin.log
        -cp :/home/fedora/zeppelin/interpreter/spark/*
            :/home/fedora/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar
            :/home/fedora/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar
            :/opt/hadoop/etc/hadoop org.apache.zeppelin.interpreter.remote.RemoteInterpreterDownloader
        10.10.2.210
        35643
        spark
        /home/fedora/zeppelin/local-repo/spark
[INFO] Interpreter launch command:
    /opt/spark/bin/spark-submit
        --class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer
        --driver-class-path
            :/home/fedora/zeppelin/local-repo/spark/*
            :/home/fedora/zeppelin/interpreter/spark/*
            :/home/fedora/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar
            :/home/fedora/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar
            :/opt/hadoop/etc/hadoop
        --driver-java-options
            -Dfile.encoding=UTF-8
            -Dlog4j.configuration=file:///home/fedora/zeppelin/conf/log4j.properties
            -Dlog4j.configurationFile=file:///home/fedora/zeppelin/conf/log4j2.properties
            -Dzeppelin.log.file=/home/fedora/zeppelin/logs/zeppelin-interpreter-spark-Carclop-Carclop-fedora-iris-gaia-blue-20220613-zeppelin.log
        --proxy-user Carclop
        --conf spark.yarn.dist.archives=/opt/spark/R/lib/sparkr.zip#sparkr
        --conf spark.submit.deployMode=client
        --conf spark.webui.yarn.useProxy=false
        --conf spark.yarn.isPython=true
        --conf spark.app.name=spark-Carclop
        --conf spark.master=yarn
        /home/fedora/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar
        10.10.2.210
        35643
        spark-Carclop
....
--END--

    #
    # ** The three test runs for the same user **
    # I've been getting it worng all day :-(
    # First iteration works.
    # Second and third iterations fail.
    #
    # Add a 30 second delay between test loops ?
    #

    #
    # Swap usercount and loopcount params, because I keep getting it worng.
    #

# -----------------------------------------------------
# Update our test-loop function.
#[root@ansibler]

    test-loop()
        {
        local usercount=${1:?'usercount required'}
        local loopcount=${2:?'loopcount required'}
        local looppause=${3:-10}
        local delaystart=${4:-1}
        local delaynotebook=${5:-1}

        rm -f /tmp/results/*

cat << EOF
    {
    "usercount": "${usercount}",
    "loopcount": "${loopcount}",
    "looppause": "${looppause}",
    "delaystart": "${delaystart}",
    "delaynotebook": "${delaynotebook}",
    "iterations": [
EOF

        local comma=''
        for i in $(seq 0 $((loopcount - 1)))
        do

            testname="multi-user-$(printf "%02d" ${usercount})-$(printf "%02d" ${i})"

cat << EOF
            ${comma}
            {
            "iteration": ${i},
            "testname": "${testname}",
            "threads":
EOF

            sleep "${looppause}"

            /tmp/run-benchmark.py \
                "${endpoint:?}" \
                "${testconfig:?}" \
                "${testusers:?}" \
                "${usercount:?}" \
                "${delaystart:?}" \
                "${delaynotebook:?}" \
            > "/tmp/results/${testname:?}.txt"

            filter-results "${testname:?}"

cat << EOF
            }
EOF
            comma=','

        done

cat << EOF
        ]
    }
EOF
        }


# -----------------------------------------------------
# Test with 1 user doing 3 loops.
#[root@ansibler]

    test-loop 1 3 \
    | tee /tmp/test-loop.json

    jq '.' /tmp/test-loop.json

    grep 'Result:' /tmp/results/*.txt

--START--
/tmp/results/multi-user-01-00.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-01-01.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-01-02.txt:------------ Test Result: [PASS] ------------
--END--


# -----------------------------------------------------
# Test with 2 users doing 3 loops.
#[root@ansibler]

    test-loop 2 3 \
    | tee /tmp/test-loop.json

    jq '.' /tmp/test-loop.json

    grep 'Result:' /tmp/results/*.txt

--START--
/tmp/results/multi-user-02-00.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-02-01.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-02-02.txt:------------ Test Result: [PASS] ------------
--END--


# -----------------------------------------------------
# Test with 4 users doing 4 loops.
#[root@ansibler]

    test-loop 4 4 \
    | tee /tmp/test-loop.json

    jq '.' /tmp/test-loop.json

    grep 'Result:' /tmp/results/*.txt

--START--
/tmp/results/multi-user-04-00.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-04-01.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-04-02.txt:------------ Test Result: [PASS] ------------
/tmp/results/multi-user-04-03.txt:------------ Test Result: [PASS] ------------
--END--


# -----------------------------------------------------
# Test with 6 users doing 4 loops.
#[root@ansibler]

    test-loop 6 4 \
    | tee /tmp/test-loop.json

    jq '.' /tmp/test-loop.json

    grep 'Result:' /tmp/results/*.txt

--START--
....
....
--END--

5:01

