#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Discussion on the IRIS TWG Slack channel regarding performance issues on the Arcus Openstack system.

    Result:

        Work in progress ...

# -----------------------------------------------------

Wed 12th Oct

Dave Morris
  3:29 PM
@Paul Browne
 Seeing really slow performance on the Arcus cloud at the moment.
3:29
Are there any known issues with Arcus ?


Paul Browne
  3:33 PM
Not seeing any right now, but would benefit from more technical context on what "slow performance" qualitatively means for your experience versus not slow


Paul Browne
  3:49 PM
Ee.g. is the signal of slowness instance build times going anomalously higher, and are these booting to local disk or to volumes, or is it volume or CephFS performance, or network i/o in or out of the cloud, etc. That kind of additional detail is very helpful.


Dave Morris
  4:56 PM
So far we have been unable to narrow it down to a specific thing.
4:57
Our automated Ansible deployment normally takes ~45min, now takes 1hr:20
4:58
Spark analysis runs that should take ~20 min are taking ~1hr


John Garbutt
  4:58 PM
do you know when that slow down started?


Dave Morris
  4:59 PM
Interesting thing is .. we have a system that was deployed 2022-08-25 which is still running at normal speed
4:59
new deployments done in the last few days are running slow
4:59
bot old and new are on the same Openstack system
5:00
it feels like it might be network related - cpu is idle for a lot of the time


John Garbutt
  5:00 PM
if you can get paul some instance uuid for the fast ones and slow ones, he can map that back to the affected hypervisors


Dave Morris
  5:00 PM
yep can do
5:01
give me 5 min and I'll have a list for you


Dave Morris
  5:10 PM
slow system iris-gaia-blue-20221011
openstack-slow-system.txt

Slow system - iris-gaia-blue-20221011
---- ----
Servers
Click to expand inline (83 lines)



5:10
Fast system iris-gaia-green-20220825
openstack-fast-system.txt

Fast system - iris-gaia-green-20220825
---- ----
Servers
Click to expand inline (91 lines)


Dave Morris
  5:17 PM
We were still trying to identify more details.
5:17
First step was to check that there wasn't an already known issue sith the system.
:+1:
1



John Garbutt
  5:22 PM
I don't have any access to investigate, but that looks like it should help paul have a quick check if the router is affected by some other user saturating it.


Paul Browne
  9:21 PM
I'll take a look, also some timestamps of those slow Ansible runs versus faster ones will be helpful, as then that'll let me specify firm time windows in metrics, etc to look into at different points in the cloud.
Does the Ansible or the Spark runs rely a lot on external traffic to the wider Internet and/or to shares in CephFS? Or is most computation done locally on faster local disk and results then transferred to CephFS or object buckets?
That the fast and slow results come from 2 different projects, so not sharing routers, leads me to wonder if the slow result has CephFS routing placed on a node that is also hosting noisy neighbour router traffic (quite likely another Iris group utilising CephFS). But I'll take a look and see what might leap out. Certainly earlier today we were seeing some high-ish read throughout, but not IOPs actually, which remained low along with OSD latencies.


Dave Morris
  5:56 AM
It isn't just these two projects, my colleague Stelios also reported slow performance on a deployment on the gaia-red project also deployed today.
5:59
The Ansible build starts with nothing, allocates a new set of VMs and then logs in to them and installs all the software. Which does use external access to fetch the software packages.
5:59
A normal deployment would take ~45min, today they were taking ~80min.
6:01
The science analysis run uses a lot of data on the CephFS shares, but I don't think that is the problem.
6:01
We are seeing the normal level of iowait for the CephFS access.
6:02
It 'feels' like a problem with initiating connections between the nodes.
6:03
I say 'feels' because I don't have hard numbers yet, but that's what it looks like watching the logs of the Spark processes.
6:04
Spark uses a lot of HTTP messages between the nodes to setup the distributed jobs to process each step.
6:05
and it looks like there is a delay in setting up each connection between the nodes


Dave Morris
  6:09 AM
I can't try a ping between the nodes because firewall doesn't allow it :disappointed:
4 replies
Last reply 1 day agoView thread
    ---
    Paul Browne
      1 day ago
    Firewall would be under your control here though, since the traffic is flowing on a user-created network?


    Dave Morris
      1 day ago
    Would a ICMP ping test provide useful information ?


    Paul Browne
      1 day ago
    Not particularly, probably, I was just confused why you can't just allow that traffic in security group rules if you want to.


    Dave Morris
      1 day ago
    You are right - just that developer time is limited so I'd prioritise the test that returns the best information.
    ---

Dave Morris
  6:31 AM
A test using ssh doesn't show a significant difference
6:31
for w in {1..6}
do
    echo
    time for i in {1..100} ; do ssh "worker0${w}" hostname ; done
done


Dave Morris
  6:37 AM
iris-gaia-green = avg 20.87
iris-gaia-blue = avg 21.99


Dave Morris
  4:21 PM
If we are stuck, we could re-deploy the blue system today to see if the problem is still an issue.
1 reply
Today at 2:08 PMView thread
    ---
    Paul Browne
      2 hours ago
    Sorry, haven't had too much time to dig into this today. So if you re-deploy blue then send me the same list of node UUIDs then I go take a closer look.
    ---

Whatever was causing the problem has been fixed.
6:01
Performance is back to normal.
6:02
Deployment build times :
20221011T055425 46m18.993s <-- normal
20221011T105132 80m38.029s
20221013T152151 105m3.383s
20221013T183746 43m52.136s <-- back to normal
(edited)
6:04
Problem started on 2022/10/11 between 05:54:25 and 10:51:32
6:04
Problem solved on 2022/10/13 between 15:21:51 and 18:37:46


Paul Browne
  2:14 PM
Nothing had changed on this side, so this resolution is unsatisfying on this side at the moment :disappointed:
I had wondered if the behaviour you're seeing may be coming from instances landing on hosts that then become CPU over-subscribed, but all your flavors are pinning to host NUMA so this should not be the case. This would be possible on hosts/projects running public flavors where CPU over-subscription is allowed.
One thing we could do is expedite the rebuilds of all your hypervisor hosts to the latest node image, which is a big upgrade from the one most are on currently. This process is happening in the background in the cloud as we move forward with upgrades to the cloud, but we can bring it forward for your projects potentially.
If it's possible for you to clear down the majority of your instances across the {data,red,green,blue} projects, then I could get those hosts rebuilt and re-integrated probably inside of 1 day. Otherwise it's a slower process of evacuating a host, then rebuilding and re-integrating and moving onto the next.
2:17
I did hear from our central networks that a problem with Janet connectivity had reduced Cambridge connectivity onto Janet by 50%, which has since been fixed. But the problems as described seemed more to be with inter-instance connectivity, rather than N-S onto global Internet.

Dave Morris
  2:38 PM
I agree,  unknown cause is unsatisfying, but we may have to live with it for now.
2:39
The instance on red is long lived, but we can backup the data and shut it down if needed.
2:40
On a normal day one of the [red|green|blue] projects is running our live services, the other two colours are used to development and testing.
2:40
So if we plan for it we could reduce everything down to just one colour for a day.
2:42
We need to get a new release done in the next few days, so I'd like to hold off until that is done.
2:45
Is it OK to leave this for a few days, let us get the new version made live and then talk again middle of next week ?


