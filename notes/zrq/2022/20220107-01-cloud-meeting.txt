#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#


    Transfer from Cumulus to Arcus

        Transfer the data

            S3 to echo - looked to be too slow
                What is the best practice for this ?
                Docker container demo we can all just use ?
                    mount target as a voilume, run transfer

            S4 client - stack trace, skipped
                Docker container demo we can all just use ?

            S5 client - stack trace, skipped
                Docker container demo we can all just use ?

            On the subject of S3 - has anyone used OpenstackSwift/S3 with Spark ?
                OpenstackSwift/S3 threw stack trace in Amazon AWS code
                EchoS3 works with with Spark, but long distance = high latency

            Resorted to using rsync over ssh

                Reliable, well documented, used it many times before.
                Small VM on each cloud, rsync over SSH from one to the other using public floating IP address.

                Transfer (size-only)

                    >   Mon Jan  3 18:32:05 UTC 2022
                    >   aglais-20211229-machine
                    >   ----
                    >   receiving incremental file list
                    >   ....
                    >   Number of files: 8,205 (reg: 8,196, dir: 5, link: 4)
                    >   ....
                    >   Total file size: 1.03T bytes
                    >   ....
                    >   sent 155.81K bytes  received 1.03T bytes  76.20M bytes/sec
                    >   total size is 1.03T  speedup is 1.00
                    >   Tue Jan  4 09:51:26 UTC 2022

                        Mon Jan  3 18:32:05
                        Tue Jan  4 09:51:26

                        15 hours, 19 minutes and 21 seconds
                        1Tbyte of data

                Verify (checksum,dry-run)

                    >   Thu Jan  6 01:13:55 UTC 2022
                    >   aglais-20211229-machine
                    >   ----
                    >   Share [aglais-data-gaia-edr3-2048]
                    >   receiving incremental file list
                    >   ....
                    >   Number of files: 8,205 (reg: 8,196, dir: 5, link: 4)
                    >   Total file size: 1.03T bytes
                    >   ....
                    >   sent 36 bytes  received 813.51K bytes  48.27 bytes/sec
                    >   total size is 1.03T  speedup is 1,265,587.78 (DRY RUN)
                    >   Thu Jan  6 05:54:50 UTC 2022

                        Thu Jan  6 01:13:55
                        Thu Jan  6 05:54:50

                        4 hours, 40 minutes and 55 seconds
                        1Tbyte of data

                ** This has consequences for an on-demand platform.
                ** 15min to configure the cluster and 15hrs to load the data isn't really 'on-demand'.
                ** Is there a better way ?

        Configure Manila/CephFS shares

            Network router configuration is un-documented.
            CephFS servers are on a hidden network, accessible via the cululus-internal network.
            The network CIDR and routing is un-documented.

            Cumulus configuration was reversed engineered from a Terrafom example from John.
            It seemed to work, but no one has checked if it is correct.
            Is there a new way - saw a comment that K8s deployments no longer needed to set up a separate router ?

            Similar pattern on the Arcus network, but confusingly the intermediate network is called 'cephfs'.
            CephFS servers are on a hidden network, behind the 'cephfs' network.
            Again, the network CIDR and routing is un-documented.

        Configure the VM network

            Possibly one remaining issue with the network routing.
            Using Pauls test deploy works, our own test deploy desn't work.
            Our test deploy instrustions work on Cumulus but not on Arcus.
            Can't see a difference, but haven't had time to check in detail.
            I need to deploy again to check.


    Balzar

        What we have now isn't a cloud.
        Gaia projects are pinned to specific hardware.
        We have the disadantages of virtualization without the benefits.

        Workshop example is the big use case.
        On demand platform means being able to scale up by x30 for a workshop next week.

        Dennis's experiments are another use case.
        Started with small cluster to analyse the Gaia data, gradually increasing the size each time we encountered limits.
        Started with 4+1 medium nodes, added more to the Spark side, 6+1 medium, then added more to the Zeppelin node, 6 medium +1 large.
        Clustering algorithm is single instance memory intensive, doubling the size each time. Now using 180G of memory in Zeppelin node (*)
        System needs to scale in response to requests from users, +1 day to double is OK, anything more is slow.

        OutOfMemory issues with using 180G of memory in rsync, might be an issue with Zeppelin ?
        Will need to run more tests in the new year.

    Spark

        We now have a set of use cases to demonstrate what happens when it hits resource limits.
        Many cases where it fails quietly.
        Developing tools to recognise and rep[ort these.











