#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Try to find out more about the limits on concurrent users.
        Started with a clean deployment 20220601-01-blue-deploy.txt

    Result:

        Work in progress ...

# -----------------------------------------------------
# Create some test users.
# TODO Move the create-user-tools to ansible/client/bin.
# TODO Add ansible/client/bin to the client PATH.
#[root@ansibler]

    source /deployments/zeppelin/bin/create-user-tools.sh

    testnames01=(
        Rhaelhall
        Fipa
        Mythicson
        Balline
        Hiness
        Anskelisia
        Iflee
        Mischiellis
        Kellaug
        Liphima
        Jarters
        Williazoga
        Carrovieus
        Pierione
        Hayesphasia
        Collinotter
        Adazoga
        Harinabla
        Sanderlotus
        Bellgrin
        )

    testnames02=(
        Hamar
        Carclop
        Halda
        Jaden
        Mavaca
        Franilley
        Masonania
        Webbbron
        Granwaler
        Stelama
        )

    testnames03=(
        Smical
        Reyesfan
        Evison
        Surbron
        Floresslight
        )

    createarrayusers \
        "${testnames01[@]}" \
    | tee /tmp/testusers-01.json \
    | jq '[ .users[] | {"name": .shirouser.name, "pass": .shirouser.pass} ]'

    >   [
    >     {
    >       "name": "Rhaelhall",
    >       "pass": "ea8aiqu1liubachohthahwieh3ko1O"
    >     }
    >     {
    >       "name": "Fipa",
    >       "pass": "eigheiZoo9Mei4fereim1ahp3weu1E"
    >     }
    >     ....
    >     ....
    >   ]


    createarrayusers \
        "${testnames02[@]}" \
    | tee /tmp/testusers-02.json \
    | jq '[ .users[] | {"name": .shirouser.name, "pass": .shirouser.pass} ]'

    >   [
    >     {
    >       "name": "Hamar",
    >       "pass": "fausaimuugh8ue3aNowaig4uozes6o"
    >     },
    >     {
    >       "name": "Carclop",
    >       "pass": "aeX8Xie9oozeasiehoh4pheeyahliC"
    >     },
    >     ....
    >     ....
    >   ]


    createarrayusers \
        "${testnames03[@]}" \
    | tee /tmp/testusers-03.json \
    | jq '[ .users[] | {"name": .shirouser.name, "pass": .shirouser.pass} ]'

    >   [
    >     {
    >       "name": "Smical",
    >       "pass": "roh1ohpaeYohY4hiequeiseiMoh0ah"
    >     },
    >     {
    >       "name": "Reyesfan",
    >       "pass": "eeyah5iegeis5ne6ohPh4hagaiduk8"
    >     },
    >     ....
    >     ....
    >   ]


# -----------------------------------------------------
# Create our benchmark script.
# TODO Create run-benchmark.py in ansible/client/bin.
# Learning Python:
#   Command line args
#   https://realpython.com/python-command-line-arguments/
#   String.format()
#   https://docs.python.org/3/library/string.html#formatstrings
#   Escape {} in format()
#   https://stackoverflow.com/a/5466478
#[root@ansibler]

    cat > /tmp/run-benchmark.py << 'EOF'
#!/bin/python3
import sys
from aglais_benchmark import AglaisBenchmarker

try:

    opts = [opt for opt in sys.argv[1:] if opt.startswith("-")]
    args = [arg for arg in sys.argv[1:] if not arg.startswith("-")]

    endpoint = args[0]
    testconfig = args[1]
    userlist = args[2]
    usercount = int(args[3])

except IndexError:

    raise SystemExit(f"Usage: {sys.argv[0]} <Zepelin endpoint> <test config> <list of users> <number of users>")

print(
"""
{{
\"config\": {{
    \"endpoint\":   \"{}\",
    \"testconfig\": \"{}\",
    \"userlist\":   \"{}\",
    \"usercount\":  \"{}\"
    }}
}}
""".format(
        endpoint,
        testconfig,
        userlist,
        usercount
        )
    )

AglaisBenchmarker(
    testconfig,
    userlist,
    "/tmp/",
    endpoint
    ).run(
        concurrent=True,
        users=usercount
        )

EOF

    chmod 'a+x' /tmp/run-benchmark.py


# -----------------------------------------------------
# Run a quick test with one user.
#[root@ansibler]

    mkdir /tmp/results

    endpoint="http://zeppelin:8080"

    testconfig=/deployments/zeppelin/test/config/quick.json

    testusers=/tmp/testusers-01.json
    testname=single-user-01
    usercount=1

    /tmp/run-benchmark.py \
        "${endpoint:?}"  \
        "${testconfig:?}"  \
        "${testusers:?}" \
        "${usercount:?}" \
    | tee "/tmp/results/${testname:?}.txt"

    >   b'Create notebook: 2H4STBJ8Y\n'
    >   b'Create notebook: 2H74SGMY6\n'
    >   b'Create notebook: 2H6XN9CJP\n'
    >   b'Create notebook: 2H7BKXBZW\n'
    >   Test completed! (34.56 seconds)
    >   ------------ Test Result: [PASS] ------------
    >   [{'GaiaDMPSetup': { .... }}]


    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        "/tmp/results/${testname:?}.txt" \
    | tee "/tmp/results/${testname:?}.json" \
    | jq '
        .[] | keys as $x | [ $x[] as $y | {name: $y, value: .[$y].result, time: .[$y].time.elapsed } ]
        '

    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.42
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 9.98
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 5.50
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 15.64
    >     }
    >   ]


# -----------------------------------------------------
# Step up to 4 users.
#[root@ansibler]

    loopcount=0
    usercount=4
    testname="multi-user-$(printf "%02d" ${usercount})-$(printf "%02d" ${loopcount})"

    /tmp/run-benchmark.py \
        "${endpoint:?}"  \
        "${testconfig:?}"  \
        "${testusers:?}" \
        "${usercount:?}" \
    | tee "/tmp/results/${testname:?}.txt"

    >   Test started [Multi User]
    >   b'Create notebook: 2H68V7CMA\n'
    >   b'Create notebook: 2H76YA1VH\n'
    >   b'Create notebook: 2H6PH6UMX\n'
    >   b'Create notebook: 2H3Z1747B\n'
    >   b'Create notebook: 2H4DCCJ4G\n'
    >   b'Create notebook: 2H7A9GPYV\n'
    >   b'Create notebook: 2H73F4DPN\n'
    >   b'Create notebook: 2H5FHSQAE\n'
    >   b'Create notebook: 2H49QWXPN\n'
    >   b'Create notebook: 2H6VHC35E\n'
    >   b'Create notebook: 2H6X7UU2N\n'
    >   b'Create notebook: 2H68BYQ2P\n'
    >   b'Create notebook: 2H469V5GK\n'
    >   b'Create notebook: 2H4MEZTQR\n'
    >   b'Create notebook: 2H4Q8DYEE\n'
    >   b'Create notebook: 2H4GYCUYS\n'
    >   Test completed! (288.09 seconds)
    >   ------------ Test Result: [PASS] ------------
    >   [{'GaiaDMPSetup': { .... }}]


    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        "/tmp/results/${testname:?}.txt" \
    | tee "/tmp/results/${testname:?}.json" \
    | jq '
        .[] | keys as $x | [ $x[] as $y | {name: $y, value: .[$y].result, time: .[$y].time.elapsed } ]
        '

    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.76
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.82
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 16.08
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 18.80
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 51.87
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 9.71
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 40.85
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 19.91
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 107.21
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 9.60
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 126.54
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 44.72
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 107.22
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 9.64
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 94.14
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 13.41
    >     }
    >   ]


# -----------------------------------------------------
# Step up to 6 users.
#[root@ansibler]

    loopcount=0
    usercount=6
    testname="multi-user-$(printf "%02d" ${usercount})-$(printf "%02d" ${loopcount})"

    /tmp/run-benchmark.py \
        "${endpoint:?}"  \
        "${testconfig:?}"  \
        "${testusers:?}" \
        "${usercount:?}" \
    | tee "/tmp/results/${testname:?}.txt"


    >   Test started [Multi User]
    >   b'Create notebook: 2H7ASRFR5\n'
    >   b'Create notebook: 2H52UD857\n'
    >   ....
    >   ....
    >   b'Create notebook: 2H5U6N7R3\n'
    >   b'Create notebook: 2H584PBEF\n'
    >   Test completed! (190.47 seconds)
    >   ------------ Test Result: [PASS] ------------
    >   [{'GaiaDMPSetup': { .... }}]


    sed "
        0,/^----/ d
        s/\"/#/g
        s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
        s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
        s/:[[:space:]]*\([,}]\),/: ''\1/g
        s/'/\"/g
        " \
        "/tmp/results/${testname:?}.txt" \
    | tee "/tmp/results/${testname:?}.json" \
    | jq '
        .[] | keys as $x | [ $x[] as $y | {name: $y, value: .[$y].result, time: .[$y].time.elapsed } ]
        '

    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.12
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.61
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 19.33
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 18.78
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.17
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.54
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 16.27
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 32.62
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.16
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.56
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 16.32
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 34.70
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.17
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.53
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 20.37
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 57.37
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 107.36
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 9.56
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 55.84
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 17.67
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 108.38
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 8.60
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 54.76
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 16.78
    >     }
    >   ]


# -----------------------------------------------------
# Step up to 6 users run 6 times.
#[root@ansibler]

    loopcount=6
    usercount=6

    for i in $(seq 0 $((loopcount - 1)))
    do
        echo ""
        echo "-------------"
        echo "Loop [${i}]"
        testname="multi-user-$(printf "%02d" ${usercount})-$(printf "%02d" ${i})"

        /tmp/run-benchmark.py \
            "${endpoint:?}"  \
            "${testconfig:?}"  \
            "${testusers:?}" \
            "${usercount:?}" \
        | tee "/tmp/results/${testname:?}.txt"

    done



    >   Test started [Multi User]
    >   ERROR:root:list index out of range
    >   Traceback (most recent call last):
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 114, in run_notebook
    >       notebookid = text.split(": ")[1]
    >   IndexError: list index out of range
    >   b'Create notebook: 2H6ZRHKWC\n'
    >   b'Create notebook: 2H4KNWDFC\n'
    >   b'Create notebook: 2H6XNQ84T\n'
    >   b'Create notebook: 2H5SM7Q62\n'
    >   b'status_code:500\n'
    >   b'Create notebook: 2H7GPPB4S\n'
    >   b'Create notebook: 2H3Y5DQJ8\n'
    >   b'Create notebook: 2H6Q37HQT\n'
    >   b'Create notebook: 2H52UWVZW\n'
    >   b'Create notebook: 2H6944K21\n'
    >   b'Create notebook: 2H656UC5E\n'
    >   b'Create notebook: 2H5XT1J5P\n'
    >   b'Create notebook: 2H669UWG5\n'
    >   b'Create notebook: 2H3PDE7SC\n'
    >   b'Create notebook: 2H56PXTTU\n'
    >   b'Create notebook: 2H4YBGHD3\n'
    >   b'Create notebook: 2H6X7DEDG\n'
    >   b'Create notebook: 2H4DD8DA5\n'
    >   b'Create notebook: 2H5K5X7YD\n'
    >   b'Create notebook: 2H4BMKJ6R\n'
    >   b'Create notebook: 2H74Q14H5\n'
    >   b'Create notebook: 2H54EKUWZ\n'
    >   b'Create notebook: 2H6TNZ9RS\n'
    >   b'Create notebook: 2H6Z9WD4H\n'
    >   Test completed! (97.63 seconds)
    >   ------------ Test Result: [FAIL] ------------
    >   [{'GaiaDMPSetup': { .... }}]


    filter-results()
        {
        local testname=${1:?'testname required'}
        sed "
            0,/^----/ d
            s/\"/#/g
            s/'\(-\{0,1\}[0-9.]\{1,\}\)'/\1/g
            s/:[[:space:]]*\([a-zA-Z]\{1,\}\)\([,}]\)/:'\1'\2/g
            s/:[[:space:]]*\([,}]\),/: ''\1/g
            s/'/\"/g
            " \
            "/tmp/results/${testname:?}.txt" \
        | tee "/tmp/results/${testname:?}.json" \
        | jq '
            .[] | keys as $x | [ $x[] as $y | {name: $y, value: .[$y].result, time: .[$y].time.elapsed } ]
            '
        }

    filter-results "${testname}"

    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.84
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.52
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.05
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 44.80
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.89
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 4.52
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 18.35
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 17.81
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.94
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.43
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 6.93
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 80.30
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.91
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.42
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 6.95
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 48.99
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "FAIL",
    >       "time": 1.14
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.39
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 5.67
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 38.74
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.93
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.48
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 5.85
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 37.53
    >     }
    >   ]


# -----------------------------------------------------
# repeat 6 users run 6 times.
#[root@ansibler]

    loopcount=6
    usercount=6

    for i in $(seq 0 $((loopcount - 1)))
    do
        echo ""
        echo "-------------"
        echo "Loop [${i}]"
        testname="multi-user-$(printf "%02d" ${usercount})-$(printf "%02d" ${i})"

        /tmp/run-benchmark.py \
            "${endpoint:?}"  \
            "${testconfig:?}"  \
            "${testusers:?}" \
            "${usercount:?}" \
        | tee "/tmp/results/${testname:?}.txt"

        filter-results "${testname:?}"

    done



    >   -------------
    >   Loop [0]
    >   
    >   {
    >   "config": {
    >       "endpoint":   "http://zeppelin:8080",
    >       "testconfig": "/deployments/zeppelin/test/config/quick.json",
    >       "userlist":   "/tmp/testusers-01.json",
    >       "usercount":  "6"
    >       }
    >   }
    >   
    >   /tmp/testusers-01.json
    >   Test started [Multi User]
    >   ERROR:root:list index out of range
    >   Traceback (most recent call last):
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 114, in run_notebook
    >       notebookid = text.split(": ")[1]
    >   IndexError: list index out of range
    >   ERROR:root:list index out of range
    >   Traceback (most recent call last):
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 114, in run_notebook
    >       notebookid = text.split(": ")[1]
    >   IndexError: list index out of range
    >   b'Create notebook: 2H71J3HT8\n'
    >   b'status_code:500\n'
    >   b'Create notebook: 2H3M55AZQ\n'
    >   b'Create notebook: 2H71K64B2\n'
    >   b'Create notebook: 2H7F7ECPE\n'
    >   b'Create notebook: 2H6Y6K2X3\n'
    >   b'Create notebook: 2H4SN74SQ\n'
    >   b'Create notebook: 2H6M2UTS9\n'
    >   b'Create notebook: 2H5GQUMGX\n'
    >   b'Create notebook: 2H4F27JU6\n'
    >   b'Create notebook: 2H6S33VH6\n'
    >   b'Create notebook: 2H5Z6UY1V\n'
    >   b'Create notebook: 2H4T2VS63\n'
    >   b'Create notebook: 2H54W82HW\n'
    >   b'Create notebook: 2H6V3RJ14\n'
    >   b'Create notebook: 2H4X2ZAWN\n'
    >   b'Create notebook: 2H4RKGA4E\n'
    >   b'Create notebook: 2H714QB6B\n'
    >   b'Create notebook: 2H69ZASTW\n'
    >   b'Create notebook: 2H7ESVDH7\n'
    >   b'status_code:500\n'
    >   b'Create notebook: 2H5HYUVU8\n'
    >   b'Create notebook: 2H3XZMVD2\n'
    >   b'Create notebook: 2H4SES5ND\n'
    >   Test completed! (98.09 seconds)
    >   ------------ Test Result: [FAIL] ------------
    >   [{'GaiaDMPSetup': { .... }}]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.05
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.40
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 6.88
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 79.32
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.09
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.68
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "FAIL",
    >       "time": 0.96
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 27.31
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "FAIL",
    >       "time": 1.23
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.47
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.93
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 82.42
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.12
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.48
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 17.26
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 21.91
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.08
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.53
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 17.30
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 34.36
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.07
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.52
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 16.23
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 29.16
    >     }
    >   ]
    >   
    >   -------------
    >   Loop [1]
    >   
    >   {
    >   "config": {
    >       "endpoint":   "http://zeppelin:8080",
    >       "testconfig": "/deployments/zeppelin/test/config/quick.json",
    >       "userlist":   "/tmp/testusers-01.json",
    >       "usercount":  "6"
    >       }
    >   }
    >   
    >   /tmp/testusers-01.json
    >   Test started [Multi User]
    >   ERROR:root:list index out of range
    >   Traceback (most recent call last):
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 114, in run_notebook
    >       notebookid = text.split(": ")[1]
    >   IndexError: list index out of range
    >   b'Create notebook: 2H64VTQ6Z\n'
    >   b'status_code:500\n'
    >   b'Create notebook: 2H3KU4T52\n'
    >   b'Create notebook: 2H4E8HU89\n'
    >   b'Create notebook: 2H5DDF7N5\n'
    >   b'Create notebook: 2H7JWA1WU\n'
    >   b'Create notebook: 2H4YA7Q18\n'
    >   b'Create notebook: 2H7J1TU3E\n'
    >   b'Create notebook: 2H4TZJZKR\n'
    >   b'Create notebook: 2H5JEE9A6\n'
    >   b'Create notebook: 2H3PXTCZ1\n'
    >   b'Create notebook: 2H5EUURTH\n'
    >   b'Create notebook: 2H6D2T488\n'
    >   b'Create notebook: 2H3PNNPBY\n'
    >   b'Create notebook: 2H6KY7RCP\n'
    >   b'Create notebook: 2H5V9T2B1\n'
    >   b'Create notebook: 2H434WDEV\n'
    >   b'Create notebook: 2H4AHWZ96\n'
    >   b'Create notebook: 2H44MU5KU\n'
    >   b'Create notebook: 2H5DY9QH9\n'
    >   b'Create notebook: 2H5R7VTWU\n'
    >   b'Create notebook: 2H3SDTRBK\n'
    >   b'Create notebook: 2H4VWQ993\n'
    >   b'Create notebook: 2H3VEV4DZ\n'
    >   Test completed! (95.58 seconds)
    >   ------------ Test Result: [FAIL] ------------
    >   [{'GaiaDMPSetup': { .... }}]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.95
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.42
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.04
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 78.13
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.01
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.42
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 18.51
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 17.74
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.96
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.41
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.99
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 77.16
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.05
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.63
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 19.49
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 22.97
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.96
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.56
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 5.95
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 33.45
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.00
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.45
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "FAIL",
    >       "time": 1.07
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 22.99
    >     }
    >   ]
    >   
    >   -------------
    >   Loop [2]
    >   
    >   {
    >   "config": {
    >       "endpoint":   "http://zeppelin:8080",
    >       "testconfig": "/deployments/zeppelin/test/config/quick.json",
    >       "userlist":   "/tmp/testusers-01.json",
    >       "usercount":  "6"
    >       }
    >   }
    >   
    >   /tmp/testusers-01.json
    >   Test started [Multi User]
    >   ERROR:root:list index out of range
    >   Traceback (most recent call last):
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 114, in run_notebook
    >       notebookid = text.split(": ")[1]
    >   IndexError: list index out of range
    >   b'Create notebook: 2H5NTYU65\n'
    >   b'Create notebook: 2H4N87JGP\n'
    >   b'Create notebook: 2H7EQ7TRJ\n'
    >   b'Create notebook: 2H66J62DZ\n'
    >   b'Create notebook: 2H7A4HWHA\n'
    >   b'Create notebook: 2H4C5PQTK\n'
    >   b'Create notebook: 2H4H9KX2N\n'
    >   b'Create notebook: 2H6EV6UQA\n'
    >   b'status_code:500\n'
    >   b'Create notebook: 2H6B3RUZT\n'
    >   b'Create notebook: 2H5W18ZZ6\n'
    >   b'Create notebook: 2H57CE3CR\n'
    >   b'Create notebook: 2H73CMGMP\n'
    >   b'Create notebook: 2H5VHG9SC\n'
    >   b'Create notebook: 2H5QBHK89\n'
    >   b'Create notebook: 2H75TQEKP\n'
    >   b'Create notebook: 2H6R8SWDT\n'
    >   b'Create notebook: 2H6XGT5U3\n'
    >   b'Create notebook: 2H71ME4GZ\n'
    >   b'Create notebook: 2H7FYHW14\n'
    >   Test completed! (96.42 seconds)
    >   ------------ Test Result: [FAIL] ------------
    >   [{'GaiaDMPSetup': { .... }}]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.93
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.48
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.83
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 51.12
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.90
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.46
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 17.26
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 19.82
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.98
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.35
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 6.79
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 79.26
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.97
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.50
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 5.80
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 18.78
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "FAIL",
    >       "time": 1.14
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.42
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 5.68
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 33.33
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.95
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.49
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 17.23
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 27.11
    >     }
    >   ]
    >   
    >   -------------
    >   Loop [3]
    >   
    >   {
    >   "config": {
    >       "endpoint":   "http://zeppelin:8080",
    >       "testconfig": "/deployments/zeppelin/test/config/quick.json",
    >       "userlist":   "/tmp/testusers-01.json",
    >       "usercount":  "6"
    >       }
    >   }
    >   
    >   /tmp/testusers-01.json
    >   Test started [Multi User]
    >   b'Create notebook: 2H3WSPQHX\n'
    >   b'Create notebook: 2H55KSSGZ\n'
    >   b'Create notebook: 2H4BV3PZD\n'
    >   b'Create notebook: 2H3WZWTG7\n'
    >   b'Create notebook: 2H6K7UYBV\n'
    >   b'Create notebook: 2H6K7SUKZ\n'
    >   b'Create notebook: 2H5N5X153\n'
    >   b'Create notebook: 2H57EFGB9\n'
    >   b'Create notebook: 2H3TD2JT7\n'
    >   b'Create notebook: 2H6JCAVR5\n'
    >   b'Create notebook: 2H6SR47MC\n'
    >   b'Create notebook: 2H4FY7KRT\n'
    >   b'Create notebook: 2H4E7M2ZA\n'
    >   b'Create notebook: 2H44PRK63\n'
    >   b'Create notebook: 2H4G8QSTR\n'
    >   b'Create notebook: 2H5ETM5B2\n'
    >   b'Create notebook: 2H5TRMM9M\n'
    >   b'Create notebook: 2H6QBGEXU\n'
    >   b'Create notebook: 2H6UVUXNA\n'
    >   b'Create notebook: 2H6JF4PJ6\n'
    >   b'Create notebook: 2H3ZGTCR2\n'
    >   b'Create notebook: 2H4D2GT23\n'
    >   b'Create notebook: 2H45DHY4V\n'
    >   b'Create notebook: 2H4BF9UNQ\n'
    >   Test completed! (67.85 seconds)
    >   ------------ Test Result: [PASS] ------------
    >   [{'GaiaDMPSetup': { .... }}]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.19
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.49
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 14.37
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 39.81
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.26
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.66
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 18.38
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 20.87
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.20
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.51
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.03
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 50.03
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.16
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.52
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 6.00
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 29.29
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.18
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.44
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 17.42
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 25.04
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.28
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.40
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.03
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 51.06
    >     }
    >   ]
    >   
    >   -------------
    >   Loop [4]
    >   
    >   {
    >   "config": {
    >       "endpoint":   "http://zeppelin:8080",
    >       "testconfig": "/deployments/zeppelin/test/config/quick.json",
    >       "userlist":   "/tmp/testusers-01.json",
    >       "usercount":  "6"
    >       }
    >   }
    >   
    >   /tmp/testusers-01.json
    >   Test started [Multi User]
    >   ERROR:root:list index out of range
    >   Traceback (most recent call last):
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 114, in run_notebook
    >       notebookid = text.split(": ")[1]
    >   IndexError: list index out of range
    >   
    >   
    >   b'status_code:500\n'
    >   b'Create notebook: 2H5F1XAGM\n'
    >   b'Create notebook: 2H4NPGZ7H\n'
    >   b'Create notebook: 2H5CVRRD5\n'
    >   b'Create notebook: 2H5ZM37WR\n'
    >   b'Create notebook: 2H7FSWM1N\n'
    >   b'Create notebook: 2H627GV3Y\n'
    >   b'Create notebook: 2H7GCDYHU\n'
    >   b'Create notebook: 2H5DZV9VE\n'
    >   b'Create notebook: 2H73QDRE4\n'
    >   b'Create notebook: 2H6WTZHVJ\n'
    >   b'Create notebook: 2H56DS6T9\n'
    >   b'Create notebook: 2H3MN37E1\n'
    >   b'Create notebook: 2H45XA8KY\n'
    >   b'Create notebook: 2H5KGM7K7\n'
    >   b'Create notebook: 2H7JNGYXN\n'
    >   b'Create notebook: 2H5T78U9T\n'
    >   b'Create notebook: 2H48YEN47\n'
    >   b'Create notebook: 2H6V9K9BJ\n'
    >   b'Create notebook: 2H6F99UVX\n'
    >   b'Create notebook: 2H79A5X5C\n'
    >   b'Create notebook: 2H6PJDV9U\n'
    >   b'Create notebook: 2H6AP11R8\n'
    >   b'Create notebook: 2H5MYZ1ZN\n'
    >   Test completed! (64.28 seconds)
    >   ------------ Test Result: [FAIL] ------------
    >   [{'GaiaDMPSetup': { .... }}]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.14
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.45
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 6.42
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 48.23
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.13
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.39
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.95
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 16.67
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.13
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.53
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.07
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 45.02
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "FAIL",
    >       "time": 1.31
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.63
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 5.72
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 20.19
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.10
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.46
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 6.81
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 34.54
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.10
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.51
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.10
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 28.13
    >     }
    >   ]
    >   
    >   -------------
    >   Loop [5]
    >   
    >   {
    >   "config": {
    >       "endpoint":   "http://zeppelin:8080",
    >       "testconfig": "/deployments/zeppelin/test/config/quick.json",
    >       "userlist":   "/tmp/testusers-01.json",
    >       "usercount":  "6"
    >       }
    >   }
    >   
    >   /tmp/testusers-01.json
    >   Test started [Multi User]
    >   b'Create notebook: 2H545BCKX\n'
    >   b'Create notebook: 2H4F9G2SR\n'
    >   b'Create notebook: 2H4A4A6CD\n'
    >   b'Create notebook: 2H5VRG689\n'
    >   b'Create notebook: 2H3SZ5V3D\n'
    >   b'Create notebook: 2H7EC9H2K\n'
    >   b'Create notebook: 2H7K2MAYX\n'
    >   b'Create notebook: 2H485E7B4\n'
    >   b'Create notebook: 2H6KUZ8A4\n'
    >   b'Create notebook: 2H3ZTEYR5\n'
    >   b'Create notebook: 2H6NYK2P5\n'
    >   b'Create notebook: 2H4XW9EFE\n'
    >   b'Create notebook: 2H41RHXRP\n'
    >   b'Create notebook: 2H4DZ1XJG\n'
    >   b'Create notebook: 2H5ENFVAZ\n'
    >   b'Create notebook: 2H4NHM9EQ\n'
    >   b'Create notebook: 2H573N67Z\n'
    >   b'Create notebook: 2H71Y12QU\n'
    >   b'Create notebook: 2H6QCD9MF\n'
    >   b'Create notebook: 2H5DU71F2\n'
    >   b'Create notebook: 2H6HGY3DB\n'
    >   b'Create notebook: 2H5KNJPT7\n'
    >   b'Create notebook: 2H3PTMUD4\n'
    >   b'Create notebook: 2H7FRWQXZ\n'
    >   Test completed! (65.45 seconds)
    >   ------------ Test Result: [PASS] ------------
    >   [{'GaiaDMPSetup': { .... }}]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.16
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.54
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.54
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 48.18
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.12
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.79
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.60
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 16.84
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.16
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 5.54
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 7.64
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 46.01
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 3.99
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.51
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 6.10
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 20.92
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.19
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.53
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 8.52
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 33.52
    >     }
    >   ]
    >   [
    >     {
    >       "name": "GaiaDMPSetup",
    >       "value": "PASS",
    >       "time": 4.11
    >     },
    >     {
    >       "name": "Library_Validation.json",
    >       "value": "PASS",
    >       "time": 6.64
    >     },
    >     {
    >       "name": "Mean_proper_motions_over_the_sky",
    >       "value": "PASS",
    >       "time": 6.38
    >     },
    >     {
    >       "name": "Source_counts_over_the_sky.json",
    >       "value": "PASS",
    >       "time": 28.41
    >     }
    >   ]


    #
    # 4/6 tests failing, with a 500 error creating a notebook.
    #

    grep 'Result:' /tmp/results/multi-user-06-*.txt

    >   /tmp/results/multi-user-06-00.txt:------------ Test Result: [FAIL] ------------
    >   /tmp/results/multi-user-06-01.txt:------------ Test Result: [FAIL] ------------
    >   /tmp/results/multi-user-06-02.txt:------------ Test Result: [FAIL] ------------
    >   /tmp/results/multi-user-06-03.txt:------------ Test Result: [PASS] ------------
    >   /tmp/results/multi-user-06-04.txt:------------ Test Result: [FAIL] ------------
    >   /tmp/results/multi-user-06-05.txt:------------ Test Result: [PASS] ------------

    #
    # Nothing visible in the Zeppelin logs.
    #

    #
    # Try adding a random pause before the create notebook step.
    #



# -----------------------------------------------------
# Try 6 users run 6 times.
#[root@ansibler]

    loopcount=6
    usercount=6

    for i in $(seq 0 $((loopcount - 1)))
    do
        echo ""
        echo "-------------"
        echo "Loop [${i}]"
        testname="multi-user-$(printf "%02d" ${usercount})-$(printf "%02d" ${i})"

        /tmp/run-benchmark.py \
            "${endpoint:?}"  \
            "${testconfig:?}"  \
            "${testusers:?}" \
            "${usercount:?}" \
        | tee "/tmp/results/${testname:?}.txt"

        filter-results "${testname:?}"
    done


    >   ....
    >   ERROR [2022-06-02 14:26:45,195] ({qtp686466458-26908} WebApplicationExceptionMapper.java[toResponse]:49) - Error response
    >   java.lang.OutOfMemoryError: GC overhead limit exceeded
    >   ....

    >   ....
    >    INFO [2022-06-02 14:21:53,009] ({qtp686466458-26219} VFSNotebookRepo.java[save]:144) - Saving note 2H6DXDKCJ to tmp/K1MB2HNMUH.json_2H6DXDKCJ.zpln
    >   ERROR [2022-06-02 14:21:53,009] ({LuceneSearch13} NoteEventAsyncListener.java[run]:128) - Fail to handle NoteEvent
    >   org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed
    >           at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:877)
    >           at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:891)
    >           at org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1468)
    >           at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1757)
    >           at org.apache.zeppelin.search.LuceneSearch.updateDoc(LuceneSearch.java:240)
    >           at org.apache.zeppelin.search.LuceneSearch.indexNoteName(LuceneSearch.java:398)
    >           at org.apache.zeppelin.search.LuceneSearch.addIndexDocAsync(LuceneSearch.java:324)
    >           at org.apache.zeppelin.search.LuceneSearch.addNoteIndex(LuceneSearch.java:305)
    >           at org.apache.zeppelin.search.SearchService.handleNoteCreateEvent(SearchService.java:108)
    >           at org.apache.zeppelin.notebook.NoteEventAsyncListener$EventHandling.run(NoteEventAsyncListener.java:113)
    >           at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >           at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >           at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.lang.OutOfMemoryError: Java heap space
    >    INFO [2022-06-02 14:21:53,010] ({qtp686466458-26219} NotebookRestApi.java[initParagraph]:1105) - Init Paragraph for user Fipa
    >   ERROR [2022-06-02 14:21:53,010] ({LuceneSearch13} NoteEventAsyncListener.java[run]:128) - Fail to handle NoteEvent
    >   org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed
    >           at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:877)
    >           at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:891)
    >           at org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1468)
    >           at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1757)
    >           at org.apache.zeppelin.search.LuceneSearch.updateDoc(LuceneSearch.java:240)
    >           at org.apache.zeppelin.search.LuceneSearch.addParagraphIndex(LuceneSearch.java:314)
    >           at org.apache.zeppelin.search.SearchService.handleParagraphCreateEvent(SearchService.java:123)
    >           at org.apache.zeppelin.notebook.NoteEventAsyncListener$EventHandling.run(NoteEventAsyncListener.java:119)
    >           at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >           at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >           at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.lang.OutOfMemoryError: Java heap space
    >    INFO [2022-06-02 14:21:53,010] ({qtp686466458-26219} NotebookRestApi.java[configureParagraph]:1116) - Configure Paragraph for user Fipa
    >    INFO [2022-06-02 14:21:53,071] ({qtp686466458-26219} NotebookRestApi.java[initParagraph]:1105) - Init Paragraph for user Fipa
    >   ....

    #
    # So now the whole thing is locking up ....
    # Not sure about Zeppelin, but the test client hangs ..
    #


# -----------------------------------------------------
# Try 4 users run 4 times.
#[root@ansibler]

    loopcount=4
    usercount=4

    for i in $(seq 0 $((loopcount - 1)))
    do
        echo ""
        echo "-------------"
        echo "Loop [${i}]"
        testname="multi-user-$(printf "%02d" ${usercount})-$(printf "%02d" ${i})"

        /tmp/run-benchmark.py \
            "${endpoint:?}"  \
            "${testconfig:?}"  \
            "${testusers:?}" \
            "${usercount:?}" \
        | tee "/tmp/results/${testname:?}.txt"

        filter-results "${testname:?}"
    done

    #
    # Locked up, nothing running on the server.
    #


    One job is listed as ACCEPTED but not RUNNING in the Hadoop UI.

    >   [Thu Jun 02 15:31:12 +0000 2022]
    >       Application is added to the scheduler and is not yet activated.
    >       Queue's AM resource limit exceeded.
    >       Details :
    >           AM Partition = <DEFAULT_PARTITION>;
    >           AM Resource Request = <memory:3072, vCores:1>;
    >           Queue Resource Limit for AM = <memory:26624, vCores:1>;
    >           User AM Resource Limit of the queue = <memory:26624, vCores:1>;
    >           Queue AM Resource Usage = <memory:24576, vCores:8>;

    #
    # Notebooks run via the Zeppelin UI get stalled as soon as they try to run a Spark task.
    #


# -----------------------------------------------------
# Restart the Hadoop services.
#[root@ansibler]


    ssh master01

        stop-all.sh

    >   WARNING: Stopping all Apache Hadoop daemons as fedora in 10 seconds.
    >   WARNING: Use CTRL-C to abort.
    >   Stopping namenodes on [master01]
    >   Stopping datanodes
    >   Stopping secondary namenodes [iris-gaia-blue-20220602-master01]
    >   iris-gaia-blue-20220602-master01: fedora@iris-gaia-blue-20220602-master01: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
    >   Stopping nodemanagers
    >   worker06: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker02: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker01: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker05: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker03: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker04: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   Stopping resourcemanager


        start-all.sh

    >   WARNING: Attempting to start all Apache Hadoop daemons as fedora in 10 seconds.
    >   WARNING: This is not a recommended production deployment configuration.
    >   WARNING: Use CTRL-C to abort.
    >   Starting namenodes on [master01]
    >   Starting datanodes
    >   Starting secondary namenodes [iris-gaia-blue-20220602-master01]
    >   iris-gaia-blue-20220602-master01: fedora@iris-gaia-blue-20220602-master01: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
    >   Starting resourcemanager
    >   Starting nodemanagers


    #
    # Notebooks run via the Zeppelin UI work OK.
    #


    #
    # Benchmark tests fail with
    #


    >   ....
    >   ------------ Test Result: [ERROR] ------------
    >   [{'GaiaDMPSetup': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '10.29', 'expected': '45.00', 'percent': '-77.13', 'start': '2022-06-02T16:35:28.967009', 'finish': '2022-06-02T16:35:39.257402'}, 'logs': ''}, 'Mean_proper_motions_over_the_sky': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '14.95', 'expected': '55.00', 'percent': '-72.82', 'start': '2022-06-02T16:35:39.257542', 'finish': '2022-06-02T16:35:54.204476'}, 'logs': ''}, 'Source_counts_over_the_sky.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '21.66', 'expected': '22.00', 'percent': '-1.53', 'start': '2022-06-02T16:35:54.205012', 'finish': '2022-06-02T16:36:15.867640'}, 'logs': ''}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '13.98', 'expected': '60.00', 'percent': '-76.70', 'start': '2022-06-02T16:36:15.868025', 'finish': '2022-06-02T16:36:29.847677'}, 'logs': ''}}, {'GaiaDMPSetup': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '11.59', 'expected': '45.00', 'percent': '-74.23', 'start': '2022-06-02T16:35:28.968149', 'finish': '2022-06-02T16:35:40.563038'}, 'logs': "Py4JJavaError: An error occurred while calling o471.toLocalIterator.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:299)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:228)\norg.apache.zeppelin.spark.SparkScala212Interpreter.open(SparkScala212Interpreter.scala:88)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:121)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.Interpreter.getInterpreterInTheSameSessionByClassName(Interpreter.java:322)\norg.apache.zeppelin.interpreter.Interpreter.getInterpreterInTheSameSessionByClassName(Interpreter.java:333)\norg.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:90)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:833)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:741)\norg.apache.zeppelin.scheduler.Job.run(Job.java:172)\norg.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\norg.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2492)\n\tat org.apache.spark.sql.execution.LocalTableScanExec.rdd$lzycompute(LocalTableScanExec.scala:52)\n\tat org.apache.spark.sql.execution.LocalTableScanExec.rdd(LocalTableScanExec.scala:48)\n\tat org.apache.spark.sql.execution.LocalTableScanExec.doExecute(LocalTableScanExec.scala:59)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeToIterator(SparkPlan.scala:409)\n\tat org.apache.spark.sql.Dataset.$anonfun$toLocalIterator$1(Dataset.scala:2996)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.toLocalIterator(Dataset.scala:2994)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o471.toLocalIterator.\\n', JavaObject id=o472), <traceback object at 0x7f7868e6ca00>)"}, 'Mean_proper_motions_over_the_sky': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '9.77', 'expected': '55.00', 'percent': '-82.23', 'start': '2022-06-02T16:35:40.563176', 'finish': '2022-06-02T16:35:50.334953'}, 'logs': "Py4JJavaError: An error occurred while calling o481.cache.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:299)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:228)\norg.apache.zeppelin.spark.SparkScala212Interpreter.open(SparkScala212Interpreter.scala:88)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:121)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.Interpreter.getInterpreterInTheSameSessionByClassName(Interpreter.java:322)\norg.apache.zeppelin.interpreter.Interpreter.getInterpreterInTheSameSessionByClassName(Interpreter.java:333)\norg.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:90)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:833)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:741)\norg.apache.zeppelin.scheduler.Job.run(Job.java:172)\norg.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\norg.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:108)\n\tat org.apache.spark.sql.SparkSession.cloneSession(SparkSession.scala:272)\n\tat org.apache.spark.sql.SparkSession$.getOrCloneSessionWithConfigsOff(SparkSession.scala:1079)\n\tat org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:96)\n\tat org.apache.spark.sql.Dataset.persist(Dataset.scala:3165)\n\tat org.apache.spark.sql.Dataset.cache(Dataset.scala:3175)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o481.cache.\\n', JavaObject id=o482), <traceback object at 0x7f7868e3e050>)"}, 'Source_counts_over_the_sky.json': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '11.07', 'expected': '22.00', 'percent': '-49.68', 'start': '2022-06-02T16:35:50.335146', 'finish': '2022-06-02T16:36:01.405690'}, 'logs': "Py4JJavaError: An error occurred while calling o487.javaToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(FLOOR((cast(source_id#47L as double) / 1.40737488355328E14))#279L, 200), ENSURE_REQUIREMENTS, [id=#181]\n+- *(1) HashAggregate(keys=[FLOOR((cast(source_id#47L as double) / 1.40737488355328E14)) AS FLOOR((cast(source_id#47L as double) / 1.40737488355328E14))#279L], functions=[partial_count(1)], output=[FLOOR((cast(source_id#47L as double) / 1.40737488355328E14))#279L, count#281L])\n   +- *(1) ColumnarToRow\n      +- FileScan parquet gaiaedr3.gaia_source[source_id#47L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/data/gaia/GEDR3/GEDR3_GAIASOURCE], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<source_id:bigint>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:3510)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:299)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:228)\norg.apache.zeppelin.spark.SparkScala212Interpreter.open(SparkScala212Interpreter.scala:88)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:121)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.Interpreter.getInterpreterInTheSameSessionByClassName(Interpreter.java:322)\norg.apache.zeppelin.interpreter.Interpreter.getInterpreterInTheSameSessionByClassName(Interpreter.java:333)\norg.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:90)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:833)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:741)\norg.apache.zeppelin.scheduler.Job.run(Job.java:172)\norg.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\norg.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1506)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:231)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:497)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)\n\tat org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:202)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 31 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o487.javaToPython.\\n', JavaObject id=o492), <traceback object at 0x7f7868e607d0>)"}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '15.91', 'expected': '60.00', 'percent': '-73.48', 'start': '2022-06-02T16:36:01.405845', 'finish': '2022-06-02T16:36:17.316469'}, 'logs': ''}}, {'GaiaDMPSetup': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '12.21', 'expected': '45.00', 'percent': '-72.87', 'start': '2022-06-02T16:35:28.968954', 'finish': '2022-06-02T16:35:41.175695'}, 'logs': "Py4JJavaError: An error occurred while calling o538.toLocalIterator.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:299)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:228)\norg.apache.zeppelin.spark.SparkScala212Interpreter.open(SparkScala212Interpreter.scala:88)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:121)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.Interpreter.getInterpreterInTheSameSessionByClassName(Interpreter.java:322)\norg.apache.zeppelin.interpreter.Interpreter.getInterpreterInTheSameSessionByClassName(Interpreter.java:333)\norg.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:90)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:833)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:741)\norg.apache.zeppelin.scheduler.Job.run(Job.java:172)\norg.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\norg.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2492)\n\tat org.apache.spark.sql.execution.LocalTableScanExec.rdd$lzycompute(LocalTableScanExec.scala:52)\n\tat org.apache.spark.sql.execution.LocalTableScanExec.rdd(LocalTableScanExec.scala:48)\n\tat org.apache.spark.sql.execution.LocalTableScanExec.doExecute(LocalTableScanExec.scala:59)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeToIterator(SparkPlan.scala:409)\n\tat org.apache.spark.sql.Dataset.$anonfun$toLocalIterator$1(Dataset.scala:2996)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.toLocalIterator(Dataset.scala:2994)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o538.toLocalIterator.\\n', JavaObject id=o539), <traceback object at 0x7f06c3de0370>)"}, 'Mean_proper_motions_over_the_sky': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '10.08', 'expected': '55.00', 'percent': '-81.67', 'start': '2022-06-02T16:35:41.175941', 'finish': '2022-06-02T16:35:51.255030'}, 'logs': "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2188)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n', JavaObject id=o556), <traceback object at 0x7f06c3e59410>)"}, 'Source_counts_over_the_sky.json': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '13.71', 'expected': '22.00', 'percent': '-37.67', 'start': '2022-06-02T16:35:51.255301', 'finish': '2022-06-02T16:36:04.966840'}, 'logs': "Py4JJavaError: An error occurred while calling o561.javaToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(FLOOR((cast(source_id#39L as double) / 1.40737488355328E14))#833L, 200), ENSURE_REQUIREMENTS, [id=#281]\n+- *(1) HashAggregate(keys=[FLOOR((cast(source_id#39L as double) / 1.40737488355328E14)) AS FLOOR((cast(source_id#39L as double) / 1.40737488355328E14))#833L], functions=[partial_count(1)], output=[FLOOR((cast(source_id#39L as double) / 1.40737488355328E14))#833L, count#835L])\n   +- *(1) ColumnarToRow\n      +- FileScan parquet gaiaedr3.gaia_source[source_id#39L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/data/gaia/GEDR3/GEDR3_GAIASOURCE], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<source_id:bigint>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:3510)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:299)\norg.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:228)\norg.apache.zeppelin.spark.SparkScala212Interpreter.open(SparkScala212Interpreter.scala:88)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:121)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.Interpreter.getInterpreterInTheSameSessionByClassName(Interpreter.java:322)\norg.apache.zeppelin.interpreter.Interpreter.getInterpreterInTheSameSessionByClassName(Interpreter.java:333)\norg.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:90)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:833)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:741)\norg.apache.zeppelin.scheduler.Job.run(Job.java:172)\norg.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\norg.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1506)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:231)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:497)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)\n\tat org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:202)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 31 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o561.javaToPython.\\n', JavaObject id=o566), <traceback object at 0x7f06c3e34f00>)"}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '13.08', 'expected': '60.00', 'percent': '-78.20', 'start': '2022-06-02T16:36:04.967016', 'finish': '2022-06-02T16:36:18.049045'}, 'logs': ''}}, {'GaiaDMPSetup': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '8.67', 'expected': '45.00', 'percent': '-80.73', 'start': '2022-06-02T16:35:28.969251', 'finish': '2022-06-02T16:35:37.640126'}, 'logs': ''}, 'Mean_proper_motions_over_the_sky': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '12.93', 'expected': '55.00', 'percent': '-76.48', 'start': '2022-06-02T16:35:37.640288', 'finish': '2022-06-02T16:35:50.574353'}, 'logs': ''}, 'Source_counts_over_the_sky.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '21.99', 'expected': '22.00', 'percent': '-0.03', 'start': '2022-06-02T16:35:50.574665', 'finish': '2022-06-02T16:36:12.567678'}, 'logs': ''}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '14.47', 'expected': '60.00', 'percent': '-75.89', 'start': '2022-06-02T16:36:12.568058', 'finish': '2022-06-02T16:36:27.036493'}, 'logs': ''}}]
    >   ....

    #
    # restart Zeppelin
    #

    #
    # Zeppelin user interface tasks seem to work OK.
    #

    #
    # Benchmark tests fail with different errors ..
    #

    >   ....
    >   ------------ Test Result: [ERROR] ------------
    >   [{'GaiaDMPSetup': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '46.18', 'expected': '45.00', 'percent': '2.63', 'start': '2022-06-02T16:39:07.998712', 'finish': '2022-06-02T16:39:54.182142'}, 'logs': ''}, 'Mean_proper_motions_over_the_sky': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '43.55', 'expected': '55.00', 'percent': '-20.83', 'start': '2022-06-02T16:39:54.182441', 'finish': '2022-06-02T16:40:37.727502'}, 'logs': ''}, 'Source_counts_over_the_sky.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '20.79', 'expected': '22.00', 'percent': '-5.51', 'start': '2022-06-02T16:40:37.727799', 'finish': '2022-06-02T16:40:58.515407'}, 'logs': ''}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '15.63', 'expected': '60.00', 'percent': '-73.96', 'start': '2022-06-02T16:40:58.516020', 'finish': '2022-06-02T16:41:14.142263'}, 'logs': ''}}, {'GaiaDMPSetup': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '60.86', 'expected': '45.00', 'percent': '35.24', 'start': '2022-06-02T16:39:07.998887', 'finish': '2022-06-02T16:40:08.855578'}, 'logs': ''}, 'Mean_proper_motions_over_the_sky': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '40.98', 'expected': '55.00', 'percent': '-25.48', 'start': '2022-06-02T16:40:08.855781', 'finish': '2022-06-02T16:40:49.840709'}, 'logs': ''}, 'Source_counts_over_the_sky.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '23.11', 'expected': '22.00', 'percent': '5.04', 'start': '2022-06-02T16:40:49.841887', 'finish': '2022-06-02T16:41:12.950354'}, 'logs': ''}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '16.19', 'expected': '60.00', 'percent': '-73.01', 'start': '2022-06-02T16:41:12.951192', 'finish': '2022-06-02T16:41:29.143872'}, 'logs': ''}}, {'GaiaDMPSetup': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '11.47', 'expected': '45.00', 'percent': '-74.50', 'start': '2022-06-02T16:39:07.999016', 'finish': '2022-06-02T16:39:19.472115'}, 'logs': 'Unexpected exception: java.util.ConcurrentModificationException\n\tat java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1633)\n\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n\tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)\n\tat org.apache.zeppelin.service.JobManagerService.getNoteJobInfoByUnixTime(JobManagerService.java:90)\n\tat org.apache.zeppelin.socket.NotebookServer.broadcastUpdateNoteJobInfo(NotebookServer.java:519)\n\tat org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:2007)\n\tat org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:105)\n\tat org.apache.zeppelin.scheduler.Job.setStatus(Job.java:141)\n\tat org.apache.zeppelin.notebook.Paragraph.setStatus(Paragraph.java:398)\n\tat org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:349)\n\tat org.apache.zeppelin.notebook.Note.run(Note.java:873)\n\tat org.apache.zeppelin.service.NotebookService.runParagraph(NotebookService.java:390)\n\tat org.apache.zeppelin.rest.NotebookRestApi.runParagraph(NotebookRestApi.java:849)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)\n\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:469)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:391)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:80)\n\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:253)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\n\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\n\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)\n\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)\n\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\n\tat org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:763)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1651)\n\tat org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)\n\tat org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)\n\tat org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)\n\tat org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)\n\tat org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)\n\tat org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)\n\tat org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)\n\tat org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)\n\tat org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)\n\tat org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:450)\n\tat org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)\n\tat org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)\n\tat org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)\n\tat org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:387)\n\tat org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)\n\tat org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)\n\tat org.apache.zeppelin.server.CorsFilter.doFilter(CorsFilter.java:64)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:567)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1610)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1377)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:507)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1580)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1292)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n\tat io.micrometer.core.instrument.binder.jetty.TimedHandler.handle(TimedHandler.java:120)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:501)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:556)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)\n\tat java.lang.Thread.run(Thread.java:748)'}, 'Mean_proper_motions_over_the_sky': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '55.94', 'expected': '55.00', 'percent': '1.71', 'start': '2022-06-02T16:39:19.472382', 'finish': '2022-06-02T16:40:15.410366'}, 'logs': 'Fail to execute line 13: df = spark.sql(query).cache()\nTraceback (most recent call last):\n  File "/tmp/1654188010798-0/zeppelin_python.py", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File "<stdin>", line 13, in <module>\n  File "/opt/spark/python/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 121;\n\'Aggregate [\'hpx_id], [\'floor((\'source_id / 140737488355328)) AS hpx_id#0, count(1) AS n#1L, \'AVG(\'pmra) AS avg_pmra#2, \'AVG(\'pmdec) AS avg_pmdec#3]\n+- \'UnresolvedRelation [gaia_source], [], false'}, 'Source_counts_over_the_sky.json': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '11.41', 'expected': '22.00', 'percent': '-48.14', 'start': '2022-06-02T16:40:15.410599', 'finish': '2022-06-02T16:40:26.819835'}, 'logs': 'Fail to execute line 21: df = spark.sql("SELECT FLOOR(source_id / %d"%(divisor) + ") AS hpx_id, COUNT(*) AS n FROM gaia_source GROUP BY hpx_id")\nTraceback (most recent call last):\n  File "/tmp/1654188010798-0/zeppelin_python.py", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File "<stdin>", line 21, in <module>\n  File "/opt/spark/python/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 72;\n\'Aggregate [\'hpx_id], [\'FLOOR((\'source_id / 140737488355328)) AS hpx_id#5, count(1) AS n#6L]\n+- \'UnresolvedRelation [gaia_source], [], false'}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '19.64', 'expected': '60.00', 'percent': '-67.26', 'start': '2022-06-02T16:40:26.820121', 'finish': '2022-06-02T16:40:46.462633'}, 'logs': ''}}, {'GaiaDMPSetup': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '11.48', 'expected': '45.00', 'percent': '-74.50', 'start': '2022-06-02T16:39:07.999045', 'finish': '2022-06-02T16:39:19.475757'}, 'logs': 'Unexpected exception: java.util.ConcurrentModificationException\n\tat java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1633)\n\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n\tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)\n\tat org.apache.zeppelin.service.JobManagerService.getNoteJobInfoByUnixTime(JobManagerService.java:90)\n\tat org.apache.zeppelin.socket.NotebookServer.broadcastUpdateNoteJobInfo(NotebookServer.java:519)\n\tat org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:2007)\n\tat org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:105)\n\tat org.apache.zeppelin.scheduler.Job.setStatus(Job.java:141)\n\tat org.apache.zeppelin.notebook.Paragraph.setStatus(Paragraph.java:398)\n\tat org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:349)\n\tat org.apache.zeppelin.notebook.Note.run(Note.java:873)\n\tat org.apache.zeppelin.service.NotebookService.runParagraph(NotebookService.java:390)\n\tat org.apache.zeppelin.rest.NotebookRestApi.runParagraph(NotebookRestApi.java:849)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)\n\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:469)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:391)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:80)\n\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:253)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\n\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\n\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)\n\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)\n\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\n\tat org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:763)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1651)\n\tat org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)\n\tat org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)\n\tat org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)\n\tat org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)\n\tat org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)\n\tat org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)\n\tat org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)\n\tat org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)\n\tat org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)\n\tat org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:450)\n\tat org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)\n\tat org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)\n\tat org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)\n\tat org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:387)\n\tat org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)\n\tat org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)\n\tat org.apache.zeppelin.server.CorsFilter.doFilter(CorsFilter.java:64)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:567)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1610)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1377)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:507)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1580)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1292)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n\tat io.micrometer.core.instrument.binder.jetty.TimedHandler.handle(TimedHandler.java:120)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:501)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:556)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)\n\tat java.lang.Thread.run(Thread.java:748)'}, 'Mean_proper_motions_over_the_sky': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '53.77', 'expected': '55.00', 'percent': '-2.24', 'start': '2022-06-02T16:39:19.475890', 'finish': '2022-06-02T16:40:13.241193'}, 'logs': 'Fail to execute line 13: df = spark.sql(query).cache()\nTraceback (most recent call last):\n  File "/tmp/1654188008780-0/zeppelin_python.py", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File "<stdin>", line 13, in <module>\n  File "/opt/spark/python/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 121;\n\'Aggregate [\'hpx_id], [\'floor((\'source_id / 140737488355328)) AS hpx_id#0, count(1) AS n#1L, \'AVG(\'pmra) AS avg_pmra#2, \'AVG(\'pmdec) AS avg_pmdec#3]\n+- \'UnresolvedRelation [gaia_source], [], false'}, 'Source_counts_over_the_sky.json': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '15.53', 'expected': '22.00', 'percent': '-29.40', 'start': '2022-06-02T16:40:13.241490', 'finish': '2022-06-02T16:40:28.773163'}, 'logs': 'Fail to execute line 21: df = spark.sql("SELECT FLOOR(source_id / %d"%(divisor) + ") AS hpx_id, COUNT(*) AS n FROM gaia_source GROUP BY hpx_id")\nTraceback (most recent call last):\n  File "/tmp/1654188008780-0/zeppelin_python.py", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File "<stdin>", line 21, in <module>\n  File "/opt/spark/python/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 72;\n\'Aggregate [\'hpx_id], [\'FLOOR((\'source_id / 140737488355328)) AS hpx_id#5, count(1) AS n#6L]\n+- \'UnresolvedRelation [gaia_source], [], false'}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '18.72', 'expected': '60.00', 'percent': '-68.80', 'start': '2022-06-02T16:40:28.773333', 'finish': '2022-06-02T16:40:47.495359'}, 'logs': ''}}]
    >   ....


    >   ....
    >   ------------ Test Result: [ERROR] ------------
    >   [{'GaiaDMPSetup': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '12.54', 'expected': '45.00', 'percent': '-72.12', 'start': '2022-06-02T17:01:13.039052', 'finish': '2022-06-02T17:01:25.583576'}, 'logs': ''}, 'Mean_proper_motions_over_the_sky': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '14.17', 'expected': '55.00', 'percent': '-74.24', 'start': '2022-06-02T17:01:25.583700', 'finish': '2022-06-02T17:01:39.749606'}, 'logs': ''}, 'Source_counts_over_the_sky.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '21.24', 'expected': '22.00', 'percent': '-3.46', 'start': '2022-06-02T17:01:39.749986', 'finish': '2022-06-02T17:02:00.988141'}, 'logs': ''}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '13.74', 'expected': '60.00', 'percent': '-77.10', 'start': '2022-06-02T17:02:00.988895', 'finish': '2022-06-02T17:02:14.730454'}, 'logs': ''}}, {'GaiaDMPSetup': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '12.43', 'expected': '45.00', 'percent': '-72.37', 'start': '2022-06-02T17:01:13.040544', 'finish': '2022-06-02T17:01:25.472640'}, 'logs': ''}, 'Mean_proper_motions_over_the_sky': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '12.40', 'expected': '55.00', 'percent': '-77.46', 'start': '2022-06-02T17:01:25.472924', 'finish': '2022-06-02T17:01:37.869753'}, 'logs': ''}, 'Source_counts_over_the_sky.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '30.66', 'expected': '22.00', 'percent': '39.35', 'start': '2022-06-02T17:01:37.870181', 'finish': '2022-06-02T17:02:08.528274'}, 'logs': ''}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '14.72', 'expected': '60.00', 'percent': '-75.47', 'start': '2022-06-02T17:02:08.528688', 'finish': '2022-06-02T17:02:23.246514'}, 'logs': ''}}, {'GaiaDMPSetup': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '9.89', 'expected': '45.00', 'percent': '-78.03', 'start': '2022-06-02T17:01:13.041417', 'finish': '2022-06-02T17:01:22.928968'}, 'logs': ''}, 'Mean_proper_motions_over_the_sky': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '12.16', 'expected': '55.00', 'percent': '-77.88', 'start': '2022-06-02T17:01:22.929136', 'finish': '2022-06-02T17:01:35.093425'}, 'logs': ''}, 'Source_counts_over_the_sky.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '34.88', 'expected': '22.00', 'percent': '58.53', 'start': '2022-06-02T17:01:35.093743', 'finish': '2022-06-02T17:02:09.970958'}, 'logs': ''}, 'Library_Validation.json': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '9.38', 'expected': '60.00', 'percent': '-84.37', 'start': '2022-06-02T17:02:09.971602', 'finish': '2022-06-02T17:02:19.347475'}, 'logs': 'Unexpected exception: java.util.ConcurrentModificationException\n\tat java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1633)\n\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n\tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)\n\tat org.apache.zeppelin.service.JobManagerService.getNoteJobInfoByUnixTime(JobManagerService.java:90)\n\tat org.apache.zeppelin.socket.NotebookServer.broadcastUpdateNoteJobInfo(NotebookServer.java:519)\n\tat org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:2007)\n\tat org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:105)\n\tat org.apache.zeppelin.scheduler.Job.setStatus(Job.java:141)\n\tat org.apache.zeppelin.notebook.Paragraph.setStatus(Paragraph.java:398)\n\tat org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:349)\n\tat org.apache.zeppelin.notebook.Note.run(Note.java:873)\n\tat org.apache.zeppelin.service.NotebookService.runParagraph(NotebookService.java:390)\n\tat org.apache.zeppelin.rest.NotebookRestApi.runParagraph(NotebookRestApi.java:849)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)\n\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:469)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:391)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:80)\n\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:253)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\n\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\n\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)\n\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)\n\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\n\tat org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:763)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1651)\n\tat org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)\n\tat org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)\n\tat org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)\n\tat org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)\n\tat org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)\n\tat org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)\n\tat org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)\n\tat org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)\n\tat org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)\n\tat org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:450)\n\tat org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)\n\tat org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)\n\tat org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)\n\tat org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:387)\n\tat org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)\n\tat org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)\n\tat org.apache.zeppelin.server.CorsFilter.doFilter(CorsFilter.java:64)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:567)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1610)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1377)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:507)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1580)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1292)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n\tat io.micrometer.core.instrument.binder.jetty.TimedHandler.handle(TimedHandler.java:120)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:501)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:556)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)\n\tat java.lang.Thread.run(Thread.java:748)'}}, {'GaiaDMPSetup': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '13.63', 'expected': '45.00', 'percent': '-69.71', 'start': '2022-06-02T17:01:13.041688', 'finish': '2022-06-02T17:01:26.672366'}, 'logs': ''}, 'Mean_proper_motions_over_the_sky': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '13.92', 'expected': '55.00', 'percent': '-74.69', 'start': '2022-06-02T17:01:26.672652', 'finish': '2022-06-02T17:01:40.593446'}, 'logs': ''}, 'Source_counts_over_the_sky.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '34.83', 'expected': '22.00', 'percent': '58.30', 'start': '2022-06-02T17:01:40.593777', 'finish': '2022-06-02T17:02:15.420529'}, 'logs': ''}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '11.82', 'expected': '60.00', 'percent': '-80.30', 'start': '2022-06-02T17:02:15.421003', 'finish': '2022-06-02T17:02:27.240596'}, 'logs': ''}}]
    >   ....

    #
    # Restarting Zeppelin and Hadoop doesn't make everything better.
    #

    #
    # Rebooting the machines ?
    #

# -----------------------------------------------------
# Reboot the machines ...
#[root@ansibler]

    ssh worker01 \
        '
        sudo reboot
        '

    ssh worker02 \
        '
        sudo reboot
        '

    ssh worker03 \
        '
        sudo reboot
        '

    ssh worker04 \
        '
        sudo reboot
        '

    ssh worker05 \
        '
        sudo reboot
        '

    ssh worker06 \
        '
        sudo reboot
        '

    ssh master01 \
        '
        sudo reboot
        '

    ssh zeppelin \
        '
        sudo reboot
        '

    ssh master01 \
        '
        start-dfs.sh
        start-yarn.sh
        '

    ssh zeppelin \
        '
        zeppelin-daemon.sh start
        '

    #
    # Still failing ...
    #

    >   ....
    >   Test completed! (196.54 seconds)
    >   ------------ Test Result: [FAIL] ------------
    >   [{'GaiaDMPSetup': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '15.75', 'expected': '45.00', 'percent': '-65.01', 'start': '2022-06-02T17:15:07.611831', 'finish': '2022-06-02T17:15:23.357102'}, 'logs': 'Unexpected exception: java.util.ConcurrentModificationException\n\tat java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1633)\n\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n\tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)\n\tat org.apache.zeppelin.service.JobManagerService.getNoteJobInfoByUnixTime(JobManagerService.java:90)\n\tat org.apache.zeppelin.socket.NotebookServer.broadcastUpdateNoteJobInfo(NotebookServer.java:519)\n\tat org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:2007)\n\tat org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:105)\n\tat org.apache.zeppelin.scheduler.Job.setStatus(Job.java:141)\n\tat org.apache.zeppelin.notebook.Paragraph.setStatus(Paragraph.java:398)\n\tat org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:349)\n\tat org.apache.zeppelin.notebook.Note.run(Note.java:873)\n\tat org.apache.zeppelin.service.NotebookService.runParagraph(NotebookService.java:390)\n\tat org.apache.zeppelin.rest.NotebookRestApi.runParagraph(NotebookRestApi.java:849)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)\n\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:469)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:391)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:80)\n\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:253)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\n\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\n\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)\n\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)\n\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\n\tat org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:763)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1651)\n\tat org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)\n\tat org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)\n\tat org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)\n\tat org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)\n\tat org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)\n\tat org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)\n\tat org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)\n\tat org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)\n\tat org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)\n\tat org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:450)\n\tat org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)\n\tat org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)\n\tat org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)\n\tat org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:387)\n\tat org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)\n\tat org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)\n\tat org.apache.zeppelin.server.CorsFilter.doFilter(CorsFilter.java:64)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1638)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:567)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:602)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1610)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1377)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:507)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1580)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1292)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n\tat io.micrometer.core.instrument.binder.jetty.TimedHandler.handle(TimedHandler.java:120)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:501)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:556)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:273)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)\n\tat java.lang.Thread.run(Thread.java:748)'}, 'Mean_proper_motions_over_the_sky': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '55.17', 'expected': '55.00', 'percent': '0.30', 'start': '2022-06-02T17:15:23.357388', 'finish': '2022-06-02T17:16:18.524291'}, 'logs': 'Fail to execute line 13: df = spark.sql(query).cache()\nTraceback (most recent call last):\n  File "/tmp/1654190174607-0/zeppelin_python.py", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File "<stdin>", line 13, in <module>\n  File "/opt/spark/python/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 121;\n\'Aggregate [\'hpx_id], [\'floor((\'source_id / 140737488355328)) AS hpx_id#0, count(1) AS n#1L, \'AVG(\'pmra) AS avg_pmra#2, \'AVG(\'pmdec) AS avg_pmdec#3]\n+- \'UnresolvedRelation [gaia_source], [], false'}, 'Source_counts_over_the_sky.json': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '13.97', 'expected': '22.00', 'percent': '-36.51', 'start': '2022-06-02T17:16:18.524520', 'finish': '2022-06-02T17:16:32.491473'}, 'logs': 'Fail to execute line 21: df = spark.sql("SELECT FLOOR(source_id / %d"%(divisor) + ") AS hpx_id, COUNT(*) AS n FROM gaia_source GROUP BY hpx_id")\nTraceback (most recent call last):\n  File "/tmp/1654190174607-0/zeppelin_python.py", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File "<stdin>", line 21, in <module>\n  File "/opt/spark/python/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 72;\n\'Aggregate [\'hpx_id], [\'FLOOR((\'source_id / 140737488355328)) AS hpx_id#5, count(1) AS n#6L]\n+- \'UnresolvedRelation [gaia_source], [], false'}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '19.35', 'expected': '60.00', 'percent': '-67.75', 'start': '2022-06-02T17:16:32.491733', 'finish': '2022-06-02T17:16:51.842261'}, 'logs': ''}}, {'GaiaDMPSetup': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '53.49', 'expected': '45.00', 'percent': '18.87', 'start': '2022-06-02T17:15:07.612010', 'finish': '2022-06-02T17:16:01.102911'}, 'logs': ''}, 'Mean_proper_motions_over_the_sky': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '93.22', 'expected': '55.00', 'percent': '69.50', 'start': '2022-06-02T17:16:01.103126', 'finish': '2022-06-02T17:17:34.325558'}, 'logs': ''}, 'Source_counts_over_the_sky.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '32.61', 'expected': '22.00', 'percent': '48.21', 'start': '2022-06-02T17:17:34.325949', 'finish': '2022-06-02T17:18:06.931299'}, 'logs': ''}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '17.20', 'expected': '60.00', 'percent': '-71.34', 'start': '2022-06-02T17:18:06.931769', 'finish': '2022-06-02T17:18:24.127882'}, 'logs': ''}}, {'GaiaDMPSetup': {'result': 'FAIL', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '8.72', 'expected': '45.00', 'percent': '-80.62', 'start': '2022-06-02T17:15:07.612168', 'finish': '2022-06-02T17:15:16.333407'}, 'logs': ''}, 'Mean_proper_motions_over_the_sky': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '54.87', 'expected': '55.00', 'percent': '-0.24', 'start': '2022-06-02T17:15:16.333582', 'finish': '2022-06-02T17:16:11.199682'}, 'logs': 'Fail to execute line 13: df = spark.sql(query).cache()\nTraceback (most recent call last):\n  File "/tmp/1654190167519-0/zeppelin_python.py", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File "<stdin>", line 13, in <module>\n  File "/opt/spark/python/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 121;\n\'Aggregate [\'hpx_id], [\'floor((\'source_id / 140737488355328)) AS hpx_id#0, count(1) AS n#1L, \'AVG(\'pmra) AS avg_pmra#2, \'AVG(\'pmdec) AS avg_pmdec#3]\n+- \'UnresolvedRelation [gaia_source], [], false'}, 'Source_counts_over_the_sky.json': {'result': 'ERROR', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '12.28', 'expected': '22.00', 'percent': '-44.17', 'start': '2022-06-02T17:16:11.199863', 'finish': '2022-06-02T17:16:23.482303'}, 'logs': 'Fail to execute line 21: df = spark.sql("SELECT FLOOR(source_id / %d"%(divisor) + ") AS hpx_id, COUNT(*) AS n FROM gaia_source GROUP BY hpx_id")\nTraceback (most recent call last):\n  File "/tmp/1654190167519-0/zeppelin_python.py", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File "<stdin>", line 21, in <module>\n  File "/opt/spark/python/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 72;\n\'Aggregate [\'hpx_id], [\'FLOOR((\'source_id / 140737488355328)) AS hpx_id#5, count(1) AS n#6L]\n+- \'UnresolvedRelation [gaia_source], [], false'}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '18.86', 'expected': '60.00', 'percent': '-68.57', 'start': '2022-06-02T17:16:23.482567', 'finish': '2022-06-02T17:16:42.343461'}, 'logs': ''}}, {'GaiaDMPSetup': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '59.78', 'expected': '45.00', 'percent': '32.85', 'start': '2022-06-02T17:15:07.612183', 'finish': '2022-06-02T17:16:07.392690'}, 'logs': ''}, 'Mean_proper_motions_over_the_sky': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '87.92', 'expected': '55.00', 'percent': '59.85', 'start': '2022-06-02T17:16:07.392815', 'finish': '2022-06-02T17:17:35.310943'}, 'logs': ''}, 'Source_counts_over_the_sky.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'SLOW', 'elapsed': '31.44', 'expected': '22.00', 'percent': '42.91', 'start': '2022-06-02T17:17:35.311292', 'finish': '2022-06-02T17:18:06.751329'}, 'logs': ''}, 'Library_Validation.json': {'result': 'PASS', 'outputs': {'valid': True}, 'time': {'result': 'FAST', 'elapsed': '16.68', 'expected': '60.00', 'percent': '-72.20', 'start': '2022-06-02T17:18:06.751983', 'finish': '2022-06-02T17:18:23.429412'}, 'logs': ''}}]
    >   ....


    Looks like this might be it ...
    https://issues.apache.org/jira/browse/ZEPPELIN-5237




    #
    # Left it to settle for several hours.
    # Tried starting again.
    # Fails to create notebooks.
    #
    # Several different things going wrong.
    # None of them to do with Spark scheduler.
    #

    >   ....
    >   ERROR [2022-06-03 04:33:24,748] ({qtp2128029086-18426} WebApplicationExceptionMapper.java[toResponse]:49) - Error response
    >   java.lang.OutOfMemoryError: Java heap space
    >   ....


    >   ....
    >    INFO [2022-06-03 04:35:26,535] ({qtp2128029086-18468} LoginRestApi.java[postLogin]:249) - {"status":"OK","message":"","body":{"principal":"Balline","ticket":"db09e8f1-0b33-4041-a80a-fcc8275a0bdc","roles":"[\"user\"]"}}
    >    INFO [2022-06-03 04:35:27,854] ({qtp2128029086-18510} NotebookRestApi.java[createNote]:385) - Creating new note by JSON {"paragraphs": [{"text": "%pyspark\n\n# Check Numpy\n\nimport numpy\nassert numpy.__version__ == \"1.20.3\" ", "user": "gaiauser", "dateUpdated": "2022-03-16T18:34:45+0000", "progress": 0, "config": {"editorSetting": {"language": "python", "editOnDblClick": false, "completionKey": "TAB", "completionSupport": true}, "colWidth": 12, "editorMode": "ace/mode/python", "fontSize": 9, "results": {}, "enabled": true}, "settings": {"params": {}, "forms": {}}, "apps": [], "runtimeInfos": {}, "progressUpdateIntervalMs": 5
    >    eCreated": "2022-03-16T18:35:53+0000", "dateStarted": "2022-03-17T14:55:20+0000", "dateFinished": "2022-03-17T14:55:20+0000", "status": "FINISHED", "$$hashKey": "object:2633"}], "name": "/tmp/8H6C2GHHFY.json", "id": "2GZ96Z759", "defaultInterpreterGroup": "spark", "version": "0.10.0", "noteParams": {}, "noteForms": {}, "angularObjects": {}, "config": {"isZeppelinNotebookCronEnable": false, "looknfeel": "default", "personalizedMode": "false"}, "info": {}, "path": "/tmp/libraries.json"}
    >    INFO [2022-06-03 04:35:29,631] ({qtp2128029086-18510} LocalConfigStorage.java[save]:70) - Save notebook authorization to file: /home/fedora/zeppelin/conf/notebook-authorization.json
    >   ERROR [2022-06-03 04:36:20,708] ({qtp2128029086-18510} WebApplicationExceptionMapper.java[toResponse]:49) - Error response
    >   java.lang.OutOfMemoryError: Java heap space
    >    INFO [2022-06-03 04:36:25,419] ({qtp2128029086-18498} LoginRestApi.java[postLogin]:249) - {"status":"OK","message":"","body":{"principal":"Fipa","ticket":"cf949dcd-2fef-48d8-9d17-c1d46d34b925","roles":"[\"user\"]"}}
    >   ....



    >   ....
    >    INFO [2022-06-03 04:44:58,084] ({qtp2128029086-18884} LocalConfigStorage.java[save]:70) - Save notebook authorization to file: /home/fedora/zeppelin/conf/notebook-authorization.json
    >   ERROR [2022-06-03 04:45:27,485] ({qtp2128029086-18877} WebApplicationExceptionMapper.java[toResponse]:49) - Error response
    >   java.util.ConcurrentModificationException
    >   	at java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1704)
    >   	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    >   	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    >   	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    >   	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    >   	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    >   	at org.apache.zeppelin.notebook.Notebook.getNotesInfo(Notebook.java:662)
    >   	at org.apache.zeppelin.service.NotebookService.listNotesInfo(NotebookService.java:245)
    >   	at org.apache.zeppelin.rest.NotebookRestApi.getNoteList(NotebookRestApi.java:318)
    >   	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
    >   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >   	at java.lang.reflect.Method.invoke(Method.java:498)
    >   ....

    This might help ..
    https://stackoverflow.com/a/67390602


    Enabling HDFS Storage for Zeppelin Notebooks
    https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.3/bk_zeppelin-component-guide/content/ch_zeppelin_upgrade_hdfs_storage.html

    We are currently using GitNotebookRepo
        <name>zeppelin.notebook.storage</name>
        <value>org.apache.zeppelin.notebook.repo.GitNotebookRepo</value>


    Separate the notebook storage from Zeppelin ..
    https://zeppelin.apache.org/docs/0.10.0/setup/storage/storage.html#notebook-storage-in-mongodb

---------------------------------------------------------------------------------------

    All of the tests fail during create-note .

    Try a simple test

    loopcount=1
    usercount=1

    >   ....
    >    INFO [2022-06-03 05:21:59,892] ({qtp2128029086-19579} LocalConfigStorage.java[save]:70) - Save notebook authorization to file: /home/fedora/zeppelin/conf/notebook-authorization.json
    >   ERROR [2022-06-03 05:22:00,491] ({qtp2128029086-19579} WebApplicationExceptionMapper.java[toResponse]:49) - Error response
    >   java.lang.OutOfMemoryError: Java heap space
    >   ....

    Restart Zeppelin

    ssh zeppelin \
        '
        zeppelin-daemon.sh restart
        '

    >   ....
    >    INFO [2022-06-03 05:24:05,084] ({main} ZeppelinLocationStrategy.java[locate]:44) - Load configuration from /home/fedora/zeppelin/conf/zeppelin-site.xml
    >    INFO [2022-06-03 05:24:05,088] ({main} ZeppelinLocationStrategy.java[locate]:44) - Load configuration from /home/fedora/zeppelin/conf/zeppelin-site.xml
    >    INFO [2022-06-03 05:24:05,146] ({main} ZeppelinConfiguration.java[create]:135) - Server Host: 0.0.0.0
    >    INFO [2022-06-03 05:24:05,146] ({main} ZeppelinConfiguration.java[create]:139) - Server Port: 8080
    >    INFO [2022-06-03 05:24:05,158] ({main} ZeppelinConfiguration.java[create]:141) - Context Path: /
    >    INFO [2022-06-03 05:24:05,158] ({main} ZeppelinConfiguration.java[create]:142) - Zeppelin Version: 0.10.0
    >    INFO [2022-06-03 05:24:05,171] ({main} Log.java[initialized]:169) - Logging initialized @614ms to org.eclipse.jetty.util.log.Slf4jLog
    >    WARN [2022-06-03 05:24:05,426] ({main} ZeppelinConfiguration.java[getConfigFSDir]:653) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
    >    WARN [2022-06-03 05:24:05,430] ({main} ZeppelinConfiguration.java[getConfigFSDir]:653) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
    >    WARN [2022-06-03 05:24:05,430] ({main} ZeppelinConfiguration.java[getConfigFSDir]:653) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir
    >    WARN [2022-06-03 05:24:05,466] ({main} LocalConfigStorage.java[loadCredentials]:88) - Credential file /home/fedora/zeppelin/conf/credentials.json is not existed
    >    INFO [2022-06-03 05:24:05,505] ({ImmediateThread-1654233845421} PluginManager.java[loadNotebookRepo]:78) - Loading NotebookRepo Plugin: org.apache.zeppelin.notebook.repo.GitNotebookRepo
    >    INFO [2022-06-03 05:24:05,579] ({ImmediateThread-1654233845421} VFSNotebookRepo.java[setNotebookDirectory]:69) - Using notebookDir: /home/fedora/zeppelin/notebook
    >    INFO [2022-06-03 05:24:05,606] ({main} ZeppelinServer.java[setupWebAppContext]:577) - warPath is: /home/fedora/zeppelin/zeppelin-web-0.10.0.war
    >    INFO [2022-06-03 05:24:05,606] ({main} ZeppelinServer.java[setupWebAppContext]:590) - ZeppelinServer Webapp path: /home/fedora/zeppelin/webapps
    >    INFO [2022-06-03 05:24:05,626] ({main} ZeppelinServer.java[setupWebAppContext]:577) - warPath is: /home/fedora/zeppelin/zeppelin-web-angular-0.10.0.war
    >    INFO [2022-06-03 05:24:05,627] ({main} ZeppelinServer.java[setupWebAppContext]:590) - ZeppelinServer Webapp path: /home/fedora/zeppelin/webapps/next
    >    INFO [2022-06-03 05:24:05,670] ({main} NotebookServer.java[<init>]:156) - NotebookServer instantiated: org.apache.zeppelin.socket.NotebookServer@1bd39d3c
    >    INFO [2022-06-03 05:24:05,671] ({main} NotebookServer.java[setNotebook]:167) - Injected NotebookProvider
    >    INFO [2022-06-03 05:24:05,671] ({main} NotebookServer.java[setServiceLocator]:161) - Injected ServiceLocator: ServiceLocatorImpl(shared-locator,0,895281180)
    >    INFO [2022-06-03 05:24:05,671] ({main} NotebookServer.java[setNotebookService]:174) - Injected NotebookServiceProvider
    >    INFO [2022-06-03 05:24:05,671] ({main} NotebookServer.java[setAuthorizationServiceProvider]:181) - Injected NotebookAuthorizationServiceProvider
    >    INFO [2022-06-03 05:24:05,671] ({main} NotebookServer.java[setConnectionManagerProvider]:187) - Injected ConnectionManagerProvider
    >    INFO [2022-06-03 05:24:05,671] ({ImmediateThread-1654233845421} GitNotebookRepo.java[init]:77) - Opening a git repo at '/home/fedora/zeppelin/notebook'
    >    INFO [2022-06-03 05:24:05,672] ({main} ZeppelinServer.java[setupClusterManagerServer]:467) - Cluster mode is disabled
    >    INFO [2022-06-03 05:24:05,672] ({main} ZeppelinServer.java[main]:251) - Starting zeppelin server
    >   ....


    Try a simple test

    loopcount=1
    usercount=1

    >   ....
    >   ------------ Test Result: [PASS] ------------
    >   ....


    Try a harder test

    loopcount=2
    usercount=2

    >   ....
    >   ------------ Test Result: [PASS] ------------
    >   ....
    >   ------------ Test Result: [PASS] ------------
    >   ....

    Try a harder test

    loopcount=4
    usercount=5



    >   ....
    >    INFO [2022-06-03 05:43:42,352] ({qtp2128029086-6278} NotebookRestApi.java[initParagraph]:1105) - Init Paragraph for user Fipa
    >    INFO [2022-06-03 05:43:42,352] ({qtp2128029086-6278} NotebookRestApi.java[configureParagraph]:1116) - Configure Paragraph for user Fipa
    >   ERROR [2022-06-03 05:43:42,356] ({qtp2128029086-6345} WebApplicationExceptionMapper.java[toResponse]:49) - Error response
    >   java.util.ConcurrentModificationException
    >   	at java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1704)
    >   	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    >   	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    >   	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    >   	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    >   	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    >   	at org.apache.zeppelin.notebook.Notebook.getNotesInfo(Notebook.java:662)
    >   	at org.apache.zeppelin.service.NotebookService.listNotesInfo(NotebookService.java:245)
    >   	at org.apache.zeppelin.rest.NotebookRestApi.getNoteList(NotebookRestApi.java:318)
    >   	at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
    >   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >   	at java.lang.reflect.Method.invoke(Method.java:498)
    >   ....


    >   ....
    >   	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
    >   	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
    >   	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:375)
    >   	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
    >   	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
    >   	at java.lang.Thread.run(Thread.java:748)
    >    INFO [2022-06-03 05:43:57,686] ({qtp2128029086-6525} LoginRestApi.java[postLogin]:249) - {"status":"FORBIDDEN","message":""}
    >   ERROR [2022-06-03 05:43:57,746] ({LuceneSearch5} NoteEventAsyncListener.java[run]:128) - Fail to handle NoteEvent
    >   java.lang.IllegalStateException: this writer hit an unrecoverable error; cannot complete commit
    >   	at org.apache.lucene.index.IndexWriter.finishCommit(IndexWriter.java:3801)
    >   	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:3779)
    >   	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3729)
    >   	at org.apache.zeppelin.search.LuceneSearch.updateDoc(LuceneSearch.java:241)
    >   	at org.apache.zeppelin.search.LuceneSearch.addIndexDocAsync(LuceneSearch.java:326)
    >   	at org.apache.zeppelin.search.LuceneSearch.addNoteIndex(LuceneSearch.java:305)
    >   	at org.apache.zeppelin.search.SearchService.handleNoteCreateEvent(SearchService.java:108)
    >   	at org.apache.zeppelin.notebook.NoteEventAsyncListener$EventHandling.run(NoteEventAsyncListener.java:113)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
    >   ERROR [2022-06-03 05:43:57,747] ({LuceneSearch5} NoteEventAsyncListener.java[run]:128) - Fail to handle NoteEvent
    >   org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed
    >   	at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:877)
    >   	at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:891)
    >   	at org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1468)
    >   	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1757)
    >   	at org.apache.zeppelin.search.LuceneSearch.updateDoc(LuceneSearch.java:240)
    >   	at org.apache.zeppelin.search.LuceneSearch.addParagraphIndex(LuceneSearch.java:314)
    >   	at org.apache.zeppelin.search.SearchService.handleParagraphCreateEvent(SearchService.java:123)
    >   	at org.apache.zeppelin.notebook.NoteEventAsyncListener$EventHandling.run(NoteEventAsyncListener.java:119)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   ....


    >   ....
    >    INFO [2022-06-03 05:44:07,033] ({qtp2128029086-6665} LocalConfigStorage.java[save]:70) - Save notebook authorization to file: /home/fedora/zeppelin/conf/notebook-authorization.json
    >    INFO [2022-06-03 05:44:07,155] ({qtp2128029086-6665} VFSNotebookRepo.java[save]:144) - Saving note 2H6BTBEZW to tmp/6G8WI0Q8EA.json_2H6BTBEZW.zpln
    >   ERROR [2022-06-03 05:44:07,155] ({LuceneSearch5} NoteEventAsyncListener.java[run]:128) - Fail to handle NoteEvent
    >   org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed
    >   	at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:877)
    >   	at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:891)
    >   	at org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1468)
    >   	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1757)
    >   	at org.apache.zeppelin.search.LuceneSearch.updateDoc(LuceneSearch.java:240)
    >   	at org.apache.zeppelin.search.LuceneSearch.indexNoteName(LuceneSearch.java:398)
    >   	at org.apache.zeppelin.search.LuceneSearch.addIndexDocAsync(LuceneSearch.java:324)
    >   	at org.apache.zeppelin.search.LuceneSearch.addNoteIndex(LuceneSearch.java:305)
    >   	at org.apache.zeppelin.search.SearchService.handleNoteCreateEvent(SearchService.java:108)
    >   	at org.apache.zeppelin.notebook.NoteEventAsyncListener$EventHandling.run(NoteEventAsyncListener.java:113)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
    >   ....

    >   ....
    >   	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
    >   	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
    >   	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
    >   	at java.lang.Thread.run(Thread.java:748)
    >    INFO [2022-06-03 05:44:44,195] ({qtp2128029086-7031} LoginRestApi.java[postLogin]:249) - {"status":"FORBIDDEN","message":""}
    >   ERROR [2022-06-03 05:44:53,301] ({SchedulerFactory73} Job.java[run]:174) - Job failed
    >   java.lang.OutOfMemoryError: Java heap space
    >   ERROR [2022-06-03 05:44:53,303] ({SchedulerFactory73} NotebookServer.java[onStatusChange]:1978) - Error
    >   java.lang.OutOfMemoryError: Java heap space
    >    WARN [2022-06-03 05:44:53,303] ({SchedulerFactory73} NotebookServer.java[onStatusChange]:1986) - Job paragraph_1654235056489_233652329 is finished, status: ERROR, exception: java.lang.OutOfMemoryError: Java heap space, result: null
    >    INFO [2022-06-03 05:44:53,303] ({SchedulerFactory73} VFSNotebookRepo.java[save]:144) - Saving note 2H3SQNHFX to tmp/A2J3VR1BSY.json_2H3SQNHFX.zpln
    >    WARN [2022-06-03 05:44:53,428] ({qtp2128029086-7118} QueuedThreadPool.java[run]:950) -
    >   java.lang.OutOfMemoryError: GC overhead limit exceeded
    >    INFO [2022-06-03 05:44:53,677] ({SchedulerFactory73} AbstractScheduler.java[runJob]:154) - Job paragraph_1654235056489_233652329 finished by scheduler RemoteInterpreter-spark-Fipa-shared_session with status ERROR
    >   ERROR [2022-06-03 05:44:53,747] ({qtp2128029086-7118} LoginRestApi.java[proceedToLogin]:213) - Exception in login:
    >   org.apache.shiro.authc.AuthenticationException: Authentication token of type [class org.apache.shiro.authc.UsernamePasswordToken] could not be authenticated by any configured realms.  Please ensure that at least one realm can authenticate these tokens.
    >   	at org.apache.shiro.authc.pam.AtLeastOneSuccessfulStrategy.afterAllAttempts(AtLeastOneSuccessfulStrategy.java:58)
    >   	at org.apache.shiro.authc.pam.ModularRealmAuthenticator.doMultiRealmAuthentication(ModularRealmAuthenticator.java:241)
    >   	at org.apache.shiro.authc.pam.ModularRealmAuthenticator.doAuthenticate(ModularRealmAuthenticator.java:275)
    >   ....


    #
    # 1226 notebooks in the authorization file.

    jq '.authInfo | length'  /home/fedora/zeppelin/conf/notebook-authorization.json

    >   1226

    #
    # Everything has ground to a halt.
    # Nothing to do with Spark, everything to do with Zeppelin.
    #
    # Browser login auth works but unable to display home page - hangs
    #

    #
    # Delete all the notebooks in 'tmp'
    # Delete the authorization file.
    # Restart Zeppelin ...

    pushd /home/fedora/zeppelin/notebook/tmp/
        rm *
    popd
    rm /home/fedora/zeppelin/conf/notebook-authorization.json

    zeppelin-daemon.sh restart

    start the same test again ..


    #
    # Locks up, first few notes created, but basically this Zeppelin instance is borked.
    # Restarting things doesn't solve it.
    #

    loopcount=2
    usercount=2

    NOW we start to see applications blocked in Hadoop
        Queue's AM resource limit exceeded.
        Details : AM Partition = <DEFAULT_PARTITION>;
        AM Resource Request = <memory:3072, vCores:1>;
        Queue Resource Limit for AM = <memory:26624, vCores:1>;
        User AM Resource Limit of the queue = <memory:26624, vCores:1>;
        Queue AM Resource Usage = <memory:24576, vCores:8>;




    Kill all applications on YARN which are in RUNNING state:
    https://stackoverflow.com/a/56035711

    ssh master01

        for x in $(yarn application -list -appStates RUNNING | awk 'NR > 2 { print $1 }'); do yarn application -kill $x; done


    restart zeppelin


    try again

    loopcount=2
    usercount=2

 INFO [2022-06-03 07:44:30,067] ({SchedulerFactory2} AbstractScheduler.java[runJob]:127) - Job paragraph_1654242268481_2129599498 started by scheduler RemoteInterpreter-spark-Fipa-shared_session
 WARN [2022-06-03 07:44:30,068] ({qtp2128029086-37} NotebookServer.java[onStatusChange]:1986) - Job paragraph_1654242267571_1686053048 is finished, status: ERROR, exception: java.util.ConcurrentModificationException, result: %text Unexpected exception: java.util.ConcurrentModificationException
	at java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1633)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.zeppelin.service.JobManagerService.getNoteJobInfoByUnixTime(JobManagerService.java:90)
	at org.apache.zeppelin.socket.NotebookServer.broadcastUpdateNoteJobInfo(NotebookServer.java:519)
	at org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:2007)
	at org.apache.zeppelin.socket.NotebookServer.onStatusChange(NotebookServer.java:105)
	at org.apache.zeppelin.scheduler.Job.setStatus(Job.java:141)
	at org.apache.zeppelin.notebook.Paragraph.setStatus(Paragraph.java:398)
	at org.apache.zeppelin.notebook.Paragraph.execute(Paragraph.java:349)
	at org.apache.zeppelin.notebook.Note.run(Note.java:873)
	at org.apache.zeppelin.service.NotebookService.runParagraph(NotebookService.java:390)
	at org.apache.zeppelin.rest.NotebookRestApi.runParagraph(NotebookRestApi.java:849)


    try again

    loopcount=2
    usercount=2


    >   ....
    >   ------------ Test Result: [PASS] ------------
    >   ....
    >   ------------ Test Result: [PASS] ------------
    >   ....

    let's see how long it will last

    loopcount=20
    usercount=2

    >   ....
    >   ------------ Test Result: [PASS] ------------
    >   ....
    >   ------------ Test Result: [PASS] ------------
    >   ....


    loopcount=4
    usercount=2

    for i in $(seq 0 $((loopcount - 1)))
    do
        echo ""
        echo "-------------"
        echo "Loop [${i}]"
        testname="multi-user-$(printf "%02d" ${usercount})-$(printf "%02d" ${i})"
        echo "Name [${testname}]"

        /tmp/run-benchmark.py \
            "${endpoint:?}" \
            "${testconfig:?}" \
            "${testusers:?}" \
            "${usercount:?}" \
        | tee "/tmp/results/${testname:?}.txt"

        filter-results "${testname:?}"
    done

    grep 'Result' /tmp/results/multi-user-02*

    >   /tmp/results/multi-user-02-00.txt:------------ Test Result: [PASS] ------------
    >   /tmp/results/multi-user-02-01.txt:------------ Test Result: [PASS] ------------
    >   /tmp/results/multi-user-02-02.txt:------------ Test Result: [PASS] ------------
    >   /tmp/results/multi-user-02-03.txt:------------ Test Result: [PASS] ------------


    loopcount=4
    usercount=4

    for i in $(seq 0 $((loopcount - 1)))
    do
        echo ""
        echo "-------------"
        echo "Loop [${i}]"
        testname="multi-user-$(printf "%02d" ${usercount})-$(printf "%02d" ${i})"
        echo "Name [${testname}]"

        /tmp/run-benchmark.py \
            "${endpoint:?}" \
            "${testconfig:?}" \
            "${testusers:?}" \
            "${usercount:?}" \
        | tee "/tmp/results/${testname:?}.txt"

        filter-results "${testname:?}"
    done

    Doesn't even get off the ground ..
    Fails almost immediately

    >   ....
    >   ERROR [2022-06-03 14:57:14,159] ({SchedulerFactory83} NotebookServer.java[onStatusChange]:1978) - Error
    >   java.lang.OutOfMemoryError: Java heap space
    >   ....


    Options
    Increase space for Zeppelin (kick the can)
    Modify tests to re-use notebooks (in progress)
    Change notebook repo to plain file rather than git - might help
    Move notebook repo to a MongDB database - interesting.

    Count the number of notebooks that caused the problem.

    jq '.authInfo | length'  /home/fedora/zeppelin/conf/notebook-authorization.json

    >   781

    find /home/fedora/zeppelin/notebook -name '*.zpln' | wc -l

    >   779

    Less than I was expecting.



    The places where it fails are

  File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 121, in run_notebook

    >       ....
    >       # Make notebook
    >       batcmd="zdairi --config " + config + " notebook create --filepath " + tmpfile
    >       pipe = subprocess.Popen(batcmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)
    >       result = pipe.communicate()[0]
    >       print (result)
    >   
    >       result = result.decode().split("\n")
    >       text = result[0]
    >       notebookid = text.split(": ")[1]
    >       ....


  File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 133, in run_notebook

    >       ....
    >       # Print notebook
    >       batcmd="zdairi --config " + config + " notebook print --notebook " + notebookid
    >       pipe = subprocess.Popen(batcmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)
    >       result = pipe.communicate()[0]
    >       result = result.decode().split("\n")
    >       json_notebook = json.loads("".join(result), strict=False)
    >       ....

    In both cases AglaisBenchmarker is expecting nice JSON and it gets an error response instead.


--------------------------------------------------------------------------

    Why is this deployment having these problems, when previous deployment had less.

    Problems with notebook repo dominate current testing

        20220602-01-concurrent-tests.txt
        20220601-02-concurrent-tests.txt


    Problems with notebook repo were there in earlier testing

        20220529-02-concurrent-tests.txt

    >   ERROR:root:list index out of range
    >   Traceback (most recent call last):
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 114, in run_notebook
    >       notebookid = text.split(": ")[1]


    We can change the notebook repository handler

        zeppelin.notebook.storage
        org.apache.zeppelin.notebook.repo.GitNotebookRepo
        org.apache.zeppelin.notebook.repo.MongoNotebookRepo

    We can install MongoDB on a Fedora node.
    https://tecadmin.net/install-mongodb-on-fedora/

    We can install MongoDB using Ansible
    https://docs.ansible.com/ansible/latest/collections/community/mongodb/index.html

    We can change the memory available to Zeppelin.
    https://zeppelin.apache.org/docs/0.10.0/setup/operation/configuration.html#zeppelin_mem

        ZEPPELIN_MEM JVM memory options
        default "-Xmx1024m -XX:MaxMetaspaceSize=512m"

