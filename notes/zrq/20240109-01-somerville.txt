#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2024, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: []
#

    Target:

        Grab a copy of the logs to look for clues.

    Result:

        Work in progress ...

# -----------------------------------------------------

    #
    # Use previous notes to try creating our cluster.
    # 20240108-02-somerville.txt
    #

    >   ....
    >   TASK [Create work cluster [somerville-gaia-jade-20240109-work]] ****************
    >   fatal: [bootstrap]: FAILED! => {"changed": false, "command": "/usr/local/bin/helm --version=0.1.0 upgrade -i --reset-values --wait --values=/opt/aglais/clusterapi-config.yml --values=/opt/aglais/openstack-clouds.yml somerville-gaia-jade-20240109-work capi/openstack-cluster", "msg": "Failure when executing Helm command. Exited 1.\nstdout: Release \"somerville-gaia-jade-20240109-work\" does not exist. Installing it now.\n\nstderr: Error: context deadline exceeded\n", "stderr": "Error: context deadline exceeded\n", "stderr_lines": ["Error: context deadline exceeded"], "stdout": "Release \"somerville-gaia-jade-20240109-work\" does not exist. Installing it now.\n", "stdout_lines": ["Release \"somerville-gaia-jade-20240109-work\" does not exist. Installing it now."]}
    >   ....


# -----------------------------------------------------
# Grab a dump of the logs from all the Pods.
#[root@ansibler]

    ssh bootstrap

        source loadconfig

        mkdir /tmp/logs

        for podnamespace in $(
            kubectl \
                --kubeconfig "${kindclusterconf:?}" \
                get pods \
                    --output json \
                    --all-namespaces \
            | jq -r '.items[].metadata | {namespace, name} | tojson'
            )
        do
            echo ""
            echo "----"

            namespace=$(echo ${podnamespace} | jq -r '.namespace')
            name=$(echo  ${podnamespace} | jq -r '.name')

            echo "Space   [${namespace}]"
            echo "Name    [${name}]"

            kubectl \
                --kubeconfig "${kindclusterconf:?}" \
                logs \
                    --namespace "${namespace:?}"  \
                    "${name:?}" \
            > "/tmp/logs/${name:?}.log"

        done

    >   ----
    >   Space   [capi-kubeadm-bootstrap-system]
    >   Name    [capi-kubeadm-bootstrap-controller-manager-55d5767547-lsgdr]
    >   
    >   ----
    >   Space   [capi-kubeadm-control-plane-system]
    >   Name    [capi-kubeadm-control-plane-controller-manager-85fd48fb9b-55kvb]
    >   
    >   ----
    >   Space   [capi-system]
    >   Name    [capi-controller-manager-7cb6bcd4db-q582n]
    >   
    >   ----
    >   Space   [capo-system]
    >   Name    [capo-controller-manager-544cb69b9d-d4rp7]
    >   
    >   ----
    >   Space   [cert-manager]
    >   Name    [cert-manager-66d9545484-htqdh]
    >   
    >   ----
    >   Space   [cert-manager]
    >   Name    [cert-manager-cainjector-7d8b6bd6fb-5r6sn]
    >   
    >   ----
    >   Space   [cert-manager]
    >   Name    [cert-manager-webhook-669b96dcfd-799nx]
    >   
    >   ----
    >   Space   [default]
    >   Name    [cluster-api-addon-provider-66cc76bbbf-znbqz]
    >   
    >   ----
    >   Space   [default]
    >   Name    [somerville-gaia-jade-20240109-work-autoscaler-5f9fdf4864-rgr9z]
    >   Error from server (BadRequest): container "autoscaler" in pod "somerville-gaia-jade-20240109-work-autoscaler-5f9fdf4864-rgr9z" is waiting to start: ContainerCreating
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [coredns-5d78c9869d-d66f2]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [coredns-5d78c9869d-mcdrs]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [etcd-somerville-gaia-jade-20240109-kind-control-plane]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [kindnet-75rcg]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [kube-apiserver-somerville-gaia-jade-20240109-kind-control-plane]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [kube-controller-manager-somerville-gaia-jade-20240109-kind-control-plane]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [kube-proxy-2qqsn]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [kube-scheduler-somerville-gaia-jade-20240109-kind-control-plane]
    >   
    >   ----
    >   Space   [local-path-storage]
    >   Name    [local-path-provisioner-6bc4bddd6b-pbvm4]


# -----------------------------------------------------
# Fold the logs into an archive.
#[user@bootstrap]

    pushd /tmp

        tar -cvzf 20240109-somerville-logs.tar.gz logs

    >   logs/local-path-provisioner-6bc4bddd6b-pbvm4.log
    >   logs/kube-scheduler-somerville-gaia-jade-20240109-kind-control-plane.log
    >   logs/kube-proxy-2qqsn.log
    >   logs/kube-controller-manager-somerville-gaia-jade-20240109-kind-control-plane.log
    >   logs/kube-apiserver-somerville-gaia-jade-20240109-kind-control-plane.log
    >   logs/kindnet-75rcg.log
    >   logs/etcd-somerville-gaia-jade-20240109-kind-control-plane.log
    >   logs/coredns-5d78c9869d-mcdrs.log
    >   logs/coredns-5d78c9869d-d66f2.log
    >   logs/somerville-gaia-jade-20240109-work-autoscaler-5f9fdf4864-rgr9z.log
    >   logs/cluster-api-addon-provider-66cc76bbbf-znbqz.log
    >   logs/cert-manager-webhook-669b96dcfd-799nx.log
    >   logs/cert-manager-cainjector-7d8b6bd6fb-5r6sn.log
    >   logs/cert-manager-66d9545484-htqdh.log
    >   logs/capo-controller-manager-544cb69b9d-d4rp7.log
    >   logs/capi-controller-manager-7cb6bcd4db-q582n.log
    >   logs/capi-kubeadm-control-plane-controller-manager-85fd48fb9b-55kvb.log
    >   logs/capi-kubeadm-bootstrap-controller-manager-55d5767547-lsgdr.log


# -----------------------------------------------------
# -----------------------------------------------------
# Copy the logs from our bootstrap node to our client container.
#[root@ansibler]

    scp bootstrap:/tmp/20240109-somerville-logs.tar.gz .

    >   20240109-somerville-logs.tar.gz         100%  304KB   2.2MB/s   00:00


# -----------------------------------------------------
# -----------------------------------------------------
# Transfer the logs from our client container to our desktop.
#[user@laptop]

    podman ps

    >   CONTAINER ID  IMAGE                                              COMMAND     CREATED         STATUS         PORTS                   NAMES
    >   033a1f53d0b5  ghcr.io/wfau/atolmis/kubernetes-client:2023.06.15  bash        30 minutes ago  Up 30 minutes  0.0.0.0:8001->8001/tcp  ansibler-jade

    uuderid=

    sudo mkdir /var/local/backups
    sudo chgrp users /var/local/backups
    sudo chmod g+w /var/local/backups


    pushd /var/local/backups

        mkdir aglais
        mkdir aglais/2024
        pushd aglais/2024

            mkdir 20240109
            pushd 20240109

                podman cp ansibler-jade:/20240109-somerville-logs.tar.gz .

                tar -xvzf 20240109-somerville-logs.tar.gz



    >   logs/local-path-provisioner-6bc4bddd6b-pbvm4.log
    >   logs/kube-scheduler-somerville-gaia-jade-20240109-kind-control-plane.log
    >   logs/kube-proxy-2qqsn.log
    >   logs/kube-controller-manager-somerville-gaia-jade-20240109-kind-control-plane.log
    >   logs/kube-apiserver-somerville-gaia-jade-20240109-kind-control-plane.log
    >   logs/kindnet-75rcg.log
    >   logs/etcd-somerville-gaia-jade-20240109-kind-control-plane.log
    >   logs/coredns-5d78c9869d-mcdrs.log
    >   logs/coredns-5d78c9869d-d66f2.log
    >   logs/somerville-gaia-jade-20240109-work-autoscaler-5f9fdf4864-rgr9z.log
    >   logs/cluster-api-addon-provider-66cc76bbbf-znbqz.log
    >   logs/cert-manager-webhook-669b96dcfd-799nx.log
    >   logs/cert-manager-cainjector-7d8b6bd6fb-5r6sn.log
    >   logs/cert-manager-66d9545484-htqdh.log
    >   logs/capo-controller-manager-544cb69b9d-d4rp7.log
    >   logs/capi-controller-manager-7cb6bcd4db-q582n.log
    >   logs/capi-kubeadm-control-plane-controller-manager-85fd48fb9b-55kvb.log
    >   logs/capi-kubeadm-bootstrap-controller-manager-55d5767547-lsgdr.log

    #
    # Search through the logs manually.
    #

    #
    # Found a clue !!
    # capo-controller-manager-544cb69b9d-d4rp7.log
    #   Invalid input for dns_nameservers.
    #   Reason: 'unknown' is not a valid nameserver. 'unknown' is not a valid IP address.

    >   ....
    >   I0109 18:28:44.989397       1 recorder.go:104] "events: Failed to create subnet k8s-clusterapi-cluster-default-somerville-gaia-jade-20240109-work: Bad request with: [POST https://somerville.ed.ac.uk:9696/v2.0/subnets], error message: {\"NeutronError\": {\"type\": \"HTTPBadRequest\", \"message\": \"Invalid input for dns_nameservers. Reason: 'unknown' is not a valid nameserver. 'unknown' is not a valid IP address.\", \"detail\": \"\"}}" type="Warning" object={"kind":"OpenStackCluster","namespace":"default","name":"somerville-gaia-jade-20240109-work","uid":"922bb235-1c1b-471a-b99f-49be5da9c584","apiVersion":"infrastructure.cluster.x-k8s.io/v1alpha7","resourceVersion":"1673"} reason="Failedcreatesubnet"
    >   E0109 18:28:44.992239       1 controller.go:329] "Reconciler error" err="failed to reconcile subnets: Bad request with: [POST https://somerville.ed.ac.uk:9696/v2.0/subnets], error message: {\"NeutronError\": {\"type\": \"HTTPBadRequest\", \"message\": \"Invalid input for dns_nameservers. Reason: 'unknown' is not a valid nameserver. 'unknown' is not a valid IP address.\", \"detail\": \"\"}}" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/somerville-gaia-jade-20240109-work" namespace="default" name="somerville-gaia-jade-20240109-work" reconcileID="f07b1637-57a4-47a5-9147-c69e3764b108"
    >   ....


    #
    # Fix the typo.
    #

    gedit deployments/cluster-api/bootstrap/ansible/config/deployments.yml

        deployments:
            somerville-jade:

    -           dnsservers: "unknown"
    +           dnsservers: "8.8.8.8"


# -----------------------------------------------------
# -----------------------------------------------------
# Delete and create everything.
#[root@ansibler]

    export cloudsite=somerville-jade

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


    ansible-playbook \
        --inventory 'bootstrap,' \
        '/deployments/cluster-api/bootstrap/ansible/00-create-all.yml'

    >   ....
    >   ....
    >   
    >   TASK [Create work cluster [somerville-gaia-jade-20240109-work]] *************************************
    >   fatal: [bootstrap]: FAILED! => {"changed": false, "command": "/usr/local/bin/helm --version=0.1.0 upgrade -i --reset-values --wait --values=/opt/aglais/clusterapi-config.yml --values=/opt/aglais/openstack-clouds.yml somerville-gaia-jade-20240109-work capi/openstack-cluster", "msg": "Failure when executing Helm command. Exited 1.\nstdout: Release \"somerville-gaia-jade-20240109-work\" does not exist. Installing it now.\n\nstderr: Error: context deadline exceeded\n", "stderr": "Error: context deadline exceeded\n", "stderr_lines": ["Error: context deadline exceeded"], "stdout": "Release \"somerville-gaia-jade-20240109-work\" does not exist. Installing it now.\n", "stdout_lines": ["Release \"somerville-gaia-jade-20240109-work\" does not exist. Installing it now."]}

# -----------------------------------------------------

    #
    # Grab a copy of the logs ...
    #

# -----------------------------------------------------

    less /tmp/logs/capo-controller-manager-544cb69b9d-gmcmt.log

    >   ....
    >   ....
    >   I0110 03:17:51.857467       1 loadbalancer.go:643] "Waiting for load balancer" ....
    >   I0110 03:17:52.103641       1 loadbalancer.go:643] "Waiting for load balancer" ....
    >   I0110 03:17:55.055050       1 recorder.go:104] "events: Deleted attach interface: ....
    >   I0110 03:17:55.405624       1 recorder.go:104] "events: Deleted port ....
    >   I0110 03:18:05.840956       1 openstackmachine_controller.go:284] "Reconciled Machine delete successfully" ....
    >   I0110 03:18:05.843069       1 recorder.go:104] "events: Deleted server somerville-gaia-jade-20240109-work-control-plane-758a2bfb-7nd7h ....
    >   I0110 03:18:12.312505       1 openstackmachine_controller.go:99] "Machine Controller has not yet set OwnerRef" ....
    >   I0110 03:18:12.462396       1 openstackmachine_controller.go:99] "Machine Controller has not yet set OwnerRef" ....
    >   I0110 03:18:12.580913       1 openstackmachine_controller.go:99] "Machine Controller has not yet set OwnerRef" ....
    >   I0110 03:18:12.630558       1 openstackmachine_controller.go:99] "Machine Controller has not yet set OwnerRef" ....
    >   I0110 03:18:13.216388       1 openstackmachine_controller.go:309] "Bootstrap data secret reference is not yet available" ....
    >   I0110 03:18:13.611068       1 openstackmachine_controller.go:309] "Bootstrap data secret reference is not yet available" ....
    >   I0110 03:18:13.861388       1 openstackmachine_controller.go:309] "Bootstrap data secret reference is not yet available" ....
    >   I0110 03:18:14.256408       1 openstackmachine_controller.go:317] "Reconciling Machine" controller="openstackmachine" ....
    >   I0110 03:18:14.376062       1 openstackmachine_controller.go:443] "Machine does not exist, creating Machine" ....
    >   I0110 03:18:15.086129       1 recorder.go:104] "events: Created port somerville-gaia-jade-20240109-work-control-plane-758a2bfb-p6g4x-0 ....
    >   I0110 03:18:26.626094       1 recorder.go:104] "events: Created server somerville-gaia-jade-20240109-work-control-plane-758a2bfb-p6g4x ....
    >   I0110 03:18:26.626572       1 openstackmachine_controller.go:363] "Machine instance state is ACTIVE" ....
    >   I0110 03:18:26.626924       1 loadbalancer.go:403] "Reconciling load balancer member" ....
    >   I0110 03:18:26.760726       1 loadbalancer.go:448] "Creating load balancer member" ....
    >   I0110 03:18:26.760815       1 loadbalancer.go:643] "Waiting for load balancer" ....
    >   I0110 03:18:27.086719       1 loadbalancer.go:643] "Waiting for load balancer" ....
    >   I0110 03:18:31.320329       1 openstackmachine_controller.go:429] "Reconciled Machine create successfully" ....
    >   I0110 03:18:33.703000       1 openstackmachine_controller.go:317] "Reconciling Machine" ....
    >   I0110 03:18:33.958877       1 openstackmachine_controller.go:363] "Machine instance state is ACTIVE" ....
    >   I0110 03:18:33.958995       1 loadbalancer.go:403] "Reconciling load balancer member" ....
    >   I0110 03:18:34.106439       1 openstackmachine_controller.go:429] "Reconciled Machine create successfully" ....
    >   I0110 03:18:42.915569       1 openstackmachine_controller.go:317] "Reconciling Machine" ....
    >   I0110 03:18:43.160715       1 openstackmachine_controller.go:363] "Machine instance state is ACTIVE" ....
    >   I0110 03:18:43.160940       1 loadbalancer.go:403] "Reconciling load balancer member" ....
    >   I0110 03:18:43.306522       1 openstackmachine_controller.go:429] "Reconciled Machine create successfully" ....
    >   I0110 03:18:43.315186       1 openstackmachine_controller.go:317] "Reconciling Machine" ....
    >   I0110 03:18:43.562668       1 openstackmachine_controller.go:363] "Machine instance state is ACTIVE" ....
    >   I0110 03:18:43.563055       1 loadbalancer.go:403] "Reconciling load balancer member" ....
    >   I0110 03:18:43.721437       1 openstackmachine_controller.go:429] "Reconciled Machine create successfully" ....
    >   I0110 03:18:53.080936       1 openstackmachine_controller.go:317] "Reconciling Machine" ....
    >   I0110 03:18:53.326638       1 openstackmachine_controller.go:363] "Machine instance state is ACTIVE" ....
    >   I0110 03:18:53.326990       1 loadbalancer.go:403] "Reconciling load balancer member" ....
    >   I0110 03:18:53.471793       1 openstackmachine_controller.go:429] "Reconciled Machine create successfully" ....


    less /tmp/logs/capi-controller-manager-7cb6bcd4db-jkqh4.log

    >   ....
    >   I0109 19:38:12.831059       1 recorder.go:104] "events: Machine default/somerville-gaia-jade-20240109-work-control-plane/somerville-gaia-jade-20240109-work-control-plane-t968c/ has unhealthy node " type="Normal" object={"kind":"Machine","namespace":"default","name":"somerville-gaia-jade-20240109-work-control-plane-t968c","uid":"5c0cd850-27d1-4025-a60f-4afa5ef91a4d","apiVersion":"cluster.x-k8s.io/v1beta1","resourceVersion":"3100"} reason="DetectedUnhealthy"
    >   E0109 19:38:12.910135       1 controller.go:329] "Reconciler error" err="failed to create cluster accessor: error creating client for remote cluster \"default/somerville-gaia-jade-20240109-work\": error getting rest mapping: failed to get API group resources: unable to retrieve the complete list of server APIs: v1: client rate limiter Wait returned an error: context deadline exceeded - error from a previous attempt: EOF" controller="machine" controllerGroup="cluster.x-k8s.io" controllerKind="Machine" Machine="default/somerville-gaia-jade-20240109-work-control-plane-t968c" namespace="default" name="somerville-gaia-jade-20240109-work-control-plane-t968c" reconcileID="2645fd54-74ea-4742-95dd-9aa653915b5f"
    >   I0109 19:38:12.914919       1 recorder.go:104] "events: Machine default/somerville-gaia-jade-20240109-work-control-plane/somerville-gaia-jade-20240109-work-control-plane-t968c/ has unhealthy node " type="Normal" object={"kind":"Machine","namespace":"default","name":"somerville-gaia-jade-20240109-work-control-plane-t968c","uid":"5c0cd850-27d1-4025-a60f-4afa5ef91a4d","apiVersion":"cluster.x-k8s.io/v1beta1","resourceVersion":"3104"} reason="DetectedUnhealthy"
    >   E0109 19:38:22.933789       1 controller.go:329] "Reconciler error" err="failed to create cluster accessor: error creating client for remote cluster \"default/somerville-gaia-jade-20240109-work\": error getting rest mapping: failed to get API group resources: unable to retrieve the complete list of server APIs: v1: client rate limiter Wait returned an error: context deadline exceeded - error from a previous attempt: EOF" controller="machine" controllerGroup="cluster.x-k8s.io" controllerKind="Machine" Machine="default/somerville-gaia-jade-20240109-work-control-plane-t968c" namespace="default" name="somerville-gaia-jade-20240109-work-control-plane-t968c" reconcileID="e5f63af7-93fc-4680-ab14-2a08ceb5277b"
    >   ....


    less /tmp/logs/capi-kubeadm-bootstrap-controller-manager-55d5767547-wtr5j.log

    >   ....
    >   I0110 03:18:13.149744       1 control_plane_init_mutex.go:86] "Waiting for Machine somerville-gaia-jade-20240109-work-control-plane-zcg47 to initialize" controller="kubeadmconfig" controllerGroup="bootstrap.cluster.x-k8s.io" controllerKind="KubeadmConfig" KubeadmConfig="default/somerville-gaia-jade-20240109-work-control-plane-lmw2g" namespace="default" name="somerville-gaia-jade-20240109-work-control-plane-lmw2g" reconcileID="19c1d6e2-99f7-45ff-abc0-934a796b2655" Machine="default/somerville-gaia-jade-20240109-work-control-plane-vl65m" Machine="default/somerville-gaia-jade-20240109-work-control-plane-vl65m" resourceVersion="135228" Cluster="default/somerville-gaia-jade-20240109-work" ConfigMap="default/somerville-gaia-jade-20240109-work-lock"
    >   I0110 03:18:13.149976       1 kubeadmconfig_controller.go:389] "A control plane is already being initialized, requeuing until control plane is ready" controller="kubeadmconfig" controllerGroup="bootstrap.cluster.x-k8s.io" controllerKind="KubeadmConfig" KubeadmConfig="default/somerville-gaia-jade-20240109-work-control-plane-lmw2g" namespace="default" name="somerville-gaia-jade-20240109-work-control-plane-lmw2g" reconcileID="19c1d6e2-99f7-45ff-abc0-934a796b2655" Machine="default/somerville-gaia-jade-20240109-work-control-plane-vl65m" Machine="default/somerville-gaia-jade-20240109-work-control-plane-vl65m" resourceVersion="135228" Cluster="default/somerville-gaia-jade-20240109-work"
    >   I0110 03:18:13.415654       1 control_plane_init_mutex.go:98] "Attempting to acquire the lock" controller="kubeadmconfig" controllerGroup="bootstrap.cluster.x-k8s.io" controllerKind="KubeadmConfig" KubeadmConfig="default/somerville-gaia-jade-20240109-work-control-plane-lmw2g" namespace="default" name="somerville-gaia-jade-20240109-work-control-plane-lmw2g" reconcileID="f27e975e-8674-4cdb-a513-0d377f9e060f" Machine="default/somerville-gaia-jade-20240109-work-control-plane-vl65m" Machine="default/somerville-gaia-jade-20240109-work-control-plane-vl65m" resourceVersion="135242" Cluster="default/somerville-gaia-jade-20240109-work" ConfigMap="default/somerville-gaia-jade-20240109-work-lock"
    >   I0110 03:18:13.487213       1 kubeadmconfig_controller.go:401] "Creating BootstrapData for the first control plane" controller="kubeadmconfig" controllerGroup="bootstrap.cluster.x-k8s.io" controllerKind="KubeadmConfig" KubeadmConfig="default/somerville-gaia-jade-20240109-work-control-plane-lmw2g" namespace="default" name="somerville-gaia-jade-20240109-work-control-plane-lmw2g" reconcileID="f27e975e-8674-4cdb-a513-0d377f9e060f" Machine="default/somerville-gaia-jade-20240109-work-control-plane-vl65m" Machine="default/somerville-gaia-jade-20240109-work-control-plane-vl65m" resourceVersion="135242" Cluster="default/somerville-gaia-jade-20240109-work"
    >   ....


    >   ....
    >   I0109 19:35:54.574763       1 loadbalancer.go:51] "Reconciling load balancer" controller="openstackcl
    >   uster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/somerville-gaia-jade-20240109-work" namespace="default" reconcileID="8d18b742-ec60-4d27-b0c1-ac750628d59b" cluster="somerville-gaia-jade-20240109-work" name="k8s-clusterapi-cluster-default-somerville-gaia-jade-20240109-work-kubeapi"
    >   I0109 19:35:54.654784       1 loadbalancer.go:643] "Waiting for load balancer" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/somerville-gaia-jade-20240109-work" namespace="default" name="somerville-gaia-jade-20240109-work" reconcileID="8d18b742-ec60-4d27-b0c1-ac750628d59b" cluster="somerville-gaia-jade-20240109-work" id="96ffab56-ccf9-4f27-8920-d393980afc31" targetStatus="ACTIVE"
    >   I0109 19:35:54.774233       1 floatingip.go:124] "Associating floating IP" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/somerville-gaia-jade-20240109-work" namespace="default" name="somerville-gaia-jade-20240109-work" reconcileID="8d18b742-ec60-4d27-b0c1-ac750628d59b" cluster="somerville-gaia-jade-20240109-work" ID="2608d384-a97e-4147-918a-0e41088d1d8c" IP="192.41.122.154"
    >   I0109 19:35:54.774274       1 floatingip.go:127] "Floating IP already associated:" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/somerville-gaia-jade-20240109-work" namespace="default" name="somerville-gaia-jade-20240109-work" reconcileID="8d18b742-ec60-4d27-b0c1-ac750628d59b" cluster="somerville-gaia-jade-20240109-work" ID="2608d384-a97e-4147-918a-0e41088d1d8c" IP="192.41.122.154"
    >   I0109 19:35:54.938159       1 openstackcluster_controller.go:312] "Reconciling Bastion" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/somerville-gaia-jade-20240109-work" namespace="default" name="somerville-gaia-jade-20240109-work" reconcileID="8d18b742-ec60-4d27-b0c1-ac750628d59b" cluster="somerville-gaia-jade-20240109-work"
    >   I0109 19:35:55.205548       1 openstackcluster_controller.go:307] "Reconciled Cluster created successfully" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/somerville-gaia-jade-20240109-work" namespace="default" name="somerville-gaia-jade-20240109-work" reconcileID="8d18b742-ec60-4d27-b0c1-ac750628d59b" cluster="somerville-gaia-jade-20240109-work"
    >   ....

# -----------------------------------------------------
# -----------------------------------------------------
# On a hunch ... check the load balancer status ..
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer list

    >   +--------------------------------------+---------------------------------------------------------------------------+----------------------------------+---------------+---------------------+------------------+----------+
    >   | id                                   | name                                                                      | project_id                       | vip_address   | provisioning_status | operating_status | provider |
    >   +--------------------------------------+---------------------------------------------------------------------------+----------------------------------+---------------+---------------------+------------------+----------+
    >   | 96ffab56-ccf9-4f27-8920-d393980afc31 | k8s-clusterapi-cluster-default-somerville-gaia-jade-20240109-work-kubeapi | be227fe0300b4ce5b03f44264df615df | 192.168.3.221 | ACTIVE              | ERROR            | amphora  |
    >   +--------------------------------------+---------------------------------------------------------------------------+----------------------------------+---------------+---------------------+------------------+----------+


    balancerid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            loadbalancer list \
                --format json \
        | jq -r '.[0].id'
        )

    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer show \
            --format json \
            "${balancerid}" \
    | jq '.'

    >   {
    >     "admin_state_up": true,
    >     "availability_zone": null,
    >     "created_at": "2024-01-09T19:33:10",
    >     "description": "Created by cluster-api-provider-openstack cluster default-somerville-gaia-jade-20240109-work",
    >     "flavor_id": null,
    >     "id": "96ffab56-ccf9-4f27-8920-d393980afc31",
    >     "listeners": "ff844b43-4523-496d-b3f8-283ccbc485cd",
    >     "name": "k8s-clusterapi-cluster-default-somerville-gaia-jade-20240109-work-kubeapi",
    >     "operating_status": "ERROR",
    >     "pools": "6991360a-1d13-4366-9a0c-b2d4ea04eb78",
    >     "project_id": "be227fe0300b4ce5b03f44264df615df",
    >     "provider": "amphora",
    >     "provisioning_status": "ACTIVE",
    >     "updated_at": "2024-01-10T04:21:50",
    >     "vip_address": "192.168.3.221",
    >     "vip_network_id": "92b16c70-c32d-4504-8a66-53b88b12426d",
    >     "vip_port_id": "d476287b-3532-4dee-95d9-3e1a70621f61",
    >     "vip_qos_policy_id": null,
    >     "vip_subnet_id": "0f3950c8-16d5-46e5-b98e-346c07c9ec46",
    >     "tags": ""
    >   }

    #
    # Same error as Cambridge Arcus !?
    #

    >   {
    >     ....
    >     "operating_status": "ERROR",
    >     ....
    >   }

    #
    # Either ...
    #     There is a generic issue with this version of capi controllers ?
    #     Or, our scripts and/or config have the error.
    #

# -----------------------------------------------------

    Google:

        cluster-api-provider-openstack loadbalancer operating_status ERROR

# -----------------------------------------------------

Found the STFC cloud docs

    https://stfc-cloud-docs.readthedocs.io/en/latest/#
    https://stfc-cloud-docs.readthedocs.io/en/latest/Kubernetes/index.html
    https://stfc.atlassian.net/wiki/spaces/CLOUDKB/pages/211878034/Cluster+API+Setup

# -----------------------------------------------------

    STFC cloud are moving to RKE2 ?

    RKE2, also known as RKE Government, is Rancher's next-generation Kubernetes distribution.
    https://docs.rke2.io/

# -----------------------------------------------------

    Kubernetes Cluster API Provider OpenStack - Troubleshooting
    https://cluster-api-openstack.sigs.k8s.io/topics/troubleshooting.html

    ssh bootstrap

        source loadconfig

        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            --namespace capo-system \
            logs \
                --follow \
                --selector control-plane=capo-controller-manager \
                --container manager

    >   ....
    >   I0110 05:03:36.353857       1 openstackmachine_controller.go:309] "Bootstrap data secret reference is not yet available" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/somerville-gaia-jade-20240109-work-control-plane-758a2bfb-vvh2j" namespace="default" name="somerville-gaia-jade-20240109-work-control-plane-758a2bfb-vvh2j" reconcileID="47797801-2839-43ee-96ce-a0f7fe789202" openStackMachine="somerville-gaia-jade-20240109-work-control-plane-758a2bfb-vvh2j" machine="somerville-gaia-jade-20240109-work-control-plane-fvbgb" cluster="somerville-gaia-jade-20240109-work" openStackCluster="somerville-gaia-jade-20240109-work"
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------

    #
    # Could we be running into the name length problem ?
    #

    deployments/cluster-api/bootstrap/ansible/config/deployments.yml

        #deployname: "{{ cloudname }}-{{ datestamp.stdout }}"
        deployname: "test-{{ datestamp.stdout }}"

# -----------------------------------------------------

    ssh bootstrap

        source loadconfig

        clusterctl \
            --kubeconfig "${kindclusterconf:?}" \
            describe cluster \
                "${workclustername:?}"

    >   NAME                                                                   READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/test-20240110-work                                             False  Warning   ScalingUp                    4m19s  Scaling up control plane to 3 replicas (actual 1)
    >   ├─ClusterInfrastructure - OpenStackCluster/test-20240110-work
    >   ├─ControlPlane - KubeadmControlPlane/test-20240110-work-control-plane  False  Warning   ScalingUp                    4m19s  Scaling up control plane to 3 replicas (actual 1)
    >   │ └─Machine/test-20240110-work-control-plane-ptlc6                     True                                          4m21s
    >   └─Workers
    >     └─MachineDeployment/test-20240110-work-md-0                          False  Warning   WaitingForAvailableMachines  16m    Minimum availability requires 2 replicas, current 0 available
    >       └─3 Machines...                                                    False  Info      WaitingForBootstrapData      15m    See test-20240110-work-md-0-9m5j4-dpdrb, test-20240110-work-md-0-9m5j4-lxdg4, ...


        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            --namespace capo-system \
            logs \
                --follow \
                --selector control-plane=capo-controller-manager \
                --container manager

    >   ....
    >   I0110 05:53:17.044253       1 openstackmachine_controller.go:429] "Reconciled Machine create successfully" ....
    >   I0110 05:54:36.197724       1 openstackmachine_controller.go:309] "Bootstrap data secret reference is not yet available" ....
    >   I0110 05:54:36.199474       1 openstackmachine_controller.go:309] "Bootstrap data secret reference is not yet available" ....
    >   I0110 05:54:36.202068       1 openstackmachine_controller.go:309] "Bootstrap data secret reference is not yet available" ....
    >   I0110 05:54:36.205449       1 openstackmachine_controller.go:317] "Reconciling Machine" controller="openstackcluster" ....
    >   I0110 05:54:36.479199       1 openstackmachine_controller.go:363] "Machine instance state is ACTIVE" ....
    >   I0110 05:54:36.479256       1 loadbalancer.go:403] "Reconciling load balancer member" ....
    >   I0110 05:54:36.640026       1 openstackmachine_controller.go:429] "Reconciled Machine create successfully" ....
    >   ....

# -----------------------------------------------------
# -----------------------------------------------------

    #
    # The Kubernetes versions don't match.
    #

    deployments/cluster-api/bootstrap/ansible/config/deployments.yml

        kubernetes:

    -       version: "1.25.4"
    +       version: "1.26.7"

        deployments:
            somerville-jade:
                machines:
                    ....
                    controlnode:
                        ....
                        image:  "gaia-dmp-ubuntu-2204-kube-v1.26.7"

# -----------------------------------------------------

    source loadconfig

    watch \
        clusterctl \
            --kubeconfig "${kindclusterconf:?}" \
            describe cluster \
                "${workclustername:?}"

    >   NAME                                                                   READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/test-20240110-work                                             True                                          93s
    >   ├─ClusterInfrastructure - OpenStackCluster/test-20240110-work
    >   ├─ControlPlane - KubeadmControlPlane/test-20240110-work-control-plane  True                                          94s
    >   │ └─3 Machines...                                                      True                                          4m7s   See test-20240110-work-control-plane-6v9zc, test-20240110-work-control-plane-lc62c, ...
    >   └─Workers
    >     └─MachineDeployment/test-20240110-work-md-0                          False  Warning   WaitingForAvailableMachines  2m39s  Minimum availability requires 2 replicas, current 1 available
    >       ├─2 Machines...                                                    False  Error     InstanceCreateFailed         4m57s  See test-20240110-work-md-0-dpn7q-62rc6, test-20240110-work-md-0-dpn7q-x4svf
    >       └─Machine/test-20240110-work-md-0-dpn7q-5z2cm                      True

    >   NAME                                                                   READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/test-20240110-work                                             True                                          5m22s
    >   ├─ClusterInfrastructure - OpenStackCluster/test-20240110-work
    >   ├─ControlPlane - KubeadmControlPlane/test-20240110-work-control-plane  True                                          5m23s
    >   │ └─3 Machines...                                                      True                                          7m56s  See test-20240110-work-control-plane-6v9zc, test-20240110-work-control-plane-lc62c, ...
    >   └─Workers
    >     └─MachineDeployment/test-20240110-work-md-0                          False  Warning   WaitingForAvailableMachines  6m28s  Minimum availability requires 2 replicas, current 1 available
    >       ├─2 Machines...                                                    False  Error     InstanceCreateFailed         8m46s  See test-20240110-work-md-0-dpn7q-62rc6, test-20240110-work-md-0-dpn7q-x4svf
    >       └─Machine/test-20240110-work-md-0-dpn7q-5z2cm                      True                                          8m58s

# -----------------------------------------------------

    We don't have enough resources to create the worker nodes.
    According to Horizon we have used 80G of 128G of memory.
    Which leaves 48G of memory available.
    The worker nodes use 64G ..

    Nothing in the logs that indicate this is what is happening.

# -----------------------------------------------------

    #
    # The images are too big.
    #

    deployments/cluster-api/bootstrap/ansible/config/deployments.yml

        deployments:
            somerville-jade:
                machines:
                    ....
                    clusternode:
                        ....
        +               flavor: "qserv-worker-v2"
        -               flavor: "qserv-jump-v2"

# -----------------------------------------------------

    #
    # The VPN service quit a long time ago ..
    #

    >   ....
    >   ....
    >   Logout successful.
    >   Error: argument "via" is wrong: use nexthop syntax to specify multiple via
    >   
    >   Cookie was rejected by server; exiting.

    #
    # So either we are still connected by a zombie process.
    # ... or the firewall is allowing our IP address.
    #

# -----------------------------------------------------

    #
    # WOW, that worked :-)
    # Mismatched version of Kubernetes and not enough resources.
    #

    >   NAME                                                                   READY  SEVERITY  REASON  SINCE  MESSAGE
    >   Cluster/test-20240110-work                                             True                     3h31m
    >   ├─ClusterInfrastructure - OpenStackCluster/test-20240110-work
    >   ├─ControlPlane - KubeadmControlPlane/test-20240110-work-control-plane  True                     3h31m
    >   │ └─3 Machines...                                                      True                     3h35m  See test-20240110-work-control-plane-4fgl5, test-20240110-work-control-plane-5h957, ...
    >   └─Workers
    >     └─MachineDeployment/test-20240110-work-md-0                          True                     3h29m
    >       └─3 Machines...                                                    True                     3h33m  See test-20240110-work-md-0-hf2md-8nm7x,



