#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Calculate some numbers for Somerville 2024/25.

    Result:

        Work in progress ...

# -----------------------------------------------------

    In preparation for the expansion plans of Somerville,
    I am meeting with all the projects to determine their projected resource needs in the 2024-2025 period.
    The goal to is determine what hardware Somerville needs provide in order to meet your project requirements (Gaia Jade).
    We want to know how much you project your total project (including current usage) needs of the following:

        *   CPU
        *   RAM
        *   Ceph HDD (for instance images, volumes, and CephFS shares)
        *   Ceph SSD (for volumes, and CephFS shares)
        *   Local storage (ephemeral disk, either HDD or SSD)
        *   Number of distinct hypervisors (for anti-affinity configurations)


# -----------------------------------------------------

    Current DR3 deployment on Arcus :

        9 instances
        214 cpu cores
        350GiB memory

            zeppelin
                54 core
                86GiB memory
                20GiB flavor disc

            worker-[00-05]
                26 core
                43GiB memory
                20GiB flavor disc

            control
                2 core
                3GiB memory
                14GiB flavor disc

            monitor
                2 core
                3GiB memory
                14GiB flavor disc

        aglais-data-gaia-dr3-2048-20221107
        8TiB Manila CephFS share

        user data is 1G home and 100G data
        30 users 101*30 ~ 3TiB

    Extras:

        Object store
        8TiB Swift object store

        Cinder volumes
        8TiB Cinder volumes



        6 hypervisors, but we don't use anti-affinity (yet)
        potentially spread one worker per hypervisor

        we don't use local storage (yet)
        ideally, 6TiB (1TiB per hypervisors) to replicate main data share



