#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#


    Target:

        Test a full deployment on the Arcus cloud.

    Result:

        'OK', as in "hmm, ok I suppose .."

        Test results are either SUCCESS or SLOW.
        No major speed increase.

        Needs more work to evaluate what these mean.
        What configuration were the original bench marks done on ?
        How does the cores and memory compare with this configuration?

        (*) Note that the VM flavors are a different shape, and we need to compare old and new.
        Some may be bigger, some may be smaller.

        (*) Grafana plot shows sustained periods of 50% IOwait.
        Need to verify this graph is showing what we think it is.


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --publish 3000:3000 \
        --publish 8088:8088 \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        atolmis/ansible-client:2021.08.25 \
        bash


# -----------------------------------------------------
# Set the cloud and configuration.
#[root@ansibler]

    cloudname=iris-gaia-red

    configname=zeppelin-27.45-spark-6.27.45

# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}" \


# -----------------------------------------------------
# Create everything, using the new config.
# Using 'test' to run the built-in tests.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}" \
            "${configname:?}" \
            'test' \
        | tee /tmp/create-all.log


    >   ....
    >   ....
    >   TASK [Run benchmarker] *****************************************************************************************************************************************
    >   changed: [localhost] =>
    >       {
    >       "changed": true,
    >       "cmd": "python3 /tmp/run-test.py | tee /tmp/test-result.json",
    >       "delta": "2:34:49.699823",
    >       "end": "2022-01-21 06:57:49.976395",
    >       "rc": 0,
    >       "start": "2022-01-21 04:23:00.276572",
    >       "stderr": "",
    >       "stderr_lines": [],
    >       "stdout":
    >           "Test completed after: 9289.35 seconds
    >               {
    >               'SetUp': {
    >                   'totaltime': '46.45',
    >                   'status': 'SLOW',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   },
    >               'Mean_proper_motions_over_the_sky': {
    >                   'totaltime': '48.80',
    >                   'status': 'SUCCESS',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   },
    >               'Source_counts_over_the_sky.json': {
    >                   'totaltime': '15.97',
    >                   'status': 'SUCCESS',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   },
    >               'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {
    >                   'totaltime': '517.42',
    >                   'status': 'SLOW',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   },
    >               'QC_cuts_dev.json': {
    >                   'totaltime': '4307.06',
    >                   'status': 'SUCCESS',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   },
    >               'WD_detection_dev.json': {
    >                   'totaltime': '4353.65',
    >                   'status': 'SLOW',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   }
    >               }
    >           ",
    >       "stdout_lines": [
    >           "Test completed after: 9289.35 seconds",
    >           "...."
    >           ]
    >       }
    >
    >   PLAY RECAP ****************************************************************************************
    >   localhost   : ok=9    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >
    >   /
    >
    >   real	196m32.410s
    >   user	41m58.594s
    >   sys	7m21.803s


# -----------------------------------------------------
# Try pretty printing the test results.
# (*) example above was done manually
#[root@ansibler]

    jq '.' /tmp/test-result.json

    >   parse error: Invalid numeric literal at line 1, column 5


    sed '1d' /tmp/test-result.json | jq '.'

    >   parse error: Invalid numeric literal at line 1, column 9




# -----------------------------------------------------
# -----------------------------------------------------

    Second test run ..

    changed: [localhost] => {
        "changed": true,
        "cmd": "python3 /tmp/run-test.py | tee /tmp/test-result.json",
        "delta": "2:33:50.063424",
        "end": "2022-01-21 15:54:20.780910",
        "rc": 0,
        "start": "2022-01-21 13:20:30.717486",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "Test completed after: 9229.76 seconds
            {
            'SetUp': {
                'totaltime': '48.17',
                'status': 'SLOW',
                'msg': '',
                'valid': 'TRUE'
                },
            'Mean_proper_motions_over_the_sky': {
                'totaltime': '46.63',
                'status': 'SUCCESS',
                'msg': '',
                'valid': 'TRUE'
                },
            'Source_counts_over_the_sky.json': {
                'totaltime': '16.39',
                'status': 'SUCCESS',
                'msg': '',
                'valid': 'TRUE'
                },
            'Good_astrometric_solutions_via_ML_Random_Forrest_classifier': {
                'totaltime': '525.33',
                'status': 'SLOW',
                'msg': '',
                'valid': 'TRUE'
                },
            'QC_cuts_dev.json': {
                'totaltime': '4232.19',
                'status': 'SUCCESS',
                'msg': '',
                'valid': 'TRUE'
                },
            'WD_detection_dev.json': {
                'totaltime': '4361.05',
                'status': 'SLOW',
                'msg': '',
                'valid': 'TRUE'
                }
            }
        ....

    #
    # Are these results any goos ?
    # What configuration were the original benchmarks made on ?
    # Did we keep a benchmark for an equivalent config on the old system ?
    # Do we have a table of expected results vs configuration ...
    #


