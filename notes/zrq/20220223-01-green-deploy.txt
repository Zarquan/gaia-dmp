#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        New deployment on green cloud.

    Result:

        Success, up to a point.
        Deployment works and pasess basic tests.
        Ideally I'd like to test with more users, but the current system has a fixed limit of 3.


# -----------------------------------------------------
# Merge upstream changes.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        git checkout master

    >   Switched to branch 'master'
    >   Your branch is up to date with 'origin/master'.


        git fetch upstream

    >   remote: Enumerating objects: 112, done.
    >   remote: Counting objects: 100% (106/106), done.
    >   remote: Compressing objects: 100% (76/76), done.
    >   remote: Total 80 (delta 44), reused 27 (delta 0), pack-reused 0
    >   Unpacking objects: 100% (80/80), 18.61 KiB | 323.00 KiB/s, done.
    >   From github.com:wfau/aglais
    >      fef36b7..560da17  master     -> upstream/master


        git merge upstream/master

    >   Updating fef36b7..560da17
    >   Fast-forward
    >    deployments/common/pip/requirements.txt                        |   1 +
    >    deployments/hadoop-yarn/ansible/27-install-zeppelin.yml        |   3 +
    >    ....
    >    ....
    >    create mode 100644 notes/stv/20220211-test-deploy-benchmarks-02.txt
    >    create mode 100644 notes/stv/20220216-test-deploy-01.txt


        git status

    >   On branch master
    >   Your branch is ahead of 'origin/master' by 11 commits.
    >     (use "git push" to publish your local commits)


        git push

    >   Total 0 (delta 0), reused 0 (delta 0), pack-reused 0
    >   To github.com:Zarquan/aglais.git
    >      fef36b7..560da17  master -> master


    popd


# -----------------------------------------------------
# Create a new branch.
#[user@desktop]

    branchname=deploy

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        branchprev=$(git branch --show-current)
        branchnext=$(date '+%Y%m%d')-zrq-${branchname:?}

        git checkout master
        git checkout -b "${branchnext:?}"

    >   ....
    >   ....
    >   Switched to a new branch '20220223-zrq-deploy'


        git push --set-upstream 'origin' "$(git branch --show-current)"

    >   ....
    >   ....
    >   To github.com:Zarquan/aglais.git
    >    * [new branch]      20220223-zrq-deploy -> 20220223-zrq-deploy
    >   Branch '20220223-zrq-deploy' set up to track remote branch '20220223-zrq-deploy' from 'origin'.


    popd


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --publish 3000:3000 \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        atolmis/ansible-client:2021.08.25 \
        bash


# -----------------------------------------------------
# Set the target cloud.
#[root@ansibler]

    cloudbase='arcus'
    cloudname='iris-gaia-green'
    configname=zeppelin-27.45-spark-6.27.45


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}" \

    >   real    0m58.198s
    >   user    0m26.119s
    >   sys     0m3.203s


# -----------------------------------------------------
# Create everything, using the new config.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}" \
            "${configname:?}" \
        | tee /tmp/create-all.log

    >   real    41m45.157s
    >   user    13m29.280s
    >   sys     4m40.624s


# -----------------------------------------------------
# -----------------------------------------------------
# Check how many times the tests have actuallt been run.
# https://stackoverflow.com/questions/9081/grep-a-file-but-show-several-surrounding-lines
# https://stackoverflow.com/a/9083
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        reset

        grep \
            --recursive \
            --before-context 12 \
            --after-context  2 \
            'run-tests.sh' \
            notes

    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    num_users=1
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    concurrent=False
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    test_level=quick
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt:        /deployments/hadoop-yarn/bin/run-tests.sh \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-            "${cloudname:?}" \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-            "${configname:?}" \
    >   --
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    test_level="basic"
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    # Restart Zeppelin
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-        /deployments/hadoop-yarn/bin/restart-zeppelin.sh
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt:        /deployments/hadoop-yarn/bin/run-tests.sh \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-            "${cloudname:?}" \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-            "${configname:?}" \
    >   --
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    num_users=3
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    concurrent=True
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    test_level="multiuser"
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    # Restart Zeppelin
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-        /deployments/hadoop-yarn/bin/restart-zeppelin.sh
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt:        /deployments/hadoop-yarn/bin/run-tests.sh \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-            "${cloudname:?}" \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-            "${configname:?}" \
    >   --
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    num_users=1
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    concurrent=False
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    test_level="full"
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    # Restart Zeppelin
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-        /deployments/hadoop-yarn/bin/restart-zeppelin.sh
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt:        /deployments/hadoop-yarn/bin/run-tests.sh \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-            "${cloudname:?}" \
    >   notes/stv/20220211-test-deploy-benchmarks-01.txt-            "${configname:?}" \
    >   --
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    num_users=1
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    concurrent=False
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    test_level=quick
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt:        /deployments/hadoop-yarn/bin/run-tests.sh \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-            "${cloudname:?}" \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-            "${configname:?}" \
    >   --
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    test_level="basic"
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    # Restart Zeppelin
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-        /deployments/hadoop-yarn/bin/restart-zeppelin.sh
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt:        /deployments/hadoop-yarn/bin/run-tests.sh \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-            "${cloudname:?}" \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-            "${configname:?}" \
    >   --
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    num_users=3
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    concurrent=True
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    test_level="multiuser"
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    # Restart Zeppelin
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-        /deployments/hadoop-yarn/bin/restart-zeppelin.sh
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt:        /deployments/hadoop-yarn/bin/run-tests.sh \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-            "${cloudname:?}" \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-            "${configname:?}" \
    >   --
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    num_users=1
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    concurrent=False
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    test_level="full"
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    # Restart Zeppelin
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-        /deployments/hadoop-yarn/bin/restart-zeppelin.sh
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-    time \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt:        /deployments/hadoop-yarn/bin/run-tests.sh \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-            "${cloudname:?}" \
    >   notes/stv/20220211-test-deploy-benchmarks-02.txt-            "${configname:?}" \
    >   --
    >   notes/stv/20220216-test-deploy-01.txt-    num_users=1
    >   notes/stv/20220216-test-deploy-01.txt-    concurrent=False
    >   notes/stv/20220216-test-deploy-01.txt-    test_level="full"
    >   notes/stv/20220216-test-deploy-01.txt-
    >   notes/stv/20220216-test-deploy-01.txt-    # Restart Zeppelin
    >   notes/stv/20220216-test-deploy-01.txt-    time \
    >   notes/stv/20220216-test-deploy-01.txt-        /deployments/hadoop-yarn/bin/restart-zeppelin.sh
    >   notes/stv/20220216-test-deploy-01.txt-
    >   notes/stv/20220216-test-deploy-01.txt-    time \
    >   notes/stv/20220216-test-deploy-01.txt:        /deployments/hadoop-yarn/bin/run-tests.sh \
    >   notes/stv/20220216-test-deploy-01.txt-            "${cloudname:?}" \
    >   notes/stv/20220216-test-deploy-01.txt-            "${configname:?}" \

    #
    # Summary - total of 9 test runs.
    #

    >   num_users=1
    >   concurrent=False
    >   test_level=quick
    >
    >   num_users=1
    >   concurrent=False
    >   test_level="basic"
    >
    >   num_users=3
    >   concurrent=True
    >   test_level="multiuser"
    >
    >   num_users=1
    >   concurrent=False
    >   test_level="full"
    >
    >   num_users=1
    >   concurrent=False
    >   test_level=quick
    >
    >   num_users=1
    >   concurrent=False
    >   test_level="basic"
    >
    >   num_users=3
    >   concurrent=True
    >   test_level="multiuser"
    >
    >   num_users=1
    >   concurrent=False
    >   test_level="full"
    >
    >   num_users=1
    >   concurrent=False
    >   test_level="full"

    #
    # Summary^2
    #

    >   quick       1   false
    >   basic       1   false
    >   multiuser   3   true
    >   full        1   false
    >   quick       1   false
    >   basic       1   false
    >   multiuser   3   true
    >   full        1   false
    >   full        1   false

    #
    # Summary^3
    #

    >   quick       1   run twice
    >   basic       1   run twice
    >   multiuser   3   run twice
    >   full        1   run three times

    #
    # Looks like that is it.
    #

    #
    # Why do we have separate params for concurrent and num_users ?
    #
    # Looks like with concurrent=false it will ignore the num_users and just run a single test.
    # https://github.com/wfau/aglais-testing/blob/f9d60969521f74bd12de37508953ce23f54ee0f2/aglais_benchmark/aglais_benchmark.py#L126-L135
    #
    # What happens if we set concurrent = true and num_users = 1 ?
    # Looking at the code my guess it will just run one copy of the test.
    # So the concurrent flag is redundant ?
    # Actually it is defaulted to false in about three places, so we do need to set it.
    #

    #
    # What is the difference between [quick, basic, full] and [multiuser] ?
    # In the code, not a lot, just a different echo statement.
    # https://github.com/wfau/aglais/blob/560da17dc540e28be2f32a140b77844664a3493a/deployments/hadoop-yarn/bin/run-tests.sh#L59-L88
    #
    # The only difference I can see is a different test config.
    # https://github.com/wfau/aglais/blob/master/deployments/zeppelin/test/config/multiuser.json
    #
    # In which case, why not run [quick, basic, full] concurrebtly and drop the [multiuser] option ?
    #

    #
    # Only one set of results for the full test in the notes.
    # The other two cases just say '# Running...'
    #


# -----------------------------------------------------
# Run quick test, single user.
#[root@ansibler]

    numusers=1
    concurrent=false
    testlevel=quick

    time \
        /deployments/hadoop-yarn/bin/restart-zeppelin.sh

    time \
        /deployments/hadoop-yarn/bin/run-tests.sh \
            "${cloudname:?}"  \
            "${configname:?}" \
            "${testlevel:?}"  \
	        "${concurrent:?}" \
	        "${numusers:?}"  \
        | tee /tmp/test-quick-001.log

    >   real    2m2.049s
    >   user    1m41.808s
    >   sys     0m12.926s

    >   TASK [Run benchmarker] ..
    >   changed: [localhost] => {
    >       "changed": true,
    >       "cmd": "python3 /tmp/run-test.py | tee /tmp/test-result.json",
    >       "delta": "0:00:00.257800",
    >       "end": "2022-02-24 05:35:59.074600",
    >       "rc": 0,
    >       "start": "2022-02-24 05:35:58.816800",
    >       "stderr": "Traceback (
    >           most recent call last):
    >           File \"/tmp/run-test.py\", line 3, in <module>
    >           AglaisBenchmarker(\"/deployments/zeppelin/test/config/quick.json\", \"/tmp/\").run(concurrent=false, users=1)
    >           NameError: name 'false' is not defined",
    >       "stderr_lines": [
    >           "Traceback (most recent call last):",
    >           "  File \"/tmp/run-test.py\", line 3, in <module>",
    >           "    AglaisBenchmarker(\"/deployments/zeppelin/test/config/quick.json\", \"/tmp/\").run(concurrent=false, users=1)",
    >           "NameError: name 'false' is not defined"
    >           ],
    >       "stdout": "",
    >       "stdout_lines": []
    >       }
    >
    >   PLAY RECAP ..
    >   localhost : ok=12   changed=9    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0

    #
    # So the test fails becase .. 'false' not 'False' ?
    # and the Ansible task completes because the test doesn't retirn an error code.
    #


# -----------------------------------------------------
# Run quick test, single user.
#[root@ansibler]

    numusers=1
    concurrent=False
    testlevel=quick

    time \
        /deployments/hadoop-yarn/bin/restart-zeppelin.sh

    time \
        /deployments/hadoop-yarn/bin/run-tests.sh \
            "${cloudname:?}"  \
            "${configname:?}" \
            "${testlevel:?}"  \
	        "${concurrent:?}" \
	        "${numusers:?}"  \
        | tee /tmp/test-quick-002.log

    >   real    2m28.833s
    >   user    0m49.166s
    >   sys     0m7.065s

    >   TASK [Run benchmarker] ..
    >   changed: [localhost] => {
    >       "changed": true,
    >       "cmd": "python3 /tmp/run-test.py | tee /tmp/test-result.json",
    >       "delta": "0:01:57.797626",
    >       "end": "2022-02-24 05:43:34.691528",
    >       "rc": 0,
    >       "start": "2022-02-24 05:41:36.893902",
    >       "stderr": "",
    >       "stderr_lines": [],
    >       "stdout": "
    >           Test started [Single User]
    >           Test completed! (117.52 seconds)
    >           ------------ Test Result: [SUCCESS] ------------
    >           [
    >               {
    >               'SetUp': {
    >                   'totaltime': '43.55',
    >                   'status': 'SUCCESS',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   },
    >               'Mean_proper_motions_over_the_sky': {
    >                   'totaltime': '47.97',
    >                   'status': 'SUCCESS',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   },
    >               'Source_counts_over_the_sky.json': {
    >                   'totaltime': '18.06',
    >                   'status': 'SUCCESS',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   },
    >               'Library_Validation.json': {
    >                   'totaltime': '7.94',
    >                   'status': 'SUCCESS',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   }
    >               }
    >           ]",
    >       "stdout_lines": [
    >           "Test started [Single User]",
    >           "Test completed! (117.52 seconds)",
    >           "------------ Test Result: [SUCCESS] ------------",
    >           "[{....}]"
    >           ]
    >       }
    >
    >   PLAY RECAP ..
    >   localhost                  : ok=12   changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


# -----------------------------------------------------
# Recover the test results.
#[root@ansibler]

    #
    # Ansible script will put the test result in the same file each time.
    # Multiple tests will overwrite the same file.
    # Python code doesn't output valid JSON.
    #

    cat /tmp/test-result.json

    >   Test started [Single User]
    >   Test completed! (117.52 seconds)
    >   ------------ Test Result: [SUCCESS] ------------
    >   [{'SetUp': {'totaltime': '43.55', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}, 'Mean_proper_motions_over_the_sky': {'totaltime': '47.97', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}, 'Source_counts_over_the_sky.json': {'totaltime': '18.06', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}, 'Library_Validation.json': {'totaltime': '7.94', 'status': 'SUCCESS', 'msg': '', 'valid': 'TRUE'}}]


    #
    # Post process the test result into valid JSON.
    sed "
        1,3 d
        s/'\([0-9.]*\)'/\1/g
        s/:[[:space:]],/: '',/g
        s/'/\"/g
        " \
        '/tmp/test-result.json' \
    | jq '.' \
    | tee /tmp/test-quick-002.json

    >   [
    >     {
    >       "SetUp": {
    >         "totaltime": 43.55,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 47.97,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 18.06,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 7.94,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       }
    >     }
    >   ]


# -----------------------------------------------------
# Run the same thing with concurrent=True, just to check.
#[root@ansibler]

    numusers=1
    concurrent=True
    testlevel=quick

    time \
        /deployments/hadoop-yarn/bin/restart-zeppelin.sh

    time \
        /deployments/hadoop-yarn/bin/run-tests.sh \
            "${cloudname:?}"  \
            "${configname:?}" \
            "${testlevel:?}"  \
	        "${concurrent:?}" \
	        "${numusers:?}"  \
        | tee /tmp/test-quick-003.log

    >   real    2m15.234s
    >   user    0m47.685s
    >   sys     0m7.030s

    >   TASK [Run benchmarker] ..
    >   changed: [localhost] => {
    >       "changed": true,
    >       "cmd": "python3 /tmp/run-test.py | tee /tmp/test-result.json",
    >       "delta": "0:01:43.167917",
    >       "end": "2022-02-24 05:58:37.479201",
    >       "rc": 0,
    >       "start": "2022-02-24 05:56:54.311284",
    >       "stderr": "",
    >       "stderr_lines": [],
    >       "stdout": "
    >           Test started [Multi User]
    >           Test completed! (102.84 seconds)
    >           ------------ Test Result: [SUCCESS] ------------
    >           [
    >               {
    >               'SetUp': {
    >                   'totaltime': '41.24',
    >                   'status': 'SUCCESS',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   },
    >               'Mean_proper_motions_over_the_sky': {
    >                   'totaltime': '39.48',
    >                   'status': 'SUCCESS',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   },
    >               'Source_counts_over_the_sky.json': {
    >                   'totaltime': '14.07',
    >                   'status': 'SUCCESS',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   },
    >               'Library_Validation.json': {
    >                   'totaltime': '7.95',
    >                   'status': 'SUCCESS',
    >                   'msg': '',
    >                   'valid': 'TRUE'
    >                   }
    >               }
    >           ]",
    >           "stdout_lines": [
    >               "Test started [Multi User]",
    >               "Test completed! (102.84 seconds)",
    >               "------------ Test Result: [SUCCESS] ------------",
    >               "[{....}]"
    >               ]
    >           }
    >
    >   PLAY RECAP ..
    >   localhost                  : ok=12   changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


    #
    # Post process the test result into valid JSON.
    sed "
        1,3 d
        s/'\([0-9.]*\)'/\1/g
        s/:[[:space:]],/: '',/g
        s/'/\"/g
        " \
        '/tmp/test-result.json' \
    | jq '.' \
    | tee /tmp/test-quick-003.json

    >   [
    >     {
    >       "SetUp": {
    >         "totaltime": 41.24,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 39.48,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 14.07,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 7.95,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       }
    >     }
    >   ]


# -----------------------------------------------------
# Run the same thing with two users.
#[root@ansibler]

    numusers=2
    concurrent=True
    testlevel=quick

    time \
        /deployments/hadoop-yarn/bin/restart-zeppelin.sh

    time \
        /deployments/hadoop-yarn/bin/run-tests.sh \
            "${cloudname:?}"  \
            "${configname:?}" \
            "${testlevel:?}"  \
	        "${concurrent:?}" \
	        "${numusers:?}"  \
        | tee /tmp/test-quick-004.log

    >   real    2m27.202s
    >   user    0m50.994s
    >   sys     0m6.845s

    >   TASK [Run benchmarker] ..
    >   changed: [localhost] => {
    >       "changed": true,
    >       "cmd": "python3 /tmp/run-test.py | tee /tmp/test-result.json",
    >       "delta": "0:01:57.892374",
    >       "end": "2022-02-24 06:09:08.038958",
    >       "rc": 0,
    >       "start": "2022-02-24 06:07:10.146584",
    >       "stderr": "",
    >       "stderr_lines": [],
    >       "stdout": "Test started [Multi User]\nTest completed! (117.62 seconds)\n------------ Test Result: [SLOW] ------------\n[{....}]",
    >       "stdout_lines": [
    >           "Test started [Multi User]",
    >           "Test completed! (117.62 seconds)",
    >           "------------ Test Result: [SLOW] ------------",
    >           "[{....}]"
    >           ]
    >       }
    >
    >   PLAY RECAP ..
    >   localhost                  : ok=12   changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


    #
    # Post process the test result into valid JSON.
    sed "
        1,3 d
        s/'\([0-9.]*\)'/\1/g
        s/:[[:space:]],/: '',/g
        s/'/\"/g
        " \
        '/tmp/test-result.json' \
    | jq '.' \
    | tee /tmp/test-quick-004.json

    >   [
    >     {
    >       "SetUp": {
    >         "totaltime": 42.49,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 47.54,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 12.89,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 9.12,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       }
    >     },
    >     {
    >       "SetUp": {
    >         "totaltime": 50.06,
    >         "status": "SLOW",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 46.18,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 12.73,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 8.63,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       }
    >     }
    >   ]

    #
    # The results are in a plain array,
    # with no indication of which user account generated which result.
    # Throwing away useful information.
    #


# -----------------------------------------------------
# Run the same thing with concurrent=False, just to check.
#[root@ansibler]

    numusers=2
    concurrent=False
    testlevel=quick

    time \
        /deployments/hadoop-yarn/bin/restart-zeppelin.sh

    time \
        /deployments/hadoop-yarn/bin/run-tests.sh \
            "${cloudname:?}"  \
            "${configname:?}" \
            "${testlevel:?}"  \
	        "${concurrent:?}" \
	        "${numusers:?}"  \
        | tee /tmp/test-quick-005.log

    >   real    2m2.205s
    >   user    0m44.874s
    >   sys     0m6.874s

    >   TASK [Run benchmarker] ..
    >   changed: [localhost] => {
    >       "changed": true,
    >       "cmd": "python3 /tmp/run-test.py | tee /tmp/test-result.json",
    >       "delta": "0:01:29.726231",
    >       "end": "2022-02-24 06:18:32.440038",
    >       "rc": 0,
    >       "start": "2022-02-24 06:17:02.713807",
    >       "stderr": "",
    >       "stderr_lines": [],
    >       "stdout": "....",
    >       "stdout_lines": [
    >           "Test started [Single User]",
    >           "Test completed! (89.46 seconds)",
    >           "------------ Test Result: [SUCCESS] ------------",
    >           "[{....}]"
    >           ]
    >       }
    >
    >   PLAY RECAP ..
    >   localhost                  : ok=12   changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


    #
    # Post process the test result into valid JSON.
    sed "
        1,3 d
        s/'\([0-9.]*\)'/\1/g
        s/:[[:space:]],/: '',/g
        s/'/\"/g
        " \
        '/tmp/test-result.json' \
    | jq '.' \
    | tee /tmp/test-quick-005.json

    >   [
    >     {
    >       "SetUp": {
    >         "totaltime": 40.48,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 30.21,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 10.87,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 7.89,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       }
    >     }
    >   ]

    #
    # So, numusers=2 and concurrent=False is the same as numusers=1.
    # Why do we have two params ?
    #


# -----------------------------------------------------
# Run the same thing with three users.
#[root@ansibler]

    numusers=3
    concurrent=True
    testlevel=quick
    testdate=$(date '+%Y%m%d-%H%M%S')

    time \
        /deployments/hadoop-yarn/bin/restart-zeppelin.sh

    time \
        /deployments/hadoop-yarn/bin/run-tests.sh \
            "${cloudname:?}"  \
            "${configname:?}" \
            "${testlevel:?}"  \
	        "${concurrent:?}" \
	        "${numusers:?}"  \
        | tee /tmp/test-${testlevel:?}-${testdate:?}.log

    sed "
        1,3 d
        s/'\([0-9.]*\)'/\1/g
        s/:[[:space:]],/: '',/g
        s/'/\"/g
        " \
        '/tmp/test-result.json' \
    | jq '.' \
    | tee /tmp/test-${testlevel:?}-${testdate:?}.json

    >   real    4m2.819s
    >   user    1m17.325s
    >   sys     0m10.356s

    >   ....
    >   ....

    >   [
    >     {
    >       "SetUp": {
    >         "totaltime": 52.39,
    >         "status": "SLOW",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 31,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 12.75,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 7.71,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       }
    >     },
    >     {
    >       "SetUp": {
    >         "totaltime": 155.16,
    >         "status": "SLOW",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 36.22,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 10.63,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 8.76,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       }
    >     },
    >     {
    >       "SetUp": {
    >         "totaltime": 42.74,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 33.3,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 12.94,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 7.84,
    >         "status": "SUCCESS",
    >         "msg": "",
    >         "valid": "TRUE"
    >       }
    >     }
    >   ]

    #
    # What information do the valid and msg fields provide ?
    # It looks like valid is always TRUE and msg is always blank.
    #


# -----------------------------------------------------
# Summary of results.
# -----------------------------------------------------
# Quick test, single user.
#[root@ansibler]

    numusers=1
    concurrent=False
    testlevel=quick

    >   real    2m28.833s
    >   user    0m49.166s
    >   sys     0m7.065s

    >   [
    >     {
    >       "SetUp": {
    >         "totaltime": 43.55,
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 47.97,
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 18.06,
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 7.94,
    >       }
    >     }
    >   ]


# -----------------------------------------------------
# Quick test, single user, concurrent=True, just to check.
#[root@ansibler]

    numusers=1
    concurrent=True
    testlevel=quick

    >   real    2m15.234s
    >   user    0m47.685s
    >   sys     0m7.030s

    >   [
    >     {
    >       "SetUp": {
    >         "totaltime": 41.24,
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 39.48,
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 14.07,
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 7.95,
    >       }
    >     }
    >   ]


# -----------------------------------------------------
# Quick test, two users.
#[root@ansibler]

    numusers=2
    concurrent=True
    testlevel=quick

    >   real    2m27.202s
    >   user    0m50.994s
    >   sys     0m6.845s

    >   [
    >     {
    >       "SetUp": {
    >         "totaltime": 42.49,
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 47.54,
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 12.89,
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 9.12,
    >       }
    >     },
    >     {
    >       "SetUp": {
    >         "totaltime": 50.06,
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 46.18,
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 12.73,
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 8.63,
    >       }
    >     }
    >   ]


# -----------------------------------------------------
# Quick test, two users concurrent=False, just to check.
#[root@ansibler]

    numusers=2
    concurrent=False
    testlevel=quick

    >   real    2m2.205s
    >   user    0m44.874s
    >   sys     0m6.874s

    >   [
    >     {
    >       "SetUp": {
    >         "totaltime": 40.48,
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 30.21,
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 10.87,
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 7.89,
    >       }
    >     }
    >   ]


# -----------------------------------------------------
# Quick test, three users.
#[root@ansibler]

    numusers=3
    concurrent=True
    testlevel=quick
    testdate=$(date '+%Y%m%d-%H%M%S')

    >   real    4m2.819s
    >   user    1m17.325s
    >   sys     0m10.356s

    >   [
    >     {
    >       "SetUp": {
    >         "totaltime": 52.39,
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 31,
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 12.75,
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 7.71,
    >       }
    >     },
    >     {
    >       "SetUp": {
    >         "totaltime": 155.16,
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 36.22,
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 10.63,
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 8.76,
    >       }
    >     },
    >     {
    >       "SetUp": {
    >         "totaltime": 42.74,
    >       },
    >       "Mean_proper_motions_over_the_sky": {
    >         "totaltime": 33.3,
    >       },
    >       "Source_counts_over_the_sky.json": {
    >         "totaltime": 12.94,
    >       },
    >       "Library_Validation.json": {
    >         "totaltime": 7.84,
    >       }
    >     }
    >   ]

    #
    # With three users, the SetUp notebook takes a lot longer, 155s compared to an average of 44s for the other runs.
    # This matches the reports from Stelios that the current config will run two users concurently, and make the rest wait in the queue.
    #

    #
    # Ideally I'd like to run more users, but the current test system has a fixed limit of 3 concurent test runs.
    # From reading the notes, we would need to manually add more user accounts to run more concurent tests.
    #

    #
    # For this deployment ..
    # Add the user accounts and notebooks.
    # Add the Grafana metrics.
    # Login as normal user and check things ar as expected.
    # Run a final full test.
    #





