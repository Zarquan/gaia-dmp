#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2024, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: []
#

    Target:

        Slack message from Greg:

            "I've mentioned the matter to StackHPC, who've asked if you could
            regenerate any application credentials you are using and retry please?"

        Unlikley - if the credentials were wrong/broken we would get errors creating
        things. I don't see how bad credentials would effect network traffic
        _after_ everything has been created.

        Let's try it anyway.

    Result:

        Work in progress ...

# -----------------------------------------------------
# Update our app credentials.
#[user@desktop]

    gedit "${HOME}/clouds.yaml" &

        clouds:
          somerville-jade:
            auth:
              auth_url: https://somerville.ed.ac.uk:5000
    ~         application_credential_id: "...."
    ~         application_credential_secret: "...."


# -----------------------------------------------------
# Checkout the corresponding commit.
# https://github.com/wfau/gaia-dmp/commit/c90884c249b189d4f6e162481d01dfcbb4b4b0b6
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        git checkout 'c90884c'

    >   Note: switching to 'c90884c'.
    >
    >   You are in 'detached HEAD' state. You can look around, make experimental
    >   changes and commit them, and you can discard any commits you make in this
    >   state without impacting any branches by switching back to a branch.
    >   ....
    >   ....


# -----------------------------------------------------
# Run our local client.
#[user@laptop]

    source "${HOME:?}/aglais.env"
    export PATH=${PATH}:${AGLAIS_CODE}/bin

    agclient jade

    >   ....
    >   ....


# -----------------------------------------------------
# Delete and create everything.
#[root@ansibler]

    export cloudsite=somerville-jade

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    ansible-playbook \
        --inventory 'bootstrap,' \
        '/deployments/cluster-api/bootstrap/ansible/00-create-all.yml'

    >   ....
    >   ....
    >   PLAY RECAP *******************************************************************************************************
    >   bootstrap                  : ok=57   changed=44   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   localhost                  : ok=35   changed=26   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


# -----------------------------------------------------
# Check the deployment logs.
#[root@ansibler]

    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            get deployments \
                --all-namespaces
        '

    >   NAMESPACE                           NAME                                            READY   UP-TO-DATE   AVAILABLE   AGE
    >   capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager       1/1     1            1           8m8s
    >   capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager   1/1     1            1           8m3s
    >   capi-system                         capi-controller-manager                         1/1     1            1           8m11s
    >   capo-system                         capo-controller-manager                         1/1     1            1           7m56s
    >   cert-manager                        cert-manager                                    1/1     1            1           9m58s
    >   cert-manager                        cert-manager-cainjector                         1/1     1            1           9m59s
    >   cert-manager                        cert-manager-webhook                            1/1     1            1           9m57s
    >   default                             cluster-api-addon-provider                      1/1     1            1           7m42s
    >   default                             somerville-jade-20240118-work-autoscaler        0/1     1            0           5m59s
    >   kube-system                         coredns                                         2/2     2            2           10m
    >   local-path-storage                  local-path-provisioner                          1/1     1            1           10m


    ssh bootstrap -t \
        '
        source loadconfig
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            logs \
                --follow \
                --namespace capi-system \
                deployments/capi-controller-manager
        '

    >   ....
    >   I0118 06:20:09.202829       1 machine_controller_noderef.go:58] "Waiting for infrastructure provider to report spec.providerID" controller="machine" controllerGroup="cluster.x-k8s.io" controllerKind="Machine" Machine="default/somerville-jade-20240118-work-md-0-7nx46-6pmbc" namespace="default" name="somerville-jade-20240118-work-md-0-7nx46-6pmbc" reconcileID="c39275c3-ffaf-4621-b1f2-b4f2cb60427b" MachineSet="default/somerville-jade-20240118-work-md-0-7nx46" MachineDeployment="default/somerville-jade-20240118-work-md-0" Cluster="default/somerville-jade-20240118-work" OpenStackMachine="default/somerville-jade-20240118-work-md-0-d980a92e-fhfhg"
    >   I0118 06:20:09.203855       1 machine_controller_phases.go:280] "Infrastructure provider has completed machine infrastructure provisioning and reports status.ready" controller="machine" controllerGroup="cluster.x-k8s.io" controllerKind="Machine" Machine="default/somerville-jade-20240118-work-md-0-7nx46-bd6lk" namespace="default" name="somerville-jade-20240118-work-md-0-7nx46-bd6lk" reconcileID="8134458d-93e1-45a0-801d-7e69ce111dbe" MachineSet="default/somerville-jade-20240118-work-md-0-7nx46" MachineDeployment="default/somerville-jade-20240118-work-md-0" Cluster="default/somerville-jade-20240118-work" OpenStackMachine="default/somerville-jade-20240118-work-md-0-d980a92e-s9zb4"
    >   I0118 06:20:09.279156       1 recorder.go:104] "events: Machine default/somerville-jade-20240118-work-md-0/somerville-jade-20240118-work-md-0-7nx46-6pmbc/ has unhealthy node " type="Normal" object={"kind":"Machine","namespace":"default","name":"somerville-jade-20240118-work-md-0-7nx46-6pmbc","uid":"1995a0e6-8890-4e9a-9533-ce694cb714d0","apiVersion":"cluster.x-k8s.io/v1beta1","resourceVersion":"3091"} reason="DetectedUnhealthy"
    >   I0118 06:20:09.279185       1 recorder.go:104] "events: Machine default/somerville-jade-20240118-work-md-0/somerville-jade-20240118-work-md-0-7nx46-6tfkd/ has unhealthy node " type="Normal" object={"kind":"Machine","namespace":"default","name":"somerville-jade-20240118-work-md-0-7nx46-6tfkd","uid":"b61b7067-5858-40f6-83f5-0c764409d3a1","apiVersion":"cluster.x-k8s.io/v1beta1","resourceVersion":"3013"} reason="DetectedUnhealthy"
    >   ....

    >   ....
    >   I0118 06:29:39.001945       1 machinehealthcheck_controller.go:433] "Target has failed health check, marking for remediation" controller="machinehealthcheck" controllerGroup="cluster.x-k8s.io" controllerKind="MachineHealthCheck" MachineHealthCheck="default/somerville-jade-20240118-work-md-0" namespace="default" name="somerville-jade-20240118-work-md-0" reconcileID="4f350653-78a7-4c2a-9d36-88c575d44990" Cluster="default/somerville-jade-20240118-work" target="default/somerville-jade-20240118-work-md-0/somerville-jade-20240118-work-md-0-7nx46-6tfkd/" reason="NodeStartupTimeout" message="Node failed to report startup in 10m0s"
    >   I0118 06:29:39.006152       1 machinehealthcheck_controller.go:433] "Target has failed health check, marking for remediation" controller="machinehealthcheck" controllerGroup="cluster.x-k8s.io" controllerKind="MachineHealthCheck" MachineHealthCheck="default/somerville-jade-20240118-work-control-plane" namespace="default" name="somerville-jade-20240118-work-control-plane" reconcileID="ac0d9d8b-275f-4d24-9b6d-164dea0b045d" Cluster="default/somerville-jade-20240118-work" target="default/somerville-jade-20240118-work-control-plane/somerville-jade-20240118-work-control-plane-pfrpl/" reason="NodeStartupTimeout" message="Node failed to report startup in 10m0s"
    >   I0118 06:29:39.064402       1 machineset_controller.go:1015] "Deleting Machine default/somerville-jade-20240118-work-md-0-7nx46-6tfkd because it was marked as unhealthy by the MachineHealthCheck controller" controller="machineset" controllerGroup="cluster.x-k8s.io" controllerKind="MachineSet" MachineSet="default/somerville-jade-20240118-work-md-0-7nx46" namespace="default" name="somerville-jade-20240118-work-md-0-7nx46" reconcileID="dba60ec1-c697-400a-add6-27a5c91e3ec9" MachineDeployment="default/somerville-jade-20240118-work-md-0" Cluster="default/somerville-jade-20240118-work"
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------
# Watch the cluster status.
#[root@ansibler]

    podman exec \
        --tty \
        --interactive \
        ansibler-jade \
            ssh bootstrap -t \
                '
                source loadconfig
                watch \
                    clusterctl \
                        --kubeconfig "${kindclusterconf:?}" \
                        describe cluster \
                            "${workclustername:?}"
                '

    >   NAME                                                                              READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/somerville-jade-20240118-work                                             False  Warning   ScalingUp                    7m47s  Scaling up control plane to 3 replicas (actual 1)
    >   ├─ClusterInfrastructure - OpenStackCluster/somerville-jade-20240118-work
    >   ├─ControlPlane - KubeadmControlPlane/somerville-jade-20240118-work-control-plane  False  Warning   ScalingUp                    7m47s  Scaling up control plane to 3 replicas (actual 1)
    >   │ └─Machine/somerville-jade-20240118-work-control-plane-pfrpl                     True                                          7m50s
    >   └─Workers
    >     └─MachineDeployment/somerville-jade-20240118-work-md-0                          False  Warning   WaitingForAvailableMachines  9m49s  Minimum availability requires 2 replicas, current 0 available
    >       └─3 Machines...                                                               True                                          5m25s  See somerville-jade-20240118-work-md-0-7nx46-6pmbc, somerville-jade-20240118-work-md-0-7nx46-6tfkd, ...


    >   NAME                                                                              READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/somerville-jade-20240118-work                                             False  Warning   ScalingUp                    15m    Scaling up control plane to 3 replicas (actual 1)
    >   ├─ClusterInfrastructure - OpenStackCluster/somerville-jade-20240118-work
    >   ├─ControlPlane - KubeadmControlPlane/somerville-jade-20240118-work-control-plane  False  Warning   ScalingUp                    15m    Scaling up control plane to 3 replicas (actual 1)
    >   │ └─Machine/somerville-jade-20240118-work-control-plane-pfrpl                     False  Warning   NodeStartupTimeout           3m12s  Node failed to report startup in 10m0s
    >   └─Workers
    >     └─MachineDeployment/somerville-jade-20240118-work-md-0                          False  Warning   WaitingForAvailableMachines  17m    Minimum availability requires 2 replicas, current 0 available
    >       └─3 Machines...                                                               True                                          2m45s  See somerville-jade-20240118-work-md-0-7nx46-bwg9x, somerville-jade-20240118-work-md-0-7nx46-dnzdv, ...

