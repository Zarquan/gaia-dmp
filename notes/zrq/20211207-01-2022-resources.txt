#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#


    2021
        cores   336 p-cores (6 machines)

        disc    200G SS-DAS / machine = 1.2Tbyte

    2022
        cores   495 + 3.5G/p-core (8.8 machines)
        cores   165 + 10G/p-core  (2.9 machines)

                495+165 = 660 p-cores (~12 machines)

        disk    144T 72T+72T
                2T SS-DAS / machine = 24T

    2023
        cores   990 + 3.5G/p-core (17.6 machines)
        cores   330 + 10G/p-core  (5.9 machines)

                990+330 = 1320 p-cores (~24 machines)

        disk    144T 72T+72T
                2T SS-DAS / machine = 48T

    2024
        cores   1980 + 3.5G/p-core (35.3 machines)
        cores   660 + 10G/p-core (11.8 machines)

                1980+660 = 2640 p-cores (~48 machines)

        disk    144T 72T+72T
                2T SS-DAS / machine = 96T

    2025
        cores   1980 + 3.5G/p-core (35.3 machines)
        cores   660 + 10G/p-core (11.8 machines)

                1980+660 = 2640 p-cores (~48 machines)

        disk    144T 72T+72T
                2T SS-DAS / machine = 96T





Current 2021 allocation

6 x CascadeLake machines
112 hyperthreaded cores,
192G RAM (1.7G/core)
800G SSD (*) most of which is allocated to the OS and not all available to VMs

112*6 = 672 cores

Request 2022-2024

Keep 3 machines for test/dev
Scale the 3 live machines to fit the x10 data

(3 + 30) CCLake equivalent machines
112*(3 + 30) = 3696 vcores (1848 pcores)

25 with 192G of memory and 8 high memory nodes with 512G of memory
(25*192) + (8*512) = 8896G memory

72Tbytes of storage on the Ceph storage system
72Tbytes of solid state storage directly attached to the compute nodes
equvalent to approx 2 Tbytes of DAS SDD on each compute node














Our IRIS 2022 RSAP submission asked for 144Tbytes of storage, half (72Tbyte) as space on a shared CephFS and half (72Tbyte) as direct attached storage provisioned through high speed (SSD) storage directly attached to the hypervisor compute nodes.

The feedback from the the RSAP team asked for further details:

"Is this required as a single volume, or would simply having a ~2TB SSD in each compute node be sufficient? Is there a preferred technical solution, or any minimal I/O specifications? Or is a desired solution already in place, and you are just asking for a 72TB “slice” of that storage? My apologies if I missed this point within the text."

Before I send our reply I wanted to pass this by you and get your advice.

Based on what we have learned about the high bandwidth requirements of Machine learning applications the intent of our request was to spread the direct attached storage across the pool of compute nodes. So having ~2TB SSD in each compute node would indeed be the best solution.


