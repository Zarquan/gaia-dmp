#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Initial bootstrap K8s cluster from nothing.

    Result:

        Work in progress ...


# -----------------------------------------------------
# Check which platform is live.
#[user@desktop]

    ssh fedora@live.gaia-dmp.uk \
        '
        date
        hostname
        '

    >   Tue  4 Apr 15:41:25 UTC 2023
    >   iris-gaia-green-20230308-zeppelin


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    #
    # Live is green, selecting red for the deployment.
    #

    source "${HOME:?}/aglais.env"

    agcolour=red

    clientname=ansibler-${agcolour}
    cloudname=iris-gaia-${agcolour}

    podman run \
        --rm \
        --tty \
        --interactive \
        --name     "${clientname:?}" \
        --hostname "${clientname:?}" \
        --env "cloudname=${cloudname:?}" \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK:?}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        ghcr.io/wfau/atolmis/ansible-client:2022.07.25 \
        bash

    >   ....
    >   ....


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

    >   real    2m4.760s
    >   user    0m56.294s
    >   sys     0m6.073s


# -----------------------------------------------------
# Add YAML editor role to our client container.
#[root@ansibler]

    ansible-galaxy install kwoodson.yedit

    >   Starting galaxy role install process
    >   - downloading role 'yedit', owned by kwoodson
    >   - downloading role from https://github.com/kwoodson/ansible-role-yedit/archive/master.tar.gz
    >   - extracting kwoodson.yedit to /root/.ansible/roles/kwoodson.yedit
    >   - kwoodson.yedit (master) was installed successfully


# -----------------------------------------------------
# Create our deployment settings.
#[root@ansibler]

    deployname=${cloudname:?}-$(date '+%Y%m%d')
    deploydate=$(date '+%Y%m%dT%H%M%S')

    statusyml='/opt/aglais/aglais-status.yml'
    if [ ! -e "$(dirname ${statusyml})" ]
    then
        mkdir "$(dirname ${statusyml})"
    fi
    rm -f "${statusyml}"
    touch "${statusyml}"

    yq eval \
        --inplace \
        "
        .aglais.deployment.type = \"cluster-api\"   |
        .aglais.deployment.name = \"${deployname}\" |
        .aglais.deployment.date = \"${deploydate}\" |
        .aglais.openstack.cloud.name = \"${cloudname}\"
        " "${statusyml}"


# -----------------------------------------------------
# Create our bootstrap node.
#[root@ansibler]

    inventory=/deployments/cluster-api/bootstrap/ansible/config/inventory.yml

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/01-create-keypair.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/02-create-network.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/03-create-bootstrap.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/04-local-config.yml'

    >   ....
    >   ....
    >   PLAY RECAP **********************************************************************************************************
    >   localhost                  : ok=2    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   ....
    >   ....
    >   PLAY RECAP **********************************************************************************************************
    >   localhost                  : ok=4    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   ....
    >   ....
    >   PLAY RECAP **********************************************************************************************************
    >   localhost                  : ok=7    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   ....
    >   ....
    >   PLAY RECAP **********************************************************************************************************
    >   localhost                  : ok=4    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


# -----------------------------------------------------
# Check our local config.
#[root@ansibler]

    cat /opt/aglais/aglais-status.yml

    >   aglais:
    >     deployment:
    >       date: 20230404T155040
    >       name: iris-gaia-red-20230404
    >       type: cluster-api
    >     openstack:
    >       cloud:
    >         name: iris-gaia-red
    >       keypairs:
    >         team:
    >           fingerprint: 2e:84:98:98:df:70:06:0e:4c:ed:bd:d4:d6:6b:eb:16
    >           id: iris-gaia-red-20230404-keypair
    >           name: iris-gaia-red-20230404-keypair
    >       networks:
    >         internal:
    >           network:
    >             id: 05750b83-08c6-4ad4-838b-4a47fbcf29d8
    >             name: iris-gaia-red-20230404-internal-network
    >           router:
    >             id: 4ce12070-96ac-44a7-b60e-c38f0dfe68aa
    >             name: iris-gaia-red-20230404-internal-router
    >           subnet:
    >             cidr: 10.10.0.0/16
    >             id: 043d114c-86f5-4267-8ec8-97a3e028bafb
    >             name: iris-gaia-red-20230404-internal-subnet
    >       servers:
    >         bootstrap:
    >           float:
    >             external: 128.232.227.17
    >             id: 63903d51-6e08-4bf6-82c8-460483b3496d
    >             internal: 10.10.3.127
    >           server:
    >             address:
    >               ipv4: 10.10.3.127
    >             flavor:
    >               name: gaia.vm.cclake.2vcpu
    >             hostname: bootstrap
    >             id: bfbbb840-ba12-4ff6-93ff-1a5afa16c962
    >             image:
    >               id: e5c23082-cc34-4213-ad31-ff4684657691
    >               name: Fedora-34.1.2
    >             name: iris-gaia-red-20230404-bootstrap


# -----------------------------------------------------
# SSH test.
#[root@ansibler]

    ssh bootstrap \
        '
        date
        hostname
        '

    >   Tue Apr  4 15:55:48 UTC 2023
    >   iris-gaia-red-20230404-bootstrap


# -----------------------------------------------------
# Login to the bootstrap node.
#[root@ansibler]

    ssh bootstrap

    >   ....
    >   ....


# -----------------------------------------------------
# Install Docker.
# https://docs.docker.com/engine/install/fedora/#install-using-the-repository
#[fedora@bootstrap]

    sudo dnf -y install dnf-plugins-core

    sudo dnf config-manager \
        --add-repo \
        https://download.docker.com/linux/fedora/docker-ce.repo

    >   ....
    >   ....
    >   Adding repo from: https://download.docker.com/linux/fedora/docker-ce.repo


    sudo dnf install \
        -y \
        docker-ce \
        docker-ce-cli \
        containerd.io \
        docker-compose-plugin

    >   ....
    >   ....
    >   Installed:
    >     ....
    >     ....
    >     docker-ce-3:20.10.17-3.fc34.x86_64


# -----------------------------------------------------
# Start the Docker service.
#[fedora@bootstrap]

    sudo systemctl start docker

    >   ....
    >   ....


# -----------------------------------------------------
# Install kubectl.
# https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management
#[fedora@bootstrap]

cat > '/tmp/kubernetes.repo' << EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

    sudo chown 'root:root' '/tmp/kubernetes.repo'
    sudo mv '/tmp/kubernetes.repo' '/etc/yum.repos.d/kubernetes.repo'

    sudo dnf install -y 'kubectl'

    >   ....
    >   ....
    >   Installed:
    >     kubectl-1.26.3-0.x86_64


# -----------------------------------------------------
# Install kind.
# https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries
#[fedora@bootstrap]

    kindversion=0.17.0
    kindbinary=kind-${kindversion:?}
    kindtemp=/tmp/${kindbinary:?}

    curl --location --output "${kindtemp:?}" "https://kind.sigs.k8s.io/dl/v${kindversion:?}/kind-linux-amd64"
    pushd /usr/local/bin
        sudo mv "${kindtemp:?}" .
        sudo chown 'root:root' "${kindbinary:?}"
        sudo chmod 'u=rwx,g=rx,o=rx' "${kindbinary:?}"
        sudo ln -s "${kindbinary:?}" 'kind'
    popd

    >   ....
    >   ....


# -----------------------------------------------------
# Create a cluster, with logs.
# https://github.com/kubernetes-sigs/kind/pull/2478#issuecomment-1214656908
#[fedora@bootstrap]

    kind create cluster --retain

    >   ERROR: failed to create cluster: failed to list nodes: command "docker ps -a --filter label=io.x-k8s.kind.cluster=kind --format '{{.Names}}'" failed with error: exit status 1
    >   Command Output: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json?all=1&filters=%7B%22label%22%3A%7B%22io.x-k8s.kind.cluster%3Dkind%22%3Atrue%7D%7D": dial unix /var/run/docker.sock: connect: permission denied

    # Fails because we aren't root.
    Try again as root.

    sudo kind create cluster --retain

    >   Creating cluster "kind" ...
    >    ✓ Ensuring node image (kindest/node:v1.25.3) 🖼
    >    ✓ Preparing nodes 📦
    >    ✓ Writing configuration 📜
    >    ✓ Starting control-plane 🕹️
    >    ✓ Installing CNI 🔌
    >    ✓ Installing StorageClass 💾
    >   Set kubectl context to "kind-kind"
    >   ....
    >   ....


# -----------------------------------------------------
# Start to explore ....
#[fedora@bootstrap]

    sudo kubectl cluster-info --context kind-kind

    >   Kubernetes control plane is running at https://127.0.0.1:46203
    >   CoreDNS is running at https://127.0.0.1:46203/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    >   ....
    >   ....

    # Sucks that we need to be root for all of this.
    # TODO Replace Docker with Podman and run our containers rootless.


# -----------------------------------------------------
# ....
#[fedora@bootstrap]

    sudo cat /root/.kube/config

    >   apiVersion: v1
    >   clusters:
    >   - cluster:
    >       certificate-authority-data: LS0tLS1C....0tLS0tCg==
    >       server: https://127.0.0.1:46203
    >     name: kind-kind
    >   contexts:
    >   - context:
    >       cluster: kind-kind
    >       user: kind-kind
    >     name: kind-kind
    >   current-context: kind-kind
    >   kind: Config
    >   preferences: {}
    >   users:
    >   - name: kind-kind
    >     user:
    >       client-certificate-data: LS0tLS1C....0tLS0tCg==
    >       client-key-data: LS0tLS1C....0tLS0tCg==

    # Lots of 'kind' things in all that ...

    >   clusters:
    >   - name: kind-kind
    >     ....
    >   contexts:
    >   - name: kind-kind
    >     ....
    >   ...
    >   users:
    >   - name: kind-kind


    # References from one 'kind' thing to all the others.

    >   ....
    >   contexts:
    >   - context:
    >       cluster: kind-kind
    >       user: kind-kind
    >     name: kind-kind
    >   current-context: kind-kind
    >   ....


    # .. but this is a different 'kind' of thing

    >   ....
    >   kind: Config
    >   ....


# -----------------------------------------------------
# ....
#[fedora@bootstrap]






# -----------------------------------------------------

    Back to StackHPC's Helm charts.
    https://github.com/stackhpc/capi-helm-charts

        Prerequisites

            First, you must set up a Cluster API management cluster with the OpenStack Infrastructure Provider installed.

            WARNING
            This chart depends on features in cluster-api-provider-openstack that are not yet in a release.
            StackHPC maintain custom builds of cluster-api-provider-openstack for use with this chart. You can find these in the StackHPC fork of cluster-api-provider-openstack.

            Addons are managed by the Cluster API Addon Provider, which must also be installed if you wish to use the addons functionality.

            In addition, Helm must be installed and configured to access your management cluster, and the chart repository containing this chart must be configured:
                helm repo add capi https://stackhpc.github.io/capi-helm-charts

    Management cluster
    https://cluster-api.sigs.k8s.io/user/concepts.html#management-cluster

        A Kubernetes cluster that manages the lifecycle of Workload Clusters.
        A Management Cluster is also where one or more providers run, and where resources such as Machines are stored.

    Infrastructure provider
    https://cluster-api.sigs.k8s.io/user/concepts.html#infrastructure-provider

        A component responsible for the provisioning of infrastructure/computational resources required by the Cluster
        or by Machines (e.g. VMs, networking, etc.). For example, cloud Infrastructure Providers include AWS, Azure,
        and Google, and bare metal Infrastructure Providers include VMware, MAAS, and metal3.io.


