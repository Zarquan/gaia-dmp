#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Bootstrap node for running ClusterAPI

    Result:

        Work in progress ...
        Got diverted by GitHub changing their SSH key.


# -----------------------------------------------------
# Recover our notes and changes from temp.
# See https://github.com/wfau/gaia-dmp/blob/957cf5cdc2381cb77c32fd29444dd03848ff1a00/notes/zrq/20230207-01-git-branch.txt#L90-L113
#[user@desktop]

    #
    # Import the files from ~/temp

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        mv ~/temp/bootstrap/20230130-01-bootstrap.txt notes/zrq
        mv ~/temp/bootstrap/20230201-01-bootstrap.txt notes/zrq
        mv ~/temp/bootstrap/20230201-02-bootstrap.txt notes/zrq
        mv ~/temp/bootstrap/20230202-03-bootstrap.txt notes/zrq

        mv ~/temp/bootstrap/ deployments/cluster-api

    popd

    #
    # Quick check to see what we have.

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        gedit \
            notes/zrq/20230130-01-bootstrap.txt \
            notes/zrq/20230201-01-bootstrap.txt \
            notes/zrq/20230201-02-bootstrap.txt \
            notes/zrq/20230202-03-bootstrap.txt &


# -----------------------------------------------------
# Check which platform is live.
#[user@desktop]

    ssh fedora@live.gaia-dmp.uk \
        '
        date
        hostname
        '

    >   Fri 24 Mar 06:13:50 UTC 2023
    >   iris-gaia-green-20230308-zeppelin


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    #
    # Live is green, selecting red for the deployment.
    #

    source "${HOME:?}/aglais.env"

    agcolour=red

    clientname=ansibler-${agcolour}
    cloudname=iris-gaia-${agcolour}

    podman run \
        --rm \
        --tty \
        --interactive \
        --name     "${clientname:?}" \
        --hostname "${clientname:?}" \
        --env "cloudname=${cloudname:?}" \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK:?}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        ghcr.io/wfau/atolmis/ansible-client:2022.07.25 \
        bash

    >   ....
    >   ....


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

    >   real    3m36.104s
    >   user    1m36.396s
    >   sys     0m10.186s

# -----------------------------------------------------
# Add YAML editor role.
#[root@ansibler]

    ansible-galaxy install kwoodson.yedit

    >   Starting galaxy role install process
    >   - downloading role 'yedit', owned by kwoodson
    >   - downloading role from https://github.com/kwoodson/ansible-role-yedit/archive/master.tar.gz
    >   - extracting kwoodson.yedit to /root/.ansible/roles/kwoodson.yedit
    >   - kwoodson.yedit (master) was installed successfully


# -----------------------------------------------------
# Settings ...
#[root@ansibler]

    deployname=${cloudname:?}-$(date '+%Y%m%d')
    deploydate=$(date '+%Y%m%dT%H%M%S')

    statusyml='/opt/aglais/aglais-status.yml'
    if [ ! -e "$(dirname ${statusyml})" ]
    then
        mkdir "$(dirname ${statusyml})"
    fi
    rm -f "${statusyml}"
    touch "${statusyml}"

    yq eval \
        --inplace \
        "
        .aglais.deployment.type = \"cluster-api\"   |
        .aglais.deployment.name = \"${deployname}\" |
        .aglais.deployment.date = \"${deploydate}\" |
        .aglais.openstack.cloud.name = \"${cloudname}\"
        " "${statusyml}"


# -----------------------------------------------------
# Create our bootstrap components.
#[root@ansibler]

    inventory=/deployments/cluster-api/bootstrap/ansible/config/inventory.yml

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/01-create-keypair.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/02-create-network.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/03-create-bootstrap.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/04-local-config.yml'

    >   ....
    >   ....


# -----------------------------------------------------
# Check our local config.
#[root@ansibler]

    cat /opt/aglais/aglais-status.yml

    >   aglais:
    >     deployment:
    >       date: 20230324T064344
    >       name: iris-gaia-red-20230324
    >       type: cluster-api
    >     openstack:
    >       cloud:
    >         name: iris-gaia-red
    >       keypairs:
    >         team:
    >           fingerprint: 2e:84:98:98:df:70:06:0e:4c:ed:bd:d4:d6:6b:eb:16
    >           id: iris-gaia-red-20230324-keypair
    >           name: iris-gaia-red-20230324-keypair
    >       networks:
    >         internal:
    >           network:
    >             id: fe7055a7-303f-47d0-8e02-e6e6034b10ed
    >             name: iris-gaia-red-20230324-internal-network
    >           router:
    >             id: 79be0c86-b934-45d2-a7c4-475518513f88
    >             name: iris-gaia-red-20230324-internal-router
    >           subnet:
    >             cidr: 10.10.0.0/16
    >             id: b2842a3c-bc07-43ff-a13c-06c744d91456
    >             name: iris-gaia-red-20230324-internal-subnet
    >       servers:
    >         bootstrap:
    >           float:
    >             external: 128.232.226.66
    >             id: 887bd2e8-35aa-4c71-8990-0b6f69496e7b
    >             internal: 10.10.2.35
    >           server:
    >             address:
    >               ipv4: 10.10.2.35
    >             flavor:
    >               name: gaia.vm.cclake.2vcpu
    >             hostname: bootstrap
    >             id: f4d96724-fc52-4713-8a5f-4534836fcac0
    >             image:
    >               id: e5c23082-cc34-4213-ad31-ff4684657691
    >               name: Fedora-34.1.2
    >             name: iris-gaia-red-20230324-bootstrap


# -----------------------------------------------------
# SSH test.
#[root@ansibler]

    ssh bootstrap \
        '
        date
        hostname
        '

    >   Fri Mar 24 06:45:50 UTC 2023
    >   iris-gaia-red-20230324-bootstrap


# -----------------------------------------------------
# Install Podman.
#[root@ansibler]

    ssh bootstrap \
        '
        sudo dnf install -y podman
        '

    >   ....
    >   ....
    >   Installed:
    >     ....
    >     ....
    >     podman-3:3.4.7-1.fc34.x86_64
    >     podman-gvproxy-3:3.4.7-1.fc34.x86_64
    >     podman-plugins-3:3.4.7-1.fc34.x86_64


# -----------------------------------------------------
# Install kubectl.
# https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management
#[root@ansibler]

    ssh bootstrap \
        '

cat > /tmp/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

        sudo chown 'root:root' '/tmp/kubernetes.repo'
        sudo mv '/tmp/kubernetes.repo' '/etc/yum.repos.d/kubernetes.repo'

        sudo dnf install -y 'kubectl'

        '

    >   ....
    >   ....
    >   Installed:
    >     kubectl-1.26.3-0.x86_64


# -----------------------------------------------------
# Install kind.
# https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries
#[root@ansibler]

    ssh bootstrap \
        '
        kversion=0.17.0
        kfile=kind-${kversion}
        ktemp=/tmp/${kfile}

        curl --location --output "${ktemp}" "https://kind.sigs.k8s.io/dl/v${version}/kind-linux-amd64"
        pushd /usr/local/bin
            sudo mv "${ktemp}" .
            sudo chown 'root:root' "${kfile}"
            sudo chmod 'u=rwx,g=rx,o=rx' "${kfile}"
            sudo ln -s "${kfile}" "kind"
        popd

        sudo dnf install -y firewalld
        sudo systemctl start firewalld

        sudo sed -i -e 's/^FirewallBackend=.*$/FirewallBackend=iptables/' '/etc/firewalld/firewalld.conf'

cat > '/tmp/kind-podman' << EOF
#!/bin/sh
export KIND_EXPERIMENTAL_PROVIDER=podman
EOF

        sudo chown 'root:root' '/tmp/kind-podman'
        sudo mv '/tmp/kind-podman' '/etc/profile.d/kind-podman'

        source '/etc/profile.d/kind-podman'
        '


# -----------------------------------------------------actually
# Update our system config.
# https://kind.sigs.k8s.io/docs/user/rootless/#host-requirements
#[root@ansibler]

    ssh bootstrap \
        '

cat > /tmp/delegate.conf << EOF
[Service]
Delegate=yes
EOF

        sudo mkdir '/etc/systemd/user@.service.d'

        sudo chown 'root:root' '/tmp/delegate.conf'
        sudo mv '/tmp/delegate.conf' '/etc/systemd/user@.service.d/delegate.conf'

        sudo systemctl daemon-reload
        '


    ssh bootstrap \
        '
        sudo systemctl status
        '

    >   ● iris-gaia-red-20230324-bootstrap
    >       State: running
    >        Jobs: 0 queued
    >      Failed: 0 units
    >       Since: Fri 2023-03-24 06:45:01 UTC; 8min ago
    >      CGroup: /
    >              ├─user.slice
    >              │ └─user-1000.slice
    >              │   ├─user@1000.service …
    >              │   │ └─init.scope
    >              │   │   ├─763 /usr/lib/systemd/systemd --user
    >              │   │   └─765 (sd-pam)
    >              │   └─session-1.scope
    >              │     ├─ 755 sshd: fedora [priv]
    >              │     ├─ 772 sshd: fedora@notty
    >              │     ├─9828 bash -c
    >              │     ├─9841 sudo systemctl status
    >              │     └─9843 systemctl status
    >              ├─init.scope
    >              │ └─1 /usr/lib/systemd/systemd --switched-root --system --deserialize 30
    >              └─system.slice
    >                ├─systemd-udevd.service
    >                │ └─481 /usr/lib/systemd/systemd-udevd
    >                ├─dbus-broker.service
    >                │ ├─510 /usr/bin/dbus-broker-launch --scope system --audit
    >                │ └─553 dbus-broker --log 4 --controller 9 --machine-id f4d96724fc5247138a5f4534836fcac0 --max-bytes 536870912 --max-fds 4096 --max-matches 131072 --audit
    >                ├─systemd-homed.service
    >                │ └─509 /usr/lib/systemd/systemd-homed
    >                ├─system-serial\x2dgetty.slice
    >                │ └─serial-getty@ttyS0.service
    >                │   └─716 /sbin/agetty -o -p -- \u --keep-baud 115200,57600,38400,9600 ttyS0 vt220
    >                ├─chronyd.service
    >                │ └─521 /usr/sbin/chronyd
    >                ├─auditd.service
    >                │ └─460 /sbin/auditd
    >                ├─systemd-journald.service
    >                │ └─438 /usr/lib/systemd/systemd-journald
    >                ├─sshd.service
    >                │ └─728 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups
    >                ├─NetworkManager.service
    >                │ └─583 /usr/sbin/NetworkManager --no-daemon
    >                ├─firewalld.service
    >                │ └─9640 /usr/bin/python3 -s /usr/sbin/firewalld --nofork --nopid
    >                ├─systemd-userdbd.service
    >                │ ├─ 758 /usr/lib/systemd/systemd-userdbd
    >                │ ├─9770 systemd-userwork
    >                │ ├─9774 systemd-userwork
    >                │ └─9776 systemd-userwork
    >                ├─sssd.service
    >                │ ├─508 /usr/sbin/sssd -i --logger=files
    >                │ ├─554 /usr/libexec/sssd/sssd_be --domain implicit_files --uid 0 --gid 0 --logger=files
    >                │ └─555 /usr/libexec/sssd/sssd_nss --uid 0 --gid 0 --logger=files
    >                ├─systemd-oomd.service
    >                │ └─458 /usr/lib/systemd/systemd-oomd
    >                ├─systemd-resolved.service
    >                │ └─459 /usr/lib/systemd/systemd-resolved
    >                ├─system-getty.slice
    >                │ └─getty@tty1.service
    >                │   └─715 /sbin/agetty -o -p -- \u --noclear tty1 linux
    >                └─systemd-logind.service
    >                  └─556 /usr/lib/systemd/systemd-logind


# -----------------------------------------------------
# Create a cluster, with logs.
# https://github.com/kubernetes-sigs/kind/pull/2478#issuecomment-1214656908
#[root@ansibler]

    ssh bootstrap \
        '
        kind create cluster --retain
        kind export logs
        kind delete cluster
        '

    >   ....
    >   ....


    >   enabling experimental podman provider
    >   Exporting logs for cluster "kind" to:
    >   /tmp/2167801430
    >   ERROR: [
    >       command "
    >           podman exec --privileged kind-control-plane sh -c 'tar --hard-dereference -C /var/log/ -chf - . || (r=$?; [ $r -eq 1 ] || exit $r)'
    >           "
    >       failed with error: exit status 255,
    >           [
    >           command "podman exec --privileged kind-control-plane crictl images" failed with error: exit status 255,
    >           command "podman exec --privileged kind-control-plane cat /kind/version" failed with error: exit status 255,
    >           command "podman exec --privileged kind-control-plane journalctl --no-pager" failed with error: exit status 255,
    >           command "podman exec --privileged kind-control-plane journalctl --no-pager -u kubelet.service" failed with error: exit status 255,
    >           command "podman exec --privileged kind-control-plane journalctl --no-pager -u containerd.service" failed with error: exit status 255
    >           ]
    >       ]








