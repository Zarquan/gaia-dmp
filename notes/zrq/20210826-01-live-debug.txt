#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Diagnose problems on the dev deployment usied by Dennis.

    Result:

        Zeppelin node out of space in '/' due to accumulated log files.
        Need to move the log files from local dics to a Cinder volume.

        Zeppelin failed trying to update interpreter.json, no space to write new version resulted in truncated (empty) json file.

            zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log
            "java.lang.IllegalStateException: Not a JSON Object: null"

        Daemons on worker nodes failed due to OutOfMemory errors in the Java virtual machine.
        These are not normal OutOfMemoryExceptions thrown and caught inside the Java virtual machine.
        These are system process OutOfMemory errors that cause the Java virtual machine to core dump.

            hs_err_pid27205.log
            "There is insufficient memory for the Java Runtime Environment to continue."


# -----------------------------------------------------

    Error while running Ansible script to setup the SSH connections:

    >   ....
    >   PLAY [Ping tests] ..
    >
    >   TASK [Check we can connect] ..
    >   fatal: [zeppelin]: UNREACHABLE! => {"changed": false, "msg": "mkdir: cannot create directory ‘/home/fedora/.ansible/tmp/ansible-tmp-1629948723.2485855-222-3406875931649’: No space left on device\n", "unreachable": true}
    >   ok: [monitor] => {"ansible_facts": {"discovered_interpreter_python": "/usr/bin/python3"}, "changed": false, "ping": "pong"}
    >   ok: [worker01] => {"ansible_facts": {"discovered_interpreter_python": "/usr/bin/python3"}, "changed": false, "ping": "pong"}
    >   ok: [worker02] => {"ansible_facts": {"discovered_interpreter_python": "/usr/bin/python3"}, "changed": false, "ping": "pong"}
    >   ....


# -----------------------------------------------------
# Check the available disc space.
#[user@zeppelin]

    pwd

    >   /home/fedora

    du -h -d 2

    >   ....
    >   8.0K    ./spark-warehouse
    >   ....
    >   20K     ./.config
    >   ....
    >   8.0K    ./.ansible
    >   ....
    >   2.1M    ./.ipython/profile_default
    >   2.2M    ./.ipython
    >   18M     ./zeppelin-0.8.2-bin-all/notebook
    >   ....
    >   12G     ./zeppelin-0.8.2-bin-all/logs
    >   943M    ./zeppelin-0.8.2-bin-all/interpreter
    >   162M    ./zeppelin-0.8.2-bin-all/webapps
    >   ....
    >   103M    ./zeppelin-0.8.2-bin-all/lib
    >   ....
    >   13G     ./zeppelin-0.8.2-bin-all
    >   ....
    >   1.5G    ./.local/lib
    >   1.5G    ./.local
    >   ....
    >   460M    ./.cache/pip
    >   2.0M    ./.cache/jedi
    >   ....
    >   463M    ./.cache
    >   15G     .


    ls -alh zeppelin/logs/

    >   total 12G
    >   drwxrwxr-x.  2 fedora fedora 4.0K Aug 26 00:03 .
    >   drwxr-xr-x. 13 fedora fedora 4.0K Aug  5 04:59 ..
    >   -rw-rw-r--.  1 fedora fedora    0 Aug 26 03:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log
    >   -rw-rw-r--.  1 fedora fedora 378K Aug  5 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-05
    >   -rw-rw-r--.  1 fedora fedora 1.2M Aug  6 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-06
    >   -rw-rw-r--.  1 fedora fedora  10K Aug  7 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-07
    >   -rw-rw-r--.  1 fedora fedora 8.0K Aug  8 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-08
    >   -rw-rw-r--.  1 fedora fedora  66K Aug  9 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-09
    >   -rw-rw-r--.  1 fedora fedora 448K Aug 10 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-10
    >   -rw-rw-r--.  1 fedora fedora  15K Aug 11 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-11
    >   -rw-rw-r--.  1 fedora fedora 8.0K Aug 12 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-12
    >   -rw-rw-r--.  1 fedora fedora 8.0K Aug 13 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-13
    >   -rw-rw-r--.  1 fedora fedora 8.0K Aug 14 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-14
    >   -rw-rw-r--.  1 fedora fedora 8.0K Aug 15 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-15
    >   -rw-rw-r--.  1 fedora fedora 8.0K Aug 16 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-16
    >   -rw-rw-r--.  1 fedora fedora 9.4K Aug 17 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-17
    >   -rw-rw-r--.  1 fedora fedora 8.0K Aug 18 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-18
    >   -rw-rw-r--.  1 fedora fedora 8.0K Aug 19 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-19
    >   -rw-rw-r--.  1 fedora fedora 8.4K Aug 20 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-20
    >   -rw-rw-r--.  1 fedora fedora 8.0K Aug 21 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-21
    >   -rw-rw-r--.  1 fedora fedora 8.0K Aug 22 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-22
    >   -rw-rw-r--.  1 fedora fedora 158K Aug 23 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-23
    >   -rw-rw-r--.  1 fedora fedora 176K Aug 24 23:53 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-24
    >   -rw-rw-r--.  1 fedora fedora    0 Aug 25 23:03 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-25
    >   -rw-rw-r--.  1 fedora fedora  28K Aug 25 10:23 zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.out
    >   -rw-rw-r--.  1 fedora fedora    0 Aug 25 08:43 zeppelin-interpreter-md-fedora-gaia-dev-20210805-zeppelin.novalocal.log
    >   -rw-rw-r--.  1 fedora fedora 8.5K Aug  5 16:26 zeppelin-interpreter-md-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-05
    >   -rw-rw-r--.  1 fedora fedora 1.9K Aug  6 10:29 zeppelin-interpreter-md-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-06
    >   -rw-rw-r--.  1 fedora fedora 1.5K Aug  9 14:31 zeppelin-interpreter-md-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-09
    >   -rw-rw-r--.  1 fedora fedora 3.0K Aug 10 12:09 zeppelin-interpreter-md-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-10
    >   -rw-rw-r--.  1 fedora fedora 1.9K Aug 23 08:35 zeppelin-interpreter-md-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-23
    >   -rw-rw-r--.  1 fedora fedora  382 Aug 24 10:51 zeppelin-interpreter-md-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-24
    >   -rw-rw-r--.  1 fedora fedora    0 Aug 25 08:43 zeppelin-interpreter-sh-fedora-gaia-dev-20210805-zeppelin.novalocal.log
    >   -rw-rw-r--.  1 fedora fedora 4.5K Aug  5 15:50 zeppelin-interpreter-sh-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-05
    >   -rw-rw-r--.  1 fedora fedora 1.7K Aug  6 10:18 zeppelin-interpreter-sh-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-06
    >   -rw-rw-r--.  1 fedora fedora 3.3K Aug  9 14:20 zeppelin-interpreter-sh-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-09
    >   -rw-rw-r--.  1 fedora fedora 1.7K Aug 10 11:50 zeppelin-interpreter-sh-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-10
    >   -rw-rw-r--.  1 fedora fedora  11K Aug 24 09:46 zeppelin-interpreter-sh-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-24
    >   -rw-rw-r--.  1 fedora fedora    0 Aug 25 08:39 zeppelin-interpreter-spark-fedora-gaia-dev-20210805-zeppelin.novalocal.log
    >   -rw-rw-r--.  1 fedora fedora 355M Aug  5 18:20 zeppelin-interpreter-spark-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-05
    >   -rw-rw-r--.  1 fedora fedora 172M Aug  6 20:37 zeppelin-interpreter-spark-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-06
    >   -rw-rw-r--.  1 fedora fedora 165M Aug  9 17:15 zeppelin-interpreter-spark-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-09
    >   -rw-rw-r--.  1 fedora fedora 356M Aug 10 14:23 zeppelin-interpreter-spark-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-10
    >   -rw-rw-r--.  1 fedora fedora 6.2K Aug 12 07:12 zeppelin-interpreter-spark-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-12
    >   -rw-rw-r--.  1 fedora fedora 7.8G Aug 23 23:59 zeppelin-interpreter-spark-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-23
    >   -rw-rw-r--.  1 fedora fedora 2.4G Aug 24 23:59 zeppelin-interpreter-spark-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-24


# -----------------------------------------------------
# Transfer the logs ot a different disc.
#[user@zeppelin]

    mkdir -p /user/zrq/zeppelin/logs

    mv -v /home/fedora/zeppelin/logs/* /user/zrq/zeppelin/logs


# -----------------------------------------------------
# Restart Zeppelin and see if it works.
#[user@zeppelin]

    /home/fedora/zeppelin/bin/zeppelin-daemon.sh restart

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]



    tail -f logs/zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log

    >    INFO [2021-08-26 04:02:04,803] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: livy
    >    INFO [2021-08-26 04:02:04,804] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: file
    >    WARN [2021-08-26 04:02:04,847] ({main} InterpreterSettingManager.java[init]:331) - No interpreter-setting.json found in /home/fedora/zeppelin/interpreter/scio
    >    INFO [2021-08-26 04:02:04,848] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: md
    >    INFO [2021-08-26 04:02:04,849] ({main} LocalConfigStorage.java[loadInterpreterSettings]:63) - Load Interpreter Setting from file: /home/fedora/zeppelin/conf/interpreter.json
    >    WARN [2021-08-26 04:02:04,854] ({main} ContextHandler.java[log]:2355) - unavailable
    >   MultiException stack 1 of 1
    >   java.lang.IllegalStateException: Not a JSON Object: null
    >   	at com.google.gson.JsonElement.getAsJsonObject(JsonElement.java:84)
    >   	at org.apache.zeppelin.storage.ConfigStorage.buildInterpreterInfoSaving(ConfigStorage.java:86)
    >   	at org.apache.zeppelin.storage.LocalConfigStorage.loadInterpreterSettings(LocalConfigStorage.java:65)
    >   	at org.apache.zeppelin.interpreter.InterpreterSettingManager.loadFromFile(InterpreterSettingManager.java:194)
    >   	at org.apache.zeppelin.interpreter.InterpreterSettingManager.init(InterpreterSettingManager.java:339)
    >   	at org.apache.zeppelin.interpreter.InterpreterSettingManager.<init>(InterpreterSettingManager.java:173)
    >   	at org.apache.zeppelin.interpreter.InterpreterSettingManager.<init>(InterpreterSettingManager.java:133)
    >   	at org.apache.zeppelin.server.ZeppelinServer.<init>(ZeppelinServer.java:158)
    >    ....
    >    ....
    >    WARN [2021-08-26 04:02:05,209] ({main} WebAppContext.java[doStart]:554) - Failed startup of context o.e.j.w.WebAppContext@13acb0d1{zeppelin-web,/,file:///home/fedora/zeppelin-0.8.2-bin-all/webapps/webapp/,UNAVAILABLE}{/home/fedora/zeppelin/zeppelin-web-0.8.2.war}
    >   javax.servlet.ServletException: rest@355bd4==org.eclipse.jetty.servlet.NoJspServlet,jsp=rest,order=-1,inst=false,async=true
    >   	at org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:693)
    >   	at org.eclipse.jetty.servlet.ServletHolder.initialize(ServletHolder.java:429)
    >   	at org.eclipse.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:750)
    >    ....
    >    ....
    >   Caused by: A MultiException has 1 exceptions.  They are:
    >   1. java.lang.IllegalStateException: Not a JSON Object: null
    >
    >   	at org.jvnet.hk2.internal.Utilities.justCreate(Utilities.java:1085)
    >   	at org.jvnet.hk2.internal.ServiceLocatorImpl.create(ServiceLocatorImpl.java:978)
    >       ...
    >       ...
    >   	... 29 more
    >   Caused by: java.lang.IllegalStateException: Not a JSON Object: null
    >   	at com.google.gson.JsonElement.getAsJsonObject(JsonElement.java:84)
    >   	at org.apache.zeppelin.storage.ConfigStorage.buildInterpreterInfoSaving(ConfigStorage.java:86)
    >       ...
    >       ...
    >   	... 45 more
    >    INFO [2021-08-26 04:02:05,223] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@74fef3f7{HTTP/1.1,[http/1.1]}{10.10.1.66:8080}
    >    INFO [2021-08-26 04:02:05,224] ({main} Server.java[doStart]:407) - Started @3991ms
    >    INFO [2021-08-26 04:02:05,224] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started
    >    ....
    >    ....

    #
    # Is that normal ?
    # I haven't watched this part of the startup process before ..
    #


# -----------------------------------------------------
# Check the space on the zeppelin node.
#[user@zeppelin]

df -h

    >   Filesystem  Size  Used Avail Use% Mounted on
    >   devtmpfs    23G     0   23G   0% /dev
    >   tmpfs       23G     0   23G   0% /dev/shm
    >   tmpfs       23G  612K   23G   1% /run
    >   tmpfs       23G     0   23G   0% /sys/fs/cgroup
    >   /dev/vda1   20G  7.8G   12G  42% /
    >   tmpfs       4.5G     0  4.5G   0% /run/user/1000
    >   /dev/vdb    178G   61M  168G   1% /mnt/local/vdb
    >   /dev/vdc    1.0T   17M 1022G   1% /mnt/cinder/vdc
    >   ....
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------
# Check the space on the other nodes.
#[user@ansibler]

    vmnames=(
        master01
        worker01
        worker02
        worker03
        worker04
        worker05
        worker06
        )

    for vmname in "${vmnames[@]}"
    do
        echo ""
        echo "VM [${vmname:?}]"
        ssh "${vmname:?}" \
            '
            hostname
            date
            df -h /
            '
    done

    >   VM [master01]
    >   gaia-dev-20210805-master01.novalocal
    >   Thu Aug 26 04:25:27 UTC 2021
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        14G  5.2G  8.0G  40% /
    >
    >   VM [worker01]
    >   gaia-dev-20210805-worker01.novalocal
    >   Thu Aug 26 04:25:27 UTC 2021
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.0G   15G  21% /
    >
    >   VM [worker02]
    >   gaia-dev-20210805-worker02.novalocal
    >   Thu Aug 26 04:25:28 UTC 2021
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.0G   15G  21% /
    >
    >   VM [worker03]
    >   gaia-dev-20210805-worker03.novalocal
    >   Thu Aug 26 04:25:28 UTC 2021
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.0G   15G  21% /
    >
    >   VM [worker04]
    >   gaia-dev-20210805-worker04.novalocal
    >   Thu Aug 26 04:25:29 UTC 2021
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  5.1G   14G  28% /
    >
    >   VM [worker05]
    >   gaia-dev-20210805-worker05.novalocal
    >   Thu Aug 26 04:25:29 UTC 2021
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.0G   15G  21% /
    >
    >   VM [worker06]
    >   gaia-dev-20210805-worker06.novalocal
    >   Thu Aug 26 04:25:30 UTC 2021
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.0G   15G  21% /



# -----------------------------------------------------
# -----------------------------------------------------
# Website fails to load

    firefox 'http://zeppelin.gaia-dev.aglais.uk:8080/#/'

        HTTP ERROR 503

        Problem accessing /. Reason:

            Service Unavailable


# -----------------------------------------------------
# -----------------------------------------------------
# Check the old log files
#[user@zeppelin]

    pushd /user/zrq/zeppelin/logs

    tail zeppelin-interpreter-spark-fedora-gaia-dev-20210805-zeppelin.novalocal.log.2021-08-24

    >    ....
    >    ....
    >    INFO [2021-08-24 11:30:51,011] ({dispatcher-event-loop-23} Logging.scala[logInfo]:54) - Starting task 1779.0 in stage 397.0 (TID 240576, worker02, executor 25, partition 1779, PROCESS_LOCAL, 8439 bytes)
    >    INFO [2021-08-24 11:30:51,011] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 1627.0 in stage 397.0 (TID 240515) in 146 ms on worker02 (executor 25) (1835/2249)
    >    INFO [2021-08-24 11:30:51,011] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Starting task 1868.0 in stage 397.0 (TID 240577, worker01, executor 16, partition 1868, PROCESS_LOCAL, 8439 bytes)
    >    INFO [2021-08-24 11:30:51,012] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 1804.0 in stage 397.0 (TID 240510) in 152 ms on worker01 (executor 16) (1836/2249)
    >    INFO [2021-08-24 11:30:51,015] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Starting task 2109.0 in stage 397.0 (TID 240578, worker04, executor 12, partition 2109, PROCESS_LOCAL, 8439 bytes)
    >    INFO [2021-08-24 11:30:51,015] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 1989.0 in stage 397.0 (TID 240514) in 150 ms on worker04 (executor 12) (1837/2249)
    >    INFO [2021-08-24 11:30:51,020] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 2204.0 in stage 397.0 (TID 240521) in 144 ms on worker02 (executor 5) (1838/2249)
    >    INFO [2021-08-24 11:30:51,022] ({dispatcher-event-loop-14} Logging.scala[logInfo]:54) - Starting task 1588.0 in stage 397.0 (TID 240579, worker04, executor 17, partition 1588, PROCESS_LOCAL, 8439 bytes)
    >    INFO [2021-08-24 11:30:51,022] ({dispatcher-event-loop-14} Logging.scala[logInfo]:54) - Starting task 1225.0 in stage 397.0 (TID 240580, worker03, executor 14, partition 1225, PROCESS_LOCAL, 8439 bytes)
    >    INFO [2021-08-24 11:30:51,022] ({task-result-getter-0} Logging.scala

    #
    # Zeppelin log just 'ends' mid line.
    # Suggests the system ran out of space at this point ?
    #
    # Last timestamp in the log is [2021-08-24 11:30:51,022]
    # but Dennis didn't report errors until 25th
    #

    >   Dennis Crake  9:38 AM
    >   Good morning Dave, so last night the spark context was shut down during a run was this a failure or a manual shut down?
    >   9:40
    >   Okay I have truly broken something :sweat_smile: The spark context wont restart and is directing me towards the logs, I’ll message Nigel but letting you know
    >
    >   Dave Morris:house_with_garden:  11:36 AM
    >   Hiya - OK, I'll look into it and see if we can get it restarted.
    >
    >   Dennis Crake  11:37 AM
    >   Great cheers, Nige and I had a look and it looks like we lost 3 workers through the night
    >
    >   Dave Morris:house_with_garden:  11:37 AM
    >   Was that 3 workers lost working on something complex .. or 3 workers lost while the systemw as idle ?
    >
    >   Dennis Crake  11:39 AM
    >   While under load, it could be memory error? But the system has a pop up at the minute saying: Oops! There is something wrong with the notebook file system. Please check the logs for more details
    >   11:39
    >   Also the system continued for 24 minutes after the last node was lost, so not sure whats going on really,

    #
    # Could the system have stumbled on for ~24hrs with no space on '/' filesystem ?
    #


# -----------------------------------------------------
# Reboot the VM to see if it comes back healthy.
#[user@zeppelin]

    sudo reboot

    >   Shared connection to 128.232.227.216 closed.


# -----------------------------------------------------
# -----------------------------------------------------
# Restart Zeppelin and see if it works.
#[user@zeppelin]

    /home/fedora/zeppelin/bin/zeppelin-daemon.sh restart

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]


# -----------------------------------------------------
# -----------------------------------------------------
# Website fails to load

    firefox 'http://zeppelin.gaia-dev.aglais.uk:8080/#/'

        HTTP ERROR 503

        Problem accessing /. Reason:

            Service Unavailable


# -----------------------------------------------------
# -----------------------------------------------------
# Zeppelin log shows the same error.
#[user@zeppelin]

    cd zeppelin/logs/

    tail -f zeppelin-fedora-gaia-dev-20210805-zeppelin.novalocal.log

    >    INFO [2021-08-26 05:01:59,392] ({main} InterpreterSettingManager.java[registerInterpreterSetting]:425) - Register InterpreterSettingTemplate: md
    >    INFO [2021-08-26 05:01:59,394] ({main} LocalConfigStorage.java[loadInterpreterSettings]:63) - Load Interpreter Setting from file: /home/fedora/zeppelin/conf/interpreter.json
    >    WARN [2021-08-26 05:01:59,399] ({main} ContextHandler.java[log]:2355) - unavailable
    >   MultiException stack 1 of 1
    >   java.lang.IllegalStateException: Not a JSON Object: null
    >   	at com.google.gson.JsonElement.getAsJsonObject(JsonElement.java:84)
    >   	at org.apache.zeppelin.storage.ConfigStorage.buildInterpreterInfoSaving(ConfigStorage.java:86)
    >
    >   ....
    >   ....
    >   	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    >   	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    >   	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    >   	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    >   	at org.glassfish.hk2.utilities.reflection.ReflectionHelper.makeMe(ReflectionHelper.java:1375)
    >   	at org.jvnet.hk2.internal.Utilities.justCreate(Utilities.java:1083)
    >   	... 45 more
    >    INFO [2021-08-26 04:58:33,214] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@74fef3f7{HTTP/1.1,[http/1.1]}{10.10.1.66:8080}
    >    INFO [2021-08-26 04:58:33,215] ({main} Server.java[doStart]:407) - Started @4299ms
    >    INFO [2021-08-26 04:58:33,215] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started


# -----------------------------------------------------
# Check the interpreter config.
#[user@zeppelin]

    cat /home/fedora/zeppelin/conf/interpreter.json

    >   -

    #
    # Empty file.
    # My guess is the system ran out of space on the 24th,
    # but carried on running the Spark job.
    #
    # When it finally tried to reload the interpreter,
    # it couldn't update `interpreter.json` and failed.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Upload a clean interpreter config from our source code.
#[root@ansibler]

    scp /deployments/common/zeppelin/interpreter.json \
        zeppelin:/home/fedora/zeppelin/conf/interpreter.json

    >   interpreter.json        100%   18KB  94.7KB/s   00:00


# -----------------------------------------------------
# -----------------------------------------------------
# Apply the deployment tasks to generate a new interpreter.json file.
#[root@ansibler]

        # Create a list of notebooks
        find /home/fedora/zeppelin/notebook -mindepth 1 -maxdepth 1 -type d ! -name '.git' -printf '%f\n' \
        | tee /tmp/001.txt

    >   2C35YU814
    >   2EZ3MQG4S
    >   ....
    >   ....
    >   2G9BXYCKP
    >   2FF2VTAAM


        # Create a JSON array of interpreter bindings.
        sed '
            1 i \
"interpreterBindings": {
        s/^\(.*\)$/"\1": ["spark", "md", "sh"]/
        $ ! s/^\(.*\)$/\1,/
        $ a \
},
            ' /tmp/001.txt \
            | tee /tmp/002.txt


    >   "interpreterBindings": {
    >   "2C35YU814": ["spark", "md", "sh"],
    >   "2EZ3MQG4S": ["spark", "md", "sh"],
    >   ....
    >   ....
    >   "2G9BXYCKP": ["spark", "md", "sh"],
    >   "2FF2VTAAM": ["spark", "md", "sh"]
    >   },


        # Wrap our fragment as a JSON document to check
        sed '
            1 i \
{
        $ s/,//
        $ a \
}
            ' /tmp/002.txt \
        | jq '.'


    >   {
    >     "interpreterBindings": {
    >       "2C35YU814": [
    >         "spark",
    >         "md",
    >         "sh"
    >       ],
    >       ....
    >       ....
    >       "2FF2VTAAM": [
    >         "spark",
    >         "md",
    >         "sh"
    >       ]
    >     }


# -----------------------------------------------------
# Truncate any existing list.
#[user@zeppelin]

        jq '
            del(.interpreterBindings[])
            ' \
        /home/fedora/zeppelin/conf/interpreter.json \
        > /tmp/003.json

        sed -n '
            /interpreterBindings/ p
            ' /tmp/003.json

    >     "interpreterBindings": {},


# -----------------------------------------------------
# Replace the empty list with our fragment.
#[user@zeppelin]

    # Insert our binding list into the rest of the file.
    sed '
        /interpreterBindings/ {
            r /tmp/002.txt
            d
            }
        ' /tmp/003.json \
    | jq '.' \
    > /tmp/004.json

    # Run it through 'jq' to check.
    jq '
        .interpreterBindings
        ' /tmp/004.json

    >   {
    >     "2C35YU814": [
    >       "spark",
    >       "md",
    >       "sh"
    >     ],
    >     ....
    >     ....
    >     "2FF2VTAAM": [
    >       "spark",
    >       "md",
    >       "sh"
    >     ]
    >   }


# -----------------------------------------------------
# Replace the original interpreter.json from git.
#[user@zeppelin]

    mv /home/fedora/zeppelin/conf/interpreter.json \
       /home/fedora/zeppelin/conf/interpreter.origin

    cp /tmp/004.json \
       /home/fedora/zeppelin/conf/interpreter.json

    /home/fedora/zeppelin/bin/zeppelin-daemon.sh restart

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]



# -----------------------------------------------------
# Move the logs directory onto a Cinder volume.
#[user@zeppelin]

    sudo mkdir /mnt/cinder/vdc/zeppelin
    sudo chown fedora:users /mnt/cinder/vdc/zeppelin
    sudo chmod u+rwxs,g+rwxs,o+rwx /mnt/cinder/vdc/zeppelin

    cd /home/fedora/zeppelin

    mv logs/ /mnt/cinder/vdc/zeppelin/
    ln -s /mnt/cinder/vdc/zeppelin/logs/ logs
    ls -al

    >   ....
    >   ....
    >   lrwxrwxrwx.  1 fedora fedora       30 Aug 26 08:38 logs -> /mnt/cinder/vdc/zeppelin/logs/
    >   ....


# -----------------------------------------------------
# Tail the Spark interpreter log.
#[user@zeppelin]

    pushd /mnt/cinder/vdc/zeppelin/logs

        tail -f zeppelin-interpreter-spark-fedora-gaia-dev-20210805-zeppelin.novalocal.log


    >   ....
    >    INFO [2021-08-26 08:52:11,113] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 588.0 in stage 4.0 (TID 592, worker01, executor 13, partition 588, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 08:52:11,113] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 513.0 in stage 4.0 (TID 517) in 11613 ms on worker01 (executor 13) (514/2048)
    >    INFO [2021-08-26 08:52:11,114] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 589.0 in stage 4.0 (TID 593, worker02, executor 8, partition 589, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 08:52:11,114] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 524.0 in stage 4.0 (TID 528) in 10620 ms on worker02 (executor 8) (515/2048)
    >    INFO [2021-08-26 08:52:11,114] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 590.0 in stage 4.0 (TID 594, worker03, executor 12, partition 590, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 08:52:11,114] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 418.0 in stage 4.0 (TID 422) in 25313 ms on worker03 (executor 12) (516/2048)
    >    INFO [2021-08-26 08:52:11,115] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 591.0 in stage 4.0 (TID 595, worker01, executor 10, partition 591, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 08:52:11,115] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 547.0 in stage 4.0 (TID 551) in 6880 ms on worker01 (executor 10) (517/2048)
    >    INFO [2021-08-26 08:52:11,115] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 592.0 in stage 4.0 (TID 596, worker03, executor 15, partition 592, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 08:52:11,115] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 550.0 in stage 4.0 (TID 554) in 6732 ms on worker03 (executor 15) (518/2048)
    >    INFO [2021-08-26 08:52:11,116] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 593.0 in stage 4.0 (TID 597, worker02, executor 11, partition 593, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 08:52:11,116] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 536.0 in stage 4.0 (TID 540) in 8021 ms on worker02 (executor 11) (519/2048)
    >    INFO [2021-08-26 08:52:11,116] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 594.0 in stage 4.0 (TID 598, worker03, executor 15, partition 594, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 08:52:11,116] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 472.0 in stage 4.0 (TID 476) in 17074 ms on worker03 (executor 15) (520/2048)
    >    INFO [2021-08-26 08:52:11,116] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 595.0 in stage 4.0 (TID 599, worker02, executor 2, partition 595, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 08:52:11,117] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 453.0 in stage 4.0 (TID 457) in 20796 ms on worker02 (executor 2) (521/2048)
    >    INFO [2021-08-26 08:52:11,117] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 596.0 in stage 4.0 (TID 600, worker03, executor 9, partition 596, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 08:52:11,117] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 414.0 in stage 4.0 (TID 418) in 25545 ms on worker03 (executor 9) (522/2048)
    >    INFO [2021-08-26 08:52:11,117] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 597.0 in stage 4.0 (TID 601, worker03, executor 15, partition 597, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 08:52:11,117] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 525.0 in stage 4.0 (TID 529) in 10600 ms on worker03 (executor 15) (523/2048)
    >   ....


    #
    # Looks good, but only 3 of the 6 workers are mentioned in the log.
    # worker01, worker02 and worker03
    #

# -----------------------------------------------------
# Check the workers for Java stack dumps.
#[user@zeppelin]

    workers=(
        worker01
        worker02
        worker03
        worker04
        worker05
        worker06
        )

    for worker in "${workers[@]}"
    do
        echo ""
        echo "Worker [${worker:?}]"
        ssh "${worker:?}" \
            '
            hostname
            date
            ls -al | grep hs_err
            '
    done

    >   Worker [worker01]
    >   gaia-dev-20210805-worker01.novalocal
    >   Thu Aug 26 09:01:25 UTC 2021
    >
    >   Worker [worker02]
    >   gaia-dev-20210805-worker02.novalocal
    >   Thu Aug 26 09:01:26 UTC 2021
    >
    >   Worker [worker03]
    >   gaia-dev-20210805-worker03.novalocal
    >   Thu Aug 26 09:01:26 UTC 2021
    >
    >   Worker [worker04]
    >   gaia-dev-20210805-worker04.novalocal
    >   Thu Aug 26 09:01:26 UTC 2021
    >   -rw-rw-r--. 1 fedora fedora 4115 Aug 25 01:54 hs_err_pid27059.log
    >   -rw-rw-r--. 1 fedora fedora 2816 Aug 25 03:11 hs_err_pid27197.log
    >
    >   Worker [worker05]
    >   gaia-dev-20210805-worker05.novalocal
    >   Thu Aug 26 09:01:27 UTC 2021
    >   -rw-rw-r--. 1 fedora fedora 307004 Aug 25 06:22 hs_err_pid27067.log
    >   -rw-rw-r--. 1 fedora fedora 497313 Aug 25 06:22 hs_err_pid27205.log
    >
    >   Worker [worker06]
    >   gaia-dev-20210805-worker06.novalocal
    >   Thu Aug 26 09:01:27 UTC 2021
    >   -rw-rw-r--. 1 fedora fedora 109867 Aug 12 07:04 hs_err_pid27197.log

    #
    # The Java runtime has crashed at least once on each of the failed workers.
    #

    workers=(
        worker04
        worker05
        worker06
        )

    for worker in "${workers[@]}"
    do
        echo ""
        echo "Worker [${worker:?}]"
        ssh "${worker:?}" \
            '
            hostname
            date
            head hs_err*
            '
    done


    >   Worker [worker04]
    >   gaia-dev-20210805-worker04.novalocal
    >   Thu Aug 26 09:05:47 UTC 2021
    >   ==> hs_err_pid27059.log <==
    >   #
    >   # There is insufficient memory for the Java Runtime Environment to continue.
    >   # Native memory allocation (mmap) failed to map 12288 bytes for committing reserved memory.
    >   # Possible reasons:
    >   #   The system is out of physical RAM or swap space
    >   #   The process is running with CompressedOops enabled, and the Java Heap may be blocking the growth of the native heap
    >   # Possible solutions:
    >   #   Reduce memory load on the system
    >   #   Increase physical memory or swap space
    >   #   Check if swap backing store is full
    >
    >   ==> hs_err_pid27197.log <==
    >   #
    >   # There is insufficient memory for the Java Runtime Environment to continue.
    >   # Native memory allocation (mmap) failed to map 12288 bytes for committing reserved memory.
    >   # Possible reasons:
    >   #   The system is out of physical RAM or swap space
    >   #   The process is running with CompressedOops enabled, and the Java Heap may be blocking the growth of the native heap
    >   # Possible solutions:
    >   #   Reduce memory load on the system
    >   #   Increase physical memory or swap space
    >   #   Check if swap backing store is full
    >
    >   Worker [worker05]
    >   gaia-dev-20210805-worker05.novalocal
    >   Thu Aug 26 09:05:47 UTC 2021
    >   ==> hs_err_pid27067.log <==
    >   #
    >   # There is insufficient memory for the Java Runtime Environment to continue.
    >   # Native memory allocation (mmap) failed to map 12288 bytes for committing reserved memory.
    >   # Possible reasons:
    >   #   The system is out of physical RAM or swap space
    >   #   The process is running with CompressedOops enabled, and the Java Heap may be blocking the growth of the native heap
    >   # Possible solutions:
    >   #   Reduce memory load on the system
    >   #   Increase physical memory or swap space
    >   #   Check if swap backing store is full
    >
    >   ==> hs_err_pid27205.log <==
    >   #
    >   # There is insufficient memory for the Java Runtime Environment to continue.
    >   # Native memory allocation (mmap) failed to map 12288 bytes for committing reserved memory.
    >   # Possible reasons:
    >   #   The system is out of physical RAM or swap space
    >   #   The process is running with CompressedOops enabled, and the Java Heap may be blocking the growth of the native heap
    >   # Possible solutions:
    >   #   Reduce memory load on the system
    >   #   Increase physical memory or swap space
    >   #   Check if swap backing store is full
    >
    >   Worker [worker06]
    >   gaia-dev-20210805-worker06.novalocal
    >   Thu Aug 26 09:05:47 UTC 2021
    >   #
    >   # There is insufficient memory for the Java Runtime Environment to continue.
    >   # Native memory allocation (mmap) failed to map 66060288 bytes for committing reserved memory.
    >   # Possible reasons:
    >   #   The system is out of physical RAM or swap space
    >   #   The process is running with CompressedOops enabled, and the Java Heap may be blocking the growth of the native heap
    >   # Possible solutions:
    >   #   Reduce memory load on the system
    >   #   Increase physical memory or swap space
    >   #   Check if swap backing store is full


# -----------------------------------------------------
# -----------------------------------------------------

    With only 3 out of 6 workers running, the system completed the ML forest notebook in ~8min32

    datediff --format "%H:%M:%S" --input-format "%H:%M:%S" '9:50:52' '9:59:24'


# -----------------------------------------------------
# -----------------------------------------------------
# Reboot all the workers.
#[user@zeppelin]

    workers=(
        worker01
        worker02
        worker03
        worker04
        worker05
        worker06
        )

    for worker in "${workers[@]}"
    do
        echo ""
        echo "Worker [${worker:?}]"
        ssh "${worker:?}" \
            '
            hostname
            date
            sudo reboot
            '
    done


# -----------------------------------------------------
# Restart the Hadoop worker processes.
#[user@zeppelin]

    ssh master01

        start-dfs.sh
        start-yarn.sh



# -----------------------------------------------------
# -----------------------------------------------------
# Run the ML forest notebook again, and we see jobs running on all the workers.
#[user@zeppelin]

    >   ....
    >
    >    INFO [2021-08-26 09:29:15,795] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Starting task 4036.0 in stage 57.0 (TID 74693, worker01, executor 31, partition 4036, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 09:29:15,795] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 3972.0 in stage 57.0 (TID 74629) in 865 ms on worker01 (executor 31) (3887/4608)
    >
    >    INFO [2021-08-26 09:29:15,796] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 4037.0 in stage 57.0 (TID 74694, worker04, executor 24, partition 4037, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 09:29:15,796] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3966.0 in stage 57.0 (TID 74623) in 945 ms on worker04 (executor 24) (3888/4608)
    >
    >    INFO [2021-08-26 09:29:15,811] ({dispatcher-event-loop-16} Logging.scala[logInfo]:54) - Starting task 4039.0 in stage 57.0 (TID 74696, worker06, executor 27, partition 4039, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 09:29:15,811] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 3879.0 in stage 57.0 (TID 74536) in 2117 ms on worker06 (executor 27) (3890/4608)
    >
    >    INFO [2021-08-26 09:29:15,839] ({dispatcher-event-loop-11} Logging.scala[logInfo]:54) - Starting task 4041.0 in stage 57.0 (TID 74698, worker05, executor 38, partition 4041, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 09:29:15,839] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3877.0 in stage 57.0 (TID 74534) in 2164 ms on worker05 (executor 38) (3892/4608)
    >
    >    INFO [2021-08-26 09:29:15,842] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Starting task 4042.0 in stage 57.0 (TID 74699, worker02, executor 40, partition 4042, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 09:29:15,842] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 3835.0 in stage 57.0 (TID 74492) in 2740 ms on worker02 (executor 40) (3893/4608)
    >
    >    INFO [2021-08-26 09:29:15,913] ({dispatcher-event-loop-20} Logging.scala[logInfo]:54) - Starting task 4045.0 in stage 57.0 (TID 74702, worker03, executor 17, partition 4045, PROCESS_LOCAL, 8330 bytes)
    >    INFO [2021-08-26 09:29:15,914] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3902.0 in stage 57.0 (TID 74559) in 1945 ms on worker03 (executor 17) (3896/4608)
    >   ....

    Second run (without restarting interpreter)
    0:31:8

    Third run (restarted interpreter)
    datediff --format "%H:%M:%S" --input-format "%H:%M:%S" '11:03:44' '11:11:37'
    0:7:53


