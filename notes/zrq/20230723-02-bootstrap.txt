#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: [{"name": "ChatGPT","contribution": {"value": 0,"units": "%"}}]
#


    Target:

        Try deploying a cluster without a floating IP address.

    Result:

        Work in progress ...

# -----------------------------------------------------

    #
    # Delete everything and create everything, up to the target cluster.
    #

# -----------------------------------------------------
# Modify our Helm values to disable the floating IP address.
#[root@bootstrap]

    yq eval \
        --inplace \
        "
        .apiServer.associateFloatingIP = false
        " \
        /opt/aglais/clusterapi-config.yml


# -----------------------------------------------------
# Deploy our target cluster.
#[root@bootstrap]

    workclusterbase=gaia-dmp-one
    workclustername=${workclusterbase:?}-$(date '+%Y%m%d')

    workclusterpath=/opt/aglais/${workclusterbase:?}
    workclusterconf=${workclusterpath:?}/${workclustername:?}-kubeconfig.yml

    yq eval \
        --inplace \
        "
        .aglais.kubernetes.work.name = \"${workclustername}\",
        .aglais.kubernetes.work.conf = \"${workclusterconf}\"
        " \
        "${statusyml:?}"

    helm upgrade \
        --kubeconfig "${kindclusterconf:?}" \
        "${workclustername:?}" \
        capi/openstack-cluster \
            --install \
            --version "0.1.0" \
            --values '/opt/aglais/clusterapi-config.yml' \
            --values '/opt/aglais/openstack-clouds.yaml'

    clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        describe cluster \
            "${workclustername:?}"

    >   ....
    >   ....

# -----------------------------------------------------
# Explore the control plane logs.
#[root@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get pods \
            --namespace capi-kubeadm-control-plane-system

    >   NAME                                                             READY   STATUS    RESTARTS   AGE
    >   capi-kubeadm-control-plane-controller-manager-7c49d7db96-t5slx   1/1     Running   0          13m


    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        logs \
            --follow \
            --namespace capi-kubeadm-control-plane-system  \
            capi-kubeadm-control-plane-controller-manager-7c49d7db96-t5slx

    >   ....
    >   I0723 03:27:28.368415       1 controller.go:404] "Scaling up control plane" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=e991f944-787c-4810-b52f-f538ae67a13d Cluster="default/gaia-dmp-one-20230723" Desired=3 Existing=1
    >   I0723 03:27:28.368602       1 scale.go:213] "Waiting for control plane to pass preflight checks" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=e991f944-787c-4810-b52f-f538ae67a13d Cluster="default/gaia-dmp-one-20230723" failures="[machine gaia-dmp-one-20230723-control-plane-8pf7l does not have APIServerPodHealthy condition, machine gaia-dmp-one-20230723-control-plane-8pf7l does not have ControllerManagerPodHealthy condition, machine gaia-dmp-one-20230723-control-plane-8pf7l does not have SchedulerPodHealthy condition, machine gaia-dmp-one-20230723-control-plane-8pf7l does not have EtcdPodHealthy condition, machine gaia-dmp-one-20230723-control-plane-8pf7l does not have EtcdMemberHealthy condition]"
    >   E0723 03:27:38.378198       1 controller.go:199] "Failed to update KubeadmControlPlane Status" err="failed to create remote cluster client: failed to create cluster accessor: error creating dynamic rest mapper for remote cluster \"default/gaia-dmp-one-20230723\": Get \"https://192.168.3.138:6443/api?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=e991f944-787c-4810-b52f-f538ae67a13d Cluster="default/gaia-dmp-one-20230723"
    >   E0723 03:27:38.379952       1 controller.go:329] "Reconciler error" err="failed to create remote cluster client: failed to create cluster accessor: error creating dynamic rest mapper for remote cluster \"default/gaia-dmp-one-20230723\": Get \"https://192.168.3.138:6443/api?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=e991f944-787c-4810-b52f-f538ae67a13d
    >   I0723 03:27:38.380301       1 controller.go:276] "Reconcile KubeadmControlPlane" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=f3ab536e-e101-4f4a-9a53-a3ae748880c3 Cluster="default/gaia-dmp-one-20230723"
    >   ....


    #
    # Network, subnet, router and load balancer all there.
    # Only one control plane VM.
    #

# -----------------------------------------------------
# -----------------------------------------------------
# Login via another terminal.
#[user@desktop]

    podman \
        exec \
            --tty
            --interactive \
            ansibler-blue \
                bash

# -----------------------------------------------------
# Explore the Openstack resources.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        server list

    >   +--------------------------------------+----------------------------------------------------+--------+-----------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | ID                                   | Name                                               | Status | Networks                                                              | Image                             | Flavor               |
    >   +--------------------------------------+----------------------------------------------------+--------+-----------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | 58957493-97f3-481b-82cd-257f0cd0ec17 | gaia-dmp-one-20230723-control-plane-c6736511-cvb44 | ACTIVE | k8s-clusterapi-cluster-default-gaia-dmp-one-20230723=192.168.3.116    | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 33a4c184-f9bb-4e5a-81a1-c0087d1eb6ad | iris-gaia-blue-20230723-bootstrap                  | ACTIVE | iris-gaia-blue-20230723-internal-network=10.10.0.218, 128.232.226.203 | Fedora-34.1.2                     | gaia.vm.cclake.2vcpu |
    >   +--------------------------------------+----------------------------------------------------+--------+-----------------------------------------------------------------------+-----------------------------------+----------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer list

    >   +--------------------------------------+--------------------------------------------------------------+----------------------------------+---------------+---------------------+------------------+----------+
    >   | id                                   | name                                                         | project_id                       | vip_address   | provisioning_status | operating_status | provider |
    >   +--------------------------------------+--------------------------------------------------------------+----------------------------------+---------------+---------------------+------------------+----------+
    >   | 070fb321-2598-4625-9124-374e0b84e84e | k8s-clusterapi-cluster-default-gaia-dmp-one-20230723-kubeapi | e918a13fed2648758175a15fac083569 | 192.168.3.138 | ACTIVE              | ONLINE           | amphora  |
    >   +--------------------------------------+--------------------------------------------------------------+----------------------------------+---------------+---------------------+------------------+----------+


    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer show \
            '070fb321-2598-4625-9124-374e0b84e84e'

    >   +---------------------+---------------------------------------------------------------------------------+
    >   | Field               | Value                                                                           |
    >   +---------------------+---------------------------------------------------------------------------------+
    >   | admin_state_up      | True                                                                            |
    >   | availability_zone   | None                                                                            |
    >   | created_at          | 2023-07-23T03:25:14                                                             |
    >   | description         | Created by cluster-api-provider-openstack cluster default-gaia-dmp-one-20230723 |
    >   | flavor_id           | None                                                                            |
    >   | id                  | 070fb321-2598-4625-9124-374e0b84e84e                                            |
    >   | listeners           | d126685d-2061-4d2d-ab74-43d9516baf28                                            |
    >   | name                | k8s-clusterapi-cluster-default-gaia-dmp-one-20230723-kubeapi                    |
    >   | operating_status    | ERROR                                                                           |
    >   | pools               | 53b76929-f60f-4fe3-8da5-84ee88740caf                                            |
    >   | project_id          | e918a13fed2648758175a15fac083569                                                |
    >   | provider            | amphora                                                                         |
    >   | provisioning_status | ACTIVE                                                                          |
    >   | updated_at          | 2023-07-23T03:48:28                                                             |
    >   | vip_address         | 192.168.3.138                                                                   |
    >   | vip_network_id      | ceb91980-95d7-4f2d-9aee-6c909a9359ab                                            |
    >   | vip_port_id         | 60d9a21d-eb21-4022-acfd-89c6b8def788                                            |
    >   | vip_qos_policy_id   | None                                                                            |
    >   | vip_subnet_id       | 04ce08d4-fac9-4c6f-bcd5-4de5a15f5843                                            |
    >   | tags                |                                                                                 |
    >   +---------------------+---------------------------------------------------------------------------------+

    #
    # operating_status is ERROR, but we don't know why.
    #

    >   +---------------------+---------------------------------------------------------------------------------+
    >   | Field               | Value                                                                           |
    >   +---------------------+---------------------------------------------------------------------------------+
    >   | ........            | ........                                                                        |
    >   | operating_status    | ERROR                                                                           |
    >   | ........            | ........                                                                        |
    >   +---------------------+---------------------------------------------------------------------------------+

    #
    # Alternates between ERROR and ONLINE.
    #

    >   +---------------------+---------------------------------------------------------------------------------+
    >   | Field               | Value                                                                           |
    >   +---------------------+---------------------------------------------------------------------------------+
    >   | ........            | ........                                                                        |
    >   | operating_status    | ONLINE                                                                          |
    >   | ........            | ........                                                                        |
    >   +---------------------+---------------------------------------------------------------------------------+


# -----------------------------------------------------
# -----------------------------------------------------
# Login via another terminal and follow the control plane logs.
#[user@desktop]

    podman \
        exec \
            --tty
            --interactive \
            ansibler-blue \
                bash

    ssh bootstrap

    statusyml=/opt/aglais/aglais-status.yml
    touch "${statusyml:?}"

    kindclustername=$(
        yq '
           .aglais.kubernetes.kind.name
           ' "${statusyml:?}"
        )

    kindclusterconf=$(
        yq '
           .aglais.kubernetes.kind.conf
           ' "${statusyml:?}"
        )

    controlpodid=$(
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            get pods \
                --output json \
                --namespace capi-kubeadm-control-plane-system \
        | jq -r '.items[0] | .metadata.name'
        )

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        logs \
            --follow \
            --namespace capi-kubeadm-control-plane-system  \
            "${controlpodid:?}"

    >   ....
    >   I0723 03:53:55.089107       1 controller.go:276] "Reconcile KubeadmControlPlane" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=ee69ce71-5256-43c6-acb5-e7b674b6fc83 Cluster="default/gaia-dmp-one-20230723"
    >   I0723 03:53:55.116388       1 controller.go:404] "Scaling up control plane" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=ee69ce71-5256-43c6-acb5-e7b674b6fc83 Cluster="default/gaia-dmp-one-20230723" Desired=3 Existing=1
    >   I0723 03:53:55.116490       1 scale.go:213] "Waiting for control plane to pass preflight checks" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=ee69ce71-5256-43c6-acb5-e7b674b6fc83 Cluster="default/gaia-dmp-one-20230723" failures="[machine gaia-dmp-one-20230723-control-plane-w847m does not have APIServerPodHealthy condition, machine gaia-dmp-one-20230723-control-plane-w847m does not have ControllerManagerPodHealthy condition, machine gaia-dmp-one-20230723-control-plane-w847m does not have SchedulerPodHealthy condition, machine gaia-dmp-one-20230723-control-plane-w847m does not have EtcdPodHealthy condition, machine gaia-dmp-one-20230723-control-plane-w847m does not have EtcdMemberHealthy condition]"
    >   E0723 03:54:05.125702       1 controller.go:199] "Failed to update KubeadmControlPlane Status" err="failed to create remote cluster client: failed to create cluster accessor: error creating dynamic rest mapper for remote cluster \"default/gaia-dmp-one-20230723\": Get \"https://192.168.3.138:6443/api?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=ee69ce71-5256-43c6-acb5-e7b674b6fc83 Cluster="default/gaia-dmp-one-20230723"
    >   E0723 03:54:05.127694       1 controller.go:329] "Reconciler error" err="failed to create remote cluster client: failed to create cluster accessor: error creating dynamic rest mapper for remote cluster \"default/gaia-dmp-one-20230723\": Get \"https://192.168.3.138:6443/api?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=ee69ce71-5256-43c6-acb5-e7b674b6fc83
    >   ....


    >   ....
    >   Get \"https://192.168.3.138:6443/api?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
    >   ....

    #
    # The Kind cluster is running on bootstrap.
    # A component running in the KLind cluster is tryinmg to reach 'https://192.168.3.138:6443/api'
    # ... but there is no route between the two internal networks.
    #

    #
    # Try again, but set the 'external' network to our bootstrap network.
    #

# -----------------------------------------------------
# -----------------------------------------------------
# Login via another terminal and get the network identifier.
#[user@desktop]

    podman exec \
        --tty \
        --interactive \
        ansibler-blue \
            bash

    openstack \
        --os-cloud "${cloudname:?}" \
        network list

    >   +--------------------------------------+------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
    >   | ID                                   | Name                                     | Subnets                                                                                                                                                |
    >   +--------------------------------------+------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
    >   | 410920fb-5714-4447-b26a-e7b06092fc62 | cephfs                                   | 5699fb5d-8316-4b88-b889-b05c8a1ec975                                                                                                                   |
    >   | 57add367-d205-4030-a929-d75617a7c63e | CUDN-Internet                            | 1847b14d-b974-4f78-959d-44d18d4485b8, 3fcaa5a5-ba8e-49a9-bf94-d87fbb0afc42, 5f1388b3-a0c7-463e-bb58-5532c38e4b40, a79eb610-eca3-4ee8-aaf1-88f4fef5a4e7 |
    >   | a84d018f-a4f5-48a9-9846-1a13b2a1b708 | iris-gaia-blue-20230723-internal-network | 3bb3654b-1f89-4666-8b64-35432f1984ce                                                                                                                   |
    >   +--------------------------------------+------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        network list \
            --format json \
    | jq -r '.[] | select(.Name | test("iris-gaia-blue")) | .ID'

    >   a84d018f-a4f5-48a9-9846-1a13b2a1b708

# -----------------------------------------------------
# Uninstall our the cluster.
#[root@bootstrap]

    helm uninstall \
        --kubeconfig "${kindclusterconf:?}" \
        "${workclustername:?}" \

    >   These resources were kept due to the resource policy:
    >   [Secret] gaia-dmp-one-20230723-cloud-credentials
    >   [KubeadmConfigTemplate] gaia-dmp-one-20230723-md-0-99910806
    >   [KubeadmControlPlane] gaia-dmp-one-20230723-control-plane
    >   [OpenStackCluster] gaia-dmp-one-20230723
    >   [OpenStackMachineTemplate] gaia-dmp-one-20230723-control-plane-c6736511
    >   [OpenStackMachineTemplate] gaia-dmp-one-20230723-md-0-c6736511
    >   
    >   release "gaia-dmp-one-20230723" uninstalled

    #
    # Modify the external network identifier.
    #

    yq eval \
        --inplace \
        "
        .clusterNetworking.externalNetworkId = \"a84d018f-a4f5-48a9-9846-1a13b2a1b708\"
        " \
        /opt/aglais/clusterapi-config.yml

    #
    # Install the cluster.
    #

    helm upgrade \
        --kubeconfig "${kindclusterconf:?}" \
        "${workclustername:?}" \
        capi/openstack-cluster \
            --install \
            --version "0.1.0" \
            --values '/opt/aglais/clusterapi-config.yml' \
            --values '/opt/aglais/openstack-clouds.yaml'

    watch \
        clusterctl \
            --kubeconfig "${kindclusterconf:?}" \
            describe cluster \
                "${workclustername:?}"

    #
    # Errors in the logs indicate problems finding the kubeconfig secret.
    #

    >   ....
    >   E0723 04:23:09.531965       1 controller.go:329] "Reconciler error" err="failed to create remote cluster client: failed to retrieve kubeconfig secret for Cluster default/gaia-dmp-one-20230723: secrets \"gaia-dmp-one-20230723-kubeconfig\" not found" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=6e8382f5-a559-4fb4-9263-e7d61a3fd8a1
    >   ....


# -----------------------------------------------------

    #
    # Delete everything and create everything, up to the target cluster.
    #

# -----------------------------------------------------
Get the new network identifier.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        network list

    >   +--------------------------------------+------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
    >   | ID                                   | Name                                     | Subnets                                                                                                                                                |
    >   +--------------------------------------+------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
    >   | 410920fb-5714-4447-b26a-e7b06092fc62 | cephfs                                   | 5699fb5d-8316-4b88-b889-b05c8a1ec975                                                                                                                   |
    >   | 57add367-d205-4030-a929-d75617a7c63e | CUDN-Internet                            | 1847b14d-b974-4f78-959d-44d18d4485b8, 3fcaa5a5-ba8e-49a9-bf94-d87fbb0afc42, 5f1388b3-a0c7-463e-bb58-5532c38e4b40, a79eb610-eca3-4ee8-aaf1-88f4fef5a4e7 |
    >   | c283431a-f749-438d-9753-75c0c5cbf68f | iris-gaia-blue-20230723-internal-network | 88f1fb05-14bf-42cd-b9d8-031a0c0ab77f                                                                                                                   |
    >   +--------------------------------------+------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        network list \
            --format json \
    | jq -r '.[] | select(.Name | test("iris-gaia-blue")) | .ID'

    >   c283431a-f749-438d-9753-75c0c5cbf68f


# -----------------------------------------------------
# -----------------------------------------------------
# Modify our Helm values to disable the floating IP address and set the external network.
#[root@bootstrap]

    yq eval \
        --inplace \
        "
        .apiServer.associateFloatingIP = false,
        .clusterNetworking.externalNetworkId = \"c283431a-f749-438d-9753-75c0c5cbf68f\"
        " \
        /opt/aglais/clusterapi-config.yml


# -----------------------------------------------------
# Deploy our target cluster.
#[root@bootstrap]

    workclusterbase=gaia-dmp-one
    workclustername=${workclusterbase:?}-$(date '+%Y%m%d')

    workclusterpath=/opt/aglais/${workclusterbase:?}
    workclusterconf=${workclusterpath:?}/${workclustername:?}-kubeconfig.yml

    yq eval \
        --inplace \
        "
        .aglais.kubernetes.work.name = \"${workclustername}\",
        .aglais.kubernetes.work.conf = \"${workclusterconf}\"
        " \
        "${statusyml:?}"

    helm upgrade \
        --kubeconfig "${kindclusterconf:?}" \
        "${workclustername:?}" \
        capi/openstack-cluster \
            --install \
            --version "0.1.0" \
            --values '/opt/aglais/clusterapi-config.yml' \
            --values '/opt/aglais/openstack-clouds.yaml'

    watch \
        clusterctl \
            --kubeconfig "${kindclusterconf:?}" \
            describe cluster \
                "${workclustername:?}"

    >   ....
    >   ....

    #
    # Gets stuck in the same place again.
    #

    >   ....
    >   I0723 05:15:40.278567       1 controller.go:276] "Reconcile KubeadmControlPlane" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=dd2eba81-f282-4887-9adc-a6c7243af068 Cluster="default/gaia-dmp-one-20230723"
    >   I0723 05:15:40.409458       1 controller.go:285] "Cluster infrastructure is not ready yet" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=dd2eba81-f282-4887-9adc-a6c7243af068 Cluster="default/gaia-dmp-one-20230723"
    >   E0723 05:15:40.419995       1 controller.go:199] "Failed to update KubeadmControlPlane Status" err="failed to create remote cluster client: failed to retrieve kubeconfig secret for Cluster default/gaia-dmp-one-20230723: secrets \"gaia-dmp-one-20230723-kubeconfig\" not found" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=dd2eba81-f282-4887-9adc-a6c7243af068 Cluster="default/gaia-dmp-one-20230723"
    >   E0723 05:15:40.576642       1 controller.go:329] "Reconciler error" err="failed to create remote cluster client: failed to retrieve kubeconfig secret for Cluster default/gaia-dmp-one-20230723: secrets \"gaia-dmp-one-20230723-kubeconfig\" not found" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=dd2eba81-f282-4887-9adc-a6c7243af068
    >   I0723 05:15:40.585142       1 controller.go:276] "Reconcile KubeadmControlPlane" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=3d8211dd-3e7f-478a-8b02-72e93a22bafd Cluster="default/gaia-dmp-one-20230723"
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------
# Uninstall our the cluster.
#[root@bootstrap]

    helm uninstall \
        --kubeconfig "${kindclusterconf:?}" \
        "${workclustername:?}" \

    >   These resources were kept due to the resource policy:
    >   [Secret] gaia-dmp-one-20230723-cloud-credentials
    >   [KubeadmConfigTemplate] gaia-dmp-one-20230723-md-0-99910806
    >   [KubeadmControlPlane] gaia-dmp-one-20230723-control-plane
    >   [OpenStackCluster] gaia-dmp-one-20230723
    >   [OpenStackMachineTemplate] gaia-dmp-one-20230723-control-plane-c6736511
    >   [OpenStackMachineTemplate] gaia-dmp-one-20230723-md-0-c6736511
    >   
    >   release "gaia-dmp-one-20230723" uninstalled


# -----------------------------------------------------
# Modify our Helm values to restore the defaults.
#[root@bootstrap]

    yq eval \
        --inplace \
        "
        .apiServer.associateFloatingIP = true,
        .clusterNetworking.externalNetworkId = \"57add367-d205-4030-a929-d75617a7c63e\"
        " \
        /opt/aglais/clusterapi-config.yml


# -----------------------------------------------------
# Deploy our target cluster.
#[root@bootstrap]

    workclusterbase=gaia-dmp-one
    workclustername=${workclusterbase:?}-$(date '+%Y%m%d')

    workclusterpath=/opt/aglais/${workclusterbase:?}
    workclusterconf=${workclusterpath:?}/${workclustername:?}-kubeconfig.yml

    yq eval \
        --inplace \
        "
        .aglais.kubernetes.work.name = \"${workclustername}\",
        .aglais.kubernetes.work.conf = \"${workclusterconf}\"
        " \
        "${statusyml:?}"

    helm upgrade \
        --kubeconfig "${kindclusterconf:?}" \
        "${workclustername:?}" \
        capi/openstack-cluster \
            --install \
            --version "0.1.0" \
            --values '/opt/aglais/clusterapi-config.yml' \
            --values '/opt/aglais/openstack-clouds.yaml'

    #
    # Still get the same errors in the logs ..
    #

    >   ....
    >   E0723 05:42:22.568658       1 controller.go:199] "Failed to update KubeadmControlPlane Status" err="failed to create remote cluster client: failed to retrieve kubeconfig secret for Cluster default/gaia-dmp-one-20230723: secrets \"gaia-dmp-one-20230723-kubeconfig\" not found" controller="kubeadmcontrolplane" controllerGroup="controlplane.cluster.x-k8s.io" controllerKind="KubeadmControlPlane" KubeadmControlPlane="default/gaia-dmp-one-20230723-control-plane" namespace="default" name="gaia-dmp-one-20230723-control-plane" reconcileID=01778ebe-7b7a-4a65-b4ab-14dc8286c797 Cluster="default/gaia-dmp-one-20230723"
    >   ....

    #
    # OK, what have we broken ?
    # ...
    # Actually .. it seems to be getting past that.
    # Somewhere it stopped being a problem and we have ended up with a full cluster.
    #
    # So something about setting oneof those values causes the deployment to fail.
    #

    kubectl get \
        --kubeconfig "${kindclusterconf:?}"  \
        Secret

    >   NAME                                                     TYPE                      DATA   AGE
    >   gaia-dmp-one-20230723-ca                                 cluster.x-k8s.io/secret   2      7m16s
    >   gaia-dmp-one-20230723-ccm-openstack-config               Opaque                    2      9m23s
    >   gaia-dmp-one-20230723-cloud-credentials                  Opaque                    1      35m
    >   gaia-dmp-one-20230723-cni-calico-config                  Opaque                    2      9m23s
    >   gaia-dmp-one-20230723-control-plane-c5944                cluster.x-k8s.io/secret   2      3m33s
    >   gaia-dmp-one-20230723-control-plane-pkhpt                cluster.x-k8s.io/secret   2      7m16s
    >   gaia-dmp-one-20230723-control-plane-svm76                cluster.x-k8s.io/secret   2      4m52s
    >   gaia-dmp-one-20230723-csi-cinder-config                  Opaque                    2      9m23s
    >   gaia-dmp-one-20230723-etcd                               cluster.x-k8s.io/secret   2      7m16s
    >   gaia-dmp-one-20230723-kubeconfig                         cluster.x-k8s.io/secret   1      7m16s
    >   gaia-dmp-one-20230723-md-0-99910806-876vv                cluster.x-k8s.io/secret   2      5m47s
    >   gaia-dmp-one-20230723-md-0-99910806-lc8hf                cluster.x-k8s.io/secret   2      5m47s
    >   gaia-dmp-one-20230723-md-0-99910806-lc8v7                cluster.x-k8s.io/secret   2      5m47s
    >   gaia-dmp-one-20230723-mellanox-network-operator-config   Opaque                    2      9m23s
    >   gaia-dmp-one-20230723-metrics-server-config              Opaque                    2      9m23s
    >   gaia-dmp-one-20230723-node-feature-discovery-config      Opaque                    2      9m23s
    >   gaia-dmp-one-20230723-nvidia-gpu-operator-config         Opaque                    2      9m23s
    >   gaia-dmp-one-20230723-proxy                              cluster.x-k8s.io/secret   2      7m16s
    >   gaia-dmp-one-20230723-sa                                 cluster.x-k8s.io/secret   2      7m16s
    >   sh.helm.release.v1.cluster-api-addon-provider.v1         helm.sh/release.v1        1      39m
    >   sh.helm.release.v1.gaia-dmp-one-20230723.v1              helm.sh/release.v1        1      9m23s

    #
    # Creating the kubeconfig secret fails if we don't have a floating IP address ?
    # Or is the kubeconfig secret a diversion.
    # Something else fails before that.
    #
    # The kubeconfig secret would need the IP address of the control node.
    # So not having one might cause problems.
    # Need to find the code.
    #


# -----------------------------------------------------
# Fetch the work cluster config.
#[root@bootstrap]

    mkdir -p $(dirname "${workclusterconf}")

    clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        get \
            kubeconfig "${workclustername:?}" \
    | tee "${workclusterconf}" \
    | yq '.'

    >   apiVersion: v1
    >   clusters:
    >     - cluster:
    >         certificate-authority-data: ........tLS0tLQo=
    >         server: https://128.232.226.161:6443
    >       name: gaia-dmp-one-20230723
    >   contexts:
    >     - context:
    >         cluster: gaia-dmp-one-20230723
    >         user: gaia-dmp-one-20230723-admin
    >       name: gaia-dmp-one-20230723-admin@gaia-dmp-one-20230723
    >   current-context: gaia-dmp-one-20230723-admin@gaia-dmp-one-20230723
    >   kind: Config
    >   preferences: {}
    >   users:
    >     - name: gaia-dmp-one-20230723-admin
    >       user:
    >         client-certificate-data: LS0tLS1C........tLS0tLQo=
    >         client-key-data: LS0tLS1C........0tLS0tCg==
    >   [root@iris-gaia-blue-20230723-bootstrap ~]#


# -----------------------------------------------------
# Check what pods are running.
#[root@bootstrap]

    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        get pods \
            --all-namespaces

    >   NAMESPACE                NAME                                                                         READY   STATUS             RESTARTS        AGE
    >   calico-apiserver         calico-apiserver-bcb7f4b8b-5pwfc                                             1/1     Running            0               8m42s
    >   calico-apiserver         calico-apiserver-bcb7f4b8b-87jp5                                             1/1     Running            0               8m42s
    >   calico-system            calico-kube-controllers-65d97fd8f9-q8lh2                                     1/1     Running            0               10m
    >   calico-system            calico-node-2f4tf                                                            1/1     Running            0               10m
    >   calico-system            calico-node-4zpkz                                                            1/1     Running            0               10m
    >   calico-system            calico-node-ffh4k                                                            1/1     Running            0               10m
    >   calico-system            calico-node-pmjwb                                                            1/1     Running            0               8m5s
    >   calico-system            calico-node-snm7w                                                            1/1     Running            0               9m31s
    >   calico-system            calico-node-zzq7k                                                            1/1     Running            0               10m
    >   calico-system            calico-typha-779c9fcb65-hhm6w                                                1/1     Running            0               10m
    >   calico-system            calico-typha-779c9fcb65-hxcvp                                                1/1     Running            0               8m46s
    >   calico-system            calico-typha-779c9fcb65-v57f5                                                1/1     Running            0               10m
    >   calico-system            csi-node-driver-45594                                                        2/2     Running            0               8m5s
    >   calico-system            csi-node-driver-7dsvn                                                        2/2     Running            0               10m
    >   calico-system            csi-node-driver-j7nw2                                                        2/2     Running            0               10m
    >   calico-system            csi-node-driver-m2s8z                                                        2/2     Running            0               10m
    >   calico-system            csi-node-driver-mpnh7                                                        2/2     Running            0               10m
    >   calico-system            csi-node-driver-zld2s                                                        2/2     Running            0               9m31s
    >   gpu-operator             gpu-operator-56c9cf6799-stn29                                                1/1     Running            0               10m
    >   kube-system              coredns-565d847f94-gmhsn                                                     1/1     Running            0               11m
    >   kube-system              coredns-565d847f94-q9p4r                                                     1/1     Running            0               11m
    >   kube-system              etcd-gaia-dmp-one-20230723-control-plane-c6736511-dwj27                      1/1     Running            0               11m
    >   kube-system              etcd-gaia-dmp-one-20230723-control-plane-c6736511-qdw97                      1/1     Running            0               9m26s
    >   kube-system              etcd-gaia-dmp-one-20230723-control-plane-c6736511-rqq6p                      1/1     Running            0               8m1s
    >   kube-system              kube-apiserver-gaia-dmp-one-20230723-control-plane-c6736511-dwj27            1/1     Running            0               11m
    >   kube-system              kube-apiserver-gaia-dmp-one-20230723-control-plane-c6736511-qdw97            1/1     Running            0               9m30s
    >   kube-system              kube-apiserver-gaia-dmp-one-20230723-control-plane-c6736511-rqq6p            1/1     Running            0               8m1s
    >   kube-system              kube-controller-manager-gaia-dmp-one-20230723-control-plane-c6736511-dwj27   1/1     Running            1 (9m16s ago)   11m
    >   kube-system              kube-controller-manager-gaia-dmp-one-20230723-control-plane-c6736511-qdw97   1/1     Running            0               9m30s
    >   kube-system              kube-controller-manager-gaia-dmp-one-20230723-control-plane-c6736511-rqq6p   1/1     Running            0               7m57s
    >   kube-system              kube-proxy-584lt                                                             1/1     Running            0               10m
    >   kube-system              kube-proxy-6fchp                                                             1/1     Running            0               8m5s
    >   kube-system              kube-proxy-k2nkp                                                             1/1     Running            0               10m
    >   kube-system              kube-proxy-pptsm                                                             1/1     Running            0               9m31s
    >   kube-system              kube-proxy-wpsv5                                                             1/1     Running            0               11m
    >   kube-system              kube-proxy-xnf5b                                                             1/1     Running            0               10m
    >   kube-system              kube-scheduler-gaia-dmp-one-20230723-control-plane-c6736511-dwj27            1/1     Running            1 (9m16s ago)   11m
    >   kube-system              kube-scheduler-gaia-dmp-one-20230723-control-plane-c6736511-qdw97            1/1     Running            0               9m30s
    >   kube-system              kube-scheduler-gaia-dmp-one-20230723-control-plane-c6736511-rqq6p            1/1     Running            0               7m56s
    >   kube-system              metrics-server-554f79c654-krr9s                                              1/1     Running            0               10m
    >   network-operator         mellanox-network-operator-778bffd589-4pctf                                   0/1     CrashLoopBackOff   6 (3m40s ago)   10m
    >   node-feature-discovery   node-feature-discovery-master-6968cdc89f-7vltx                               1/1     Running            0               10m
    >   node-feature-discovery   node-feature-discovery-worker-5bxtb                                          1/1     Running            0               10m
    >   node-feature-discovery   node-feature-discovery-worker-bpptl                                          1/1     Running            0               10m
    >   node-feature-discovery   node-feature-discovery-worker-jg8g5                                          1/1     Running            0               10m
    >   node-feature-discovery   node-feature-discovery-worker-rnsq9                                          1/1     Running            0               9m31s
    >   node-feature-discovery   node-feature-discovery-worker-zjs62                                          1/1     Running            0               10m
    >   node-feature-discovery   node-feature-discovery-worker-zsvkb                                          1/1     Running            0               8m5s
    >   openstack-system         openstack-cinder-csi-controllerplugin-5b6944d9dc-mgglp                       6/6     Running            0               10m
    >   openstack-system         openstack-cinder-csi-nodeplugin-27s7h                                        3/3     Running            0               10m
    >   openstack-system         openstack-cinder-csi-nodeplugin-9q7ml                                        3/3     Running            0               10m
    >   openstack-system         openstack-cinder-csi-nodeplugin-htpq7                                        3/3     Running            0               10m
    >   openstack-system         openstack-cinder-csi-nodeplugin-lwr48                                        3/3     Running            0               10m
    >   openstack-system         openstack-cinder-csi-nodeplugin-nk72f                                        3/3     Running            0               8m5s
    >   openstack-system         openstack-cinder-csi-nodeplugin-pnl54                                        3/3     Running            0               9m31s
    >   openstack-system         openstack-cloud-controller-manager-m96fh                                     1/1     Running            0               8m45s
    >   openstack-system         openstack-cloud-controller-manager-rfvdf                                     1/1     Running            1 (9m16s ago)   10m
    >   openstack-system         openstack-cloud-controller-manager-whp2r                                     1/1     Running            0               7m34s
    >   tigera-operator          tigera-operator-7f96bd8bf8-64mdj                                             1/1     Running            1 (9m15s ago)   10m

