#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Try to find out more about the limits on concurrent users.
        Follow on from yesterda's notes 20220615-01-concurrent-tests.txt.

    Result:

        Work in progress ...

        TODO move from quick to complex test sets
        TODO move from 4 to 8 concurrent users
        TODO loop until <time>
        TODO find the minimum looppause


# -----------------------------------------------------
# Create our long-loop function.
# https://stackoverflow.com/questions/17548064/how-to-have-a-bash-script-loop-until-a-specific-time
# https://stackoverflow.com/a/17548151
# https://linuxize.com/post/bash-increment-decrement-variable/
#[root@ansibler]

    long-loop()
        {
        local usercount=${1:?'usercount required'}
        local loopfinish=${2:?'loopfinish required'}
        local looppause=${3:-10}
        local delaystart=${4:-1}
        local delaynotebook=${5:-1}

        rm -f /tmp/results/*

cat << EOF
    {
    "usercount": "${usercount}",
    "loopcount": "${loopcount}",
    "looppause": "${looppause}",
    "delaystart": "${delaystart}",
    "delaynotebook": "${delaynotebook}",
    "iterations": [
EOF

        local comma=''
        local iter=0
        while [ $(date "+%H") -lt ${loopfinish} ]
        do

            testname="test-$(date '+%Y%m%dT%H%M%S')-iter-$(printf "%02d" ${iter})"

cat << EOF
            ${comma}
            {
            "iteration": ${iter},
            "testname": "${testname}",
            "threads":
EOF

            sleep "${looppause}"

            /tmp/run-benchmark.py \
                "${endpoint:?}" \
                "${testconfig:?}" \
                "${testusers:?}" \
                "${usercount:?}" \
                "${delaystart:?}" \
                "${delaynotebook:?}" \
            > "/tmp/results/${testname:?}.txt"

            filter-results "${testname:?}"

cat << EOF
            }
EOF
            comma=','
            ((iter+=1))

        done

cat << EOF
        ]
    }
EOF
        }


# -----------------------------------------------------
# Test with 19 users doing 2 loops until 10:00.
#[root@ansibler]

    # local usercount=${1:?'usercount required'}
    # local loopfinish=${2:?'loopfinish required'}
    # local looppause=${3:-10}
    # local delaystart=${4:-1}
    # local delaynotebook=${5:-1}

    long-loop 19 10 10 5 5 \
    | tee /tmp/test-loop.json


    >   ....
    >               {
    >               "iteration": 13,
    >               "testname": "test-20220617T062603-iter-13",
    >               "threads": [
    >                   [
    >                       {
    >                       "name": "GaiaDMPSetup",
    >                       "value": "PASS",
    >                       "time": 36.99,
    >                       "start": "2022-06-17T06:26:13.509228",
    >                       "finish": "2022-06-17T06:26:50.498948"
    >                       },
    >                       ....
    >                       ....
    >                       {
    >                       "name": "Source_counts_over_the_sky.json",
    >                       "value": "PASS",
    >                       "time": 17.83,
    >                       "start": "2022-06-17T06:27:35.418406",
    >                       "finish": "2022-06-17T06:27:53.252655"
    >                       }
    >                   ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >               },
    >               {
    >               "iteration": 14,
    >               "testname": "test-20220617T064603-iter-14",
    >               "threads": [
    >                   [
    >                   ....

    #
    # Test reached iteration 14, and stopped around 07:03 am.
    # Everything looks normal .. just stopped.
    #


# -----------------------------------------------------
# Check the Zeppelin logs ...
#[user@zeppelin]

    #
    # The Zeppelin log looks normal ... just stopped.

    less  ${HOME}/zeppelin/logs/zeppelin-$(id -un)-$(hostname).log

    >    ....
    >    ....
    >    WARN [2022-06-17 07:02:57,859] ({Exec Default Executor} ExecRemoteInterpreterProcess.java[onProcessComplete]:226) - Process is exited with exit value 0
    >    INFO [2022-06-17 07:02:57,860] ({Exec Default Executor} ProcessLauncher.java[transition]:109) - Process state is transitioned to COMPLETED
    >    WARN [2022-06-17 07:02:58,055] ({Exec Default Executor} ExecRemoteInterpreterProcess.java[onProcessComplete]:226) - Process is exited with exit value 0
    >    INFO [2022-06-17 07:02:58,056] ({Exec Default Executor} ProcessLauncher.java[transition]:109) - Process state is transitioned to COMPLETED
    >    INFO [2022-06-17 07:03:00,338] ({qtp2128029086-288820} ExecRemoteInterpreterProcess.java[stop]:136) - Remote exec process of interpreter group: spark-Bellgrin is terminated
    >    WARN [2022-06-17 07:03:00,338] ({qtp2128029086-288820} AuthorizationService.java[getOwners]:230) - No noteAuth found for noteId: 2H7RWP7WY
    >    INFO [2022-06-17 07:03:00,344] ({qtp2128029086-288823} ExecRemoteInterpreterProcess.java[stop]:136) - Remote exec process of interpreter group: md-Bellgrin is terminated
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:784) - Start to copy dependencies for interpreter: spark
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:795) - Finish copy dependencies for interpreter: spark
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:784) - Start to copy dependencies for interpreter: spark
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:795) - Finish copy dependencies for interpreter: spark
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:784) - Start to copy dependencies for interpreter: md
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:795) - Finish copy dependencies for interpreter: md
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:784) - Start to copy dependencies for interpreter: spark
    >    INFO [2022-06-17 07:03:00,346] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:795) - Finish copy dependencies for interpreter: spark
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288316} ManagedInterpreterGroup.java[close]:102) - Close Session: shared_session for interpreter setting: md
    >    WARN [2022-06-17 07:03:00,346] ({qtp2128029086-288823} AuthorizationService.java[getOwners]:230) - No noteAuth found for noteId: 2H5PXXMKB
    >    INFO [2022-06-17 07:03:00,346] ({qtp2128029086-288316} ManagedInterpreterGroup.java[close]:106) - Remove this InterpreterGroup: md-Bellgrin as all the sessions are closed
    >    INFO [2022-06-17 07:03:00,346] ({qtp2128029086-288316} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:784) - Start to copy dependencies for interpreter: md
    >    INFO [2022-06-17 07:03:00,346] ({qtp2128029086-288316} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:795) - Finish copy dependencies for interpreter: md
    >    WARN [2022-06-17 07:03:00,347] ({qtp2128029086-288316} AuthorizationService.java[getOwners]:230) - No noteAuth found for noteId: 2H78KAXNA
    >   
    >    INFO [2022-06-17 07:10:07,760] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:275) - Validating all active sessions...
    >    INFO [2022-06-17 07:10:07,766] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:308) - Finished session validation.  No sessions were stopped.
    >   
    >    INFO [2022-06-17 08:10:07,760] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:275) - Validating all active sessions...
    >    INFO [2022-06-17 08:10:07,765] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:308) - Finished session validation.  No sessions were stopped.
    >   
    >    INFO [2022-06-17 09:10:07,760] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:275) - Validating all active sessions...
    >    INFO [2022-06-17 09:10:07,763] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:308) - Finished session validation.  No sessions were stopped.
    >   
    >    INFO [2022-06-17 10:10:07,760] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:275) - Validating all active sessions...
    >    INFO [2022-06-17 10:10:07,764] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:308) - Finished session validation.  No sessions were stopped.


    #
    # The Spark interpreter log has some clues.

    less zeppelin-interpreter-spark-Fipa-Fipa-$(id -un)-$(hostname).log

    >    ....
    >    ....
    >    INFO [2022-06-17 06:48:12,591] ({FIFOScheduler-interpreter_238135472-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448486850_407152630 finished by scheduler interpreter_238135472 with status FINISHED
    >    INFO [2022-06-17 06:48:13,651] ({FIFOScheduler-interpreter_238135472-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1655448486850_276207388 started by scheduler interpreter_238135472
    >    INFO [2022-06-17 06:48:13,781] ({FIFOScheduler-interpreter_238135472-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448486850_276207388 finished by scheduler interpreter_238135472 with status FINISHED
    >    INFO [2022-06-17 06:48:14,776] ({FIFOScheduler-interpreter_238135472-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1655448486851_1039163803 started by scheduler interpreter_238135472
    >    INFO [2022-06-17 06:48:14,780] ({FIFOScheduler-interpreter_238135472-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448486851_1039163803 finished by scheduler interpreter_238135472 with status FINISHED
    >    INFO [2022-06-17 06:48:21,314] ({pool-3-thread-1} PySparkInterpreter.java[close]:112) - Close PySparkInterpreter
    >    INFO [2022-06-17 06:48:21,314] ({pool-3-thread-1} PythonInterpreter.java[close]:258) - Kill python process
    >    INFO [2022-06-17 06:48:21,322] ({pool-3-thread-1} RemoteInterpreterServer.java[shutdown]:245) - Unregister interpreter process
    >    WARN [2022-06-17 06:48:21,328] ({Exec Default Executor} ProcessLauncher.java[onProcessFailed]:134) - Process with cmd [python, /tmp/1655448401905-0/zeppelin_python.py, 10.10.2.210, 38015] is failed due to
    >    org.apache.commons.exec.ExecuteException: Process exited with an error: 143 (Exit value: 143)
    >           at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
    >           at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
    >           at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
    >           at java.lang.Thread.run(Thread.java:748)
    >    INFO [2022-06-17 06:48:21,330] ({Exec Default Executor} ProcessLauncher.java[transition]:109) - Process state is transitioned to TERMINATED
    >    INFO [2022-06-17 06:48:21,330] ({ShutdownThread} RemoteInterpreterServer.java[run]:646) - Shutting down...
    >    INFO [2022-06-17 06:48:21,330] ({ShutdownThread} RemoteInterpreterServer.java[run]:647) - Shutdown initialized by ShutdownCall
    >    INFO [2022-06-17 06:48:21,330] ({ShutdownThread} SparkInterpreter.java[close]:182) - Close SparkInterpreter
    >    INFO [2022-06-17 06:48:21,349] ({ShutdownThread} BaseSparkScalaInterpreter.scala[cleanupStagingDirInternal]:218) - Deleted staging directory hdfs://master01:9000/albert/Fipa/.sparkStaging/application_1655122472463_1598
    >    INFO [2022-06-17 06:48:21,361] ({ShutdownThread} AbstractConnector.java[doStop]:381) - Stopped Spark@52ba061{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
    >    INFO [2022-06-17 06:48:21,363] ({ShutdownThread} Logging.scala[logInfo]:57) - Stopped Spark web UI at http://zeppelin:4040
    >    INFO [2022-06-17 06:48:21,368] ({YARN application state monitor} Logging.scala[logInfo]:57) - Interrupting monitor thread
    >    INFO [2022-06-17 06:48:21,370] ({ShutdownThread} Logging.scala[logInfo]:57) - Shutting down all executors
    >    INFO [2022-06-17 06:48:21,371] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Asking each executor to shut down
    >    INFO [2022-06-17 06:48:21,377] ({ShutdownThread} Logging.scala[logInfo]:57) - YARN client scheduler backend Stopped
    >    INFO [2022-06-17 06:48:21,385] ({dispatcher-event-loop-34} Logging.scala[logInfo]:57) - MapOutputTrackerMasterEndpoint stopped!
    >    WARN [2022-06-17 06:48:21,393] ({rpc-server-4-5} NioEventLoop.java[unexpectedSelectorWakeup]:554) - Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@78fe626c.
    >    INFO [2022-06-17 06:48:21,394] ({rpc-server-4-5} NioEventLoop.java[rebuildSelector0]:430) - Migrated 1 channel(s) to the new Selector.
    >    INFO [2022-06-17 06:48:21,397] ({ShutdownThread} Logging.scala[logInfo]:57) - MemoryStore cleared
    >    INFO [2022-06-17 06:48:21,397] ({ShutdownThread} Logging.scala[logInfo]:57) - BlockManager stopped
    >    INFO [2022-06-17 06:48:21,400] ({ShutdownThread} Logging.scala[logInfo]:57) - BlockManagerMaster stopped
    >    INFO [2022-06-17 06:48:21,406] ({dispatcher-event-loop-47} Logging.scala[logInfo]:57) - OutputCommitCoordinator stopped!
    >    INFO [2022-06-17 06:48:21,414] ({ShutdownThread} Logging.scala[logInfo]:57) - Successfully stopped SparkContext
    >    INFO [2022-06-17 06:48:21,414] ({ShutdownThread} Logging.scala[logInfo]:57) - SparkContext already stopped.
    >    INFO [2022-06-17 06:48:21,417] ({ShutdownThread} SchedulerFactory.java[destroy]:61) - Destroy all executors
    >    INFO [2022-06-17 06:48:21,418] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler interpreter_238135472
    >    WARN [2022-06-17 06:48:21,418] ({SchedulerFactory2} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    INFO [2022-06-17 06:48:21,418] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler interpreter_1908998913
    >    INFO [2022-06-17 06:48:21,419] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler interpreter_182853103
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory7} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory8} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    INFO [2022-06-17 06:48:21,419] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler interpreter_79060382
    >    INFO [2022-06-17 06:48:21,419] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler org.apache.zeppelin.spark.SparkSqlInterpreter861182032
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory6} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory3} AbstractScheduler.java[run]:91) - ParallelScheduler is interrupted
    >    INFO [2022-06-17 06:48:21,419] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler org.apache.zeppelin.spark.SparkRInterpreter526653806
    >    INFO [2022-06-17 06:48:21,419] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler interpreter_768785502
    >    INFO [2022-06-17 06:48:21,419] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler interpreter_409799365
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory4} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory5} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory1} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    INFO [2022-06-17 06:48:21,420] ({RemoteInterpreterServer-Thread} RemoteInterpreterServer.java[run]:199) - RemoteInterpreterServer-Thread finished
    >    INFO [2022-06-17 06:48:21,420] ({main} RemoteInterpreterServer.java[main]:317) - RemoteInterpreterServer thread is finished
    >    INFO [2022-06-17 06:48:21,423] ({shutdown-hook-0} Logging.scala[logInfo]:57) - Shutdown hook called
    >    INFO [2022-06-17 06:48:21,424] ({shutdown-hook-0} Logging.scala[logInfo]:57) - Deleting directory /tmp/spark-3a13c1dd-b1a5-421b-8154-9d2f56dfe1d2
    >    INFO [2022-06-17 06:48:21,427] ({shutdown-hook-0} Logging.scala[logInfo]:57) - Deleting directory /mnt/cinder/vdc/spark/temp/spark-5759feb6-4497-4370-a5ca-ddbc6c067218/pyspark-de460db7-1e8a-4134-8ceb-f9d0a912610f
    >    INFO [2022-06-17 06:48:21,430] ({shutdown-hook-0} Logging.scala[logInfo]:57) - Deleting directory /mnt/cinder/vdc/spark/temp/spark-5759feb6-4497-4370-a5ca-ddbc6c067218

    #
    # Some of this looks bad (exit code 143 is associated with out of memory issues).
    # https://stackoverflow.com/questions/42972908/container-killed-by-the-applicationmaster-exit-code-is-143
    # https://stackoverflow.com/a/52403247
    #
    # On the other hand, the Hadoop/Yarn UI shows this application as FINISHED.
    # http://master01:8088/cluster/app/application_1655122472463_1598

    >   User:                       Fipa
    >   Name:                       spark-Fipa
    >   Application Type:           SPARK
    >   Application Tags:           -
    >   Application Priority:       0 (Higher Integer value indicates higher priority)
    >   YarnApplicationState:       FINISHED
    >   Queue:                      default
    >   FinalStatus Reported by AM: SUCCEEDED
    >   Started:                    Fri Jun 17 06:46:27 +0000 2022
    >   Launched:                   Fri Jun 17 06:46:28 +0000 2022
    >   Finished:                   Fri Jun 17 06:48:21 +0000 2022
    >   Elapsed:                    1mins, 53sec
    >   Tracking URL:               History
    >   Log Aggregation Status:     DISABLED
    >   Application Timeout (Remaining Time): Unlimited
    >   Diagnostics:
    >   Unmanaged Application:      false
    >   Application Node Label expression:  <Not set>
    >   AM container Node Label expression: <DEFAULT_PARTITION>

    #
    # There is one application left in the RUNNING state.
    # http://master01:8088/cluster/app/application_1655122472463_1609

    >   User:                       Carrovieus
    >   Name:                       spark-Carrovieus
    >   Application Type:           SPARK
    >   Application Tags:
    >   Application Priority:       0 (Higher Integer value indicates higher priority)
    >   YarnApplicationState:       RUNNING: AM has registered with RM and started running.
    >   Queue:                      default
    >   FinalStatus Reported by AM: Application has not completed yet.
    >   Started:                    Fri Jun 17 06:47:23 +0000 2022
    >   Launched:                   Fri Jun 17 06:51:01 +0000 2022
    >   Finished:                   N/A
    >   Elapsed:                    4hrs, 9mins, 43sec
    >   Tracking URL:               ApplicationMaster
    >   Log Aggregation Status:     DISABLED
    >   Application Timeout (Remaining Time): Unlimited
    >   Diagnostics:
    >   Unmanaged Application:      false
    >   Application Node Label expression:  <Not set>
    >   AM container Node Label expression: <DEFAULT_PARTITION>


# -----------------------------------------------------
# Check the Zeppelin log for Carrovieus ...
#[user@zeppelin]

    less zeppelin-interpreter-spark-Carrovieus-Carrovieus-$(id -un)-$(hostname).log

    #
    # This looks good ..

    >    ....
    >    INFO [2022-06-17 06:55:01,987] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448901045_1404462898 finished by scheduler interpreter_723077581 with status FINISHED
    >    INFO [2022-06-17 06:55:02,083] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1655448901045_635421648 started by scheduler interpreter_723077581
    >    INFO [2022-06-17 06:55:02,090] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448901045_635421648 finished by scheduler interpreter_723077581 with status FINISHED
    >    INFO [2022-06-17 06:55:03,163] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1655448901045_490513828 started by scheduler interpreter_723077581
    >    INFO [2022-06-17 06:55:03,325] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448901045_490513828 finished by scheduler interpreter_723077581 with status FINISHED
    >    INFO [2022-06-17 06:55:04,290] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1655448901045_1652921845 started by scheduler interpreter_723077581
    >    INFO [2022-06-17 06:55:04,294] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448901045_1652921845 finished by scheduler interpreter_723077581 with status FINISHED
    >    ....
    >    INFO [2022-06-17 06:55:04,389] ({pool-3-thread-1} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_1994647206
    >    INFO [2022-06-17 06:55:04,389] ({pool-3-thread-1} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: org.apache.zeppelin.spark.SparkRInterpreter1459163571
    >    INFO [2022-06-17 06:55:04,390] ({pool-3-thread-1} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_600913622
    >    INFO [2022-06-17 06:55:04,390] ({pool-3-thread-1} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_1255350662
    >    INFO [2022-06-17 06:55:04,390] ({pool-3-thread-1} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_1530179903
    >    ....

    #
    # This does not look good ..

    >    ....
    >    INFO [2022-06-17 06:55:52,336] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Requesting to kill executor(s) 2, 8, 3
    >    INFO [2022-06-17 06:55:52,340] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Actual list of executor(s) to be killed is 2, 8, 3
    >    INFO [2022-06-17 06:55:52,373] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Executors 2,8,3 removed due to idle timeout.
    >    INFO [2022-06-17 06:55:52,473] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Requesting to kill executor(s) 7, 1, 4, 6
    >    INFO [2022-06-17 06:55:52,473] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Actual list of executor(s) to be killed is 7, 1, 4, 6
    >    INFO [2022-06-17 06:55:52,479] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Executors 7,1,4,6 removed due to idle timeout.
    >    INFO [2022-06-17 06:55:52,765] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 8.
    >    INFO [2022-06-17 06:55:52,769] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Executor lost: 8 (epoch 2)
    >    INFO [2022-06-17 06:55:52,770] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 2.
    >    INFO [2022-06-17 06:55:52,770] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Trying to remove executor 8 from BlockManagerMaster.
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_16 !
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_32 !
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_8 !
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_24 !
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_0 !
    >    INFO [2022-06-17 06:55:52,772] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removing block manager BlockManagerId(8, worker02, 36583, None)
    >    INFO [2022-06-17 06:55:52,772] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Removed 8 successfully in removeExecutor
    >    INFO [2022-06-17 06:55:52,773] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Executor lost: 2 (epoch 2)
    >    INFO [2022-06-17 06:55:52,773] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Trying to remove executor 2 from BlockManagerMaster.
    >    WARN [2022-06-17 06:55:52,773] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_195 !
    >    WARN [2022-06-17 06:55:52,773] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_23 !
    >    ....
    >    ....
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_24 !
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_0 !
    >    INFO [2022-06-17 06:55:52,772] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removing block manager BlockManagerId(8, worker02, 36583, None)
    >    INFO [2022-06-17 06:55:52,772] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Removed 8 successfully in removeExecutor
    >    INFO [2022-06-17 06:55:52,773] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Executor lost: 2 (epoch 2)
    >    INFO [2022-06-17 06:55:52,773] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Trying to remove executor 2 from BlockManagerMaster.
    >    WARN [2022-06-17 06:55:52,773] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_195 !
    >    WARN [2022-06-17 06:55:52,773] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_23 !
    >    ....
    >    ....
    >    WARN [2022-06-17 06:55:52,774] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_82 !
    >    WARN [2022-06-17 06:55:52,774] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_159 !
    >    INFO [2022-06-17 06:55:52,774] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removing block manager BlockManagerId(2, worker02, 33739, None)
    >    INFO [2022-06-17 06:55:52,774] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Removed 2 successfully in removeExecutor
    >    INFO [2022-06-17 06:55:52,780] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 6.
    >    INFO [2022-06-17 06:55:52,780] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Executor lost: 6 (epoch 2)
    >    INFO [2022-06-17 06:55:52,780] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 4.
    >    INFO [2022-06-17 06:55:52,780] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Trying to remove executor 6 from BlockManagerMaster.
    >    WARN [2022-06-17 06:55:52,780] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_64 !
    >    WARN [2022-06-17 06:55:52,780] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_38 !
    >    ....
    >    ....
    >    WARN [2022-06-17 06:55:52,841] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_136 !
    >    WARN [2022-06-17 06:55:52,841] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_177 !
    >    INFO [2022-06-17 06:55:52,841] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removing block manager BlockManagerId(3, worker04, 38819, None)
    >    INFO [2022-06-17 06:55:52,841] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Removed 3 successfully in removeExecutor
    >    INFO [2022-06-17 06:55:52,848] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Executor 7 on worker04 killed by driver.
    >    INFO [2022-06-17 06:55:52,870] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Executor 3 on worker04 killed by driver.
    >    INFO [2022-06-17 07:21:13,066] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removed broadcast_9_piece0 on zeppelin:36463 in memory (size: 13.1 KiB, free: 30.5 GiB)
    >    INFO [2022-06-17 07:21:13,067] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removed broadcast_9_piece0 on worker06:37225 in memory (size: 13.1 KiB, free: 3.6 GiB)
    >    INFO [2022-06-17 07:21:13,072] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removed broadcast_10_piece0 on zeppelin:36463 in memory (size: 17.0 KiB, free: 30.5 GiB)
    >    INFO [2022-06-17 07:21:13,072] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removed broadcast_10_piece0 on worker06:37225 in memory (size: 17.0 KiB, free: 3.6 GiB)


# -----------------------------------------------------
# Process list in the client container matches this.
#[root@ansibler]

    ps -ef

    >   UID          PID    PPID  C STIME TTY          TIME CMD
    >   root           1       0  0 Jun13 pts/0    00:00:01 bash
    >   root       21693       0  0 Jun14 pts/1    00:00:00 bash
    >   root       21715       1  0 Jun14 ?        00:02:42 ssh: /root/.ssh/fedora@128.232.222.196:22 [mux]
    >   root       26273   21693  0 Jun15 pts/1    00:00:00 ssh zeppelin
    >   root       36558       0  0 Jun15 pts/2    00:00:00 bash
    >   root       36578   36558  0 Jun15 pts/2    00:00:00 ssh zeppelin
    >   root       44529       1  0 02:38 pts/0    00:00:00 bash
    >   root       44530       1  0 02:38 pts/0    00:00:00 tee /tmp/test-loop.json
    >   root       49242   44529  0 06:46 pts/0    00:00:00 /bin/python3 /tmp/run-benchmark.py http://zeppelin:8080 /deployments/zeppelin/test/config/quick.json
    >   root       49243   49242  0 06:46 pts/0    00:00:00 /bin/python3 /tmp/run-benchmark.py http://zeppelin:8080 /deployments/zeppelin/test/config/quick.json
    >   ....
    >   ....
    >   root       49260   49242  0 06:46 pts/0    00:00:00 /bin/python3 /tmp/run-benchmark.py http://zeppelin:8080 /deployments/zeppelin/test/config/quick.json
    >   root       49261   49242  0 06:46 pts/0    00:00:00 /bin/python3 /tmp/run-benchmark.py http://zeppelin:8080 /deployments/zeppelin/test/config/quick.json
    >   root       49322   49243  0 06:48 pts/0    00:00:00 [zdairi] <defunct>
    >   root       49323   49243  0 06:48 pts/0    00:00:00 [zdairi] <defunct>
    >   ....
    >   ....
    >   root       49447   49250  0 06:54 pts/0    00:00:00 [zdairi] <defunct>
    >   root       49448   49250  0 06:54 pts/0    00:00:00 [zdairi] <defunct>
    >   root       49464   49254  0 06:55 pts/0    00:00:59 /usr/bin/python3 /usr/local/bin/zdairi --config /tmp/user12.yml notebook run --notebook 2H65J1XS4
    >   root       49465   49252  0 06:55 pts/0    00:00:00 [zdairi] <defunct>
    >   root       49466   49252  0 06:55 pts/0    00:00:00 [zdairi] <defunct>
    >   ....
    >   ....
    >   root       49562   49261  0 07:02 pts/0    00:00:00 [zdairi] <defunct>
    >   root       49563   49261  0 07:02 pts/0    00:00:00 [zdairi] <defunct>
    >   root       49564       0  0 10:19 pts/3    00:00:00 bash
    >   root       49582   49564  0 10:20 pts/3    00:00:00 ps -ef


    cat /tmp/user12.yml

    >   zeppelin_url: http://zeppelin:8080
    >   zeppelin_auth: true
    >   zeppelin_user: Carrovieus
    >   zeppelin_password: ########


# -----------------------------------------------------
# Check the memory and disc on the workers.
#[root@ansibler]

    workers=(
        worker01
        worker02
        worker03
        worker04
        worker05
        worker06
        )

    for worker in ${workers[@]}
    do
        echo ""
        echo "worker [${worker}]"
        ssh "${worker}" \
            '
            hostname
            date
            echo
            free -h
            echo
            df -h /
            '
    done

    >   worker [worker01]
    >   iris-gaia-blue-20220613-worker01
    >   Fri Jun 17 11:13:14 UTC 2022
    >   
    >                 total        used        free      shared  buff/cache   available
    >   Mem:           42Gi       2.8Gi        13Gi       2.0Mi        25Gi        38Gi
    >   Swap:            0B          0B          0B
    >   
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.2G   15G  23% /

    >   worker [worker02]
    >   iris-gaia-blue-20220613-worker02
    >   Fri Jun 17 11:13:14 UTC 2022
    >   
    >                 total        used        free      shared  buff/cache   available
    >   Mem:           42Gi       2.7Gi        14Gi       2.0Mi        25Gi        38Gi
    >   Swap:            0B          0B          0B
    >   
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.2G   15G  23% /

    >   worker [worker03]
    >   iris-gaia-blue-20220613-worker03
    >   Fri Jun 17 11:13:14 UTC 2022
    >   
    >                 total        used        free      shared  buff/cache   available
    >   Mem:           42Gi       3.1Gi        13Gi       2.0Mi        25Gi        38Gi
    >   Swap:            0B          0B          0B
    >   
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.2G   15G  23% /

    >   worker [worker04]
    >   iris-gaia-blue-20220613-worker04
    >   Fri Jun 17 11:13:14 UTC 2022
    >   
    >                 total        used        free      shared  buff/cache   available
    >   Mem:           42Gi       3.3Gi        14Gi       2.0Mi        24Gi        38Gi
    >   Swap:            0B          0B          0B
    >   
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.2G   15G  23% /

    >   worker [worker05]
    >   iris-gaia-blue-20220613-worker05
    >   Fri Jun 17 11:13:14 UTC 2022
    >   
    >                 total        used        free      shared  buff/cache   available
    >   Mem:           42Gi       2.8Gi        12Gi       2.0Mi        26Gi        38Gi
    >   Swap:            0B          0B          0B
    >   
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.2G   15G  23% /

    >   worker [worker06]
    >   iris-gaia-blue-20220613-worker06
    >   Fri Jun 17 11:13:14 UTC 2022
    >   
    >                 total        used        free      shared  buff/cache   available
    >   Mem:           42Gi       5.5Gi        10Gi       2.0Mi        25Gi        36Gi
    >   Swap:            0B          0B          0B
    >   
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.2G   15G  23% /



