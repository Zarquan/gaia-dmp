#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Try to find out more about the limits on concurrent users.
        Follow on from yesterda's notes 20220615-01-concurrent-tests.txt.

    Result:

        Work in progress ...

        TODO move from quick to complex test sets
        TODO move from 4 to 8 concurrent users
        TODO loop until <time>
        TODO find the minimum looppause


# -----------------------------------------------------
# Create our long-loop function.
# https://stackoverflow.com/questions/17548064/how-to-have-a-bash-script-loop-until-a-specific-time
# https://stackoverflow.com/a/17548151
# https://linuxize.com/post/bash-increment-decrement-variable/
#[root@ansibler]

    long-loop()
        {
        local usercount=${1:?'usercount required'}
        local loopfinish=${2:?'loopfinish required'}
        local looppause=${3:-10}
        local delaystart=${4:-1}
        local delaynotebook=${5:-1}

        rm -f /tmp/results/*

cat << EOF
    {
    "usercount": "${usercount}",
    "loopcount": "${loopcount}",
    "looppause": "${looppause}",
    "delaystart": "${delaystart}",
    "delaynotebook": "${delaynotebook}",
    "iterations": [
EOF

        local comma=''
        local iter=0
        while [ $(date "+%H") -lt ${loopfinish} ]
        do

            testname="test-$(date '+%Y%m%dT%H%M%S')-iter-$(printf "%02d" ${iter})"

cat << EOF
            ${comma}
            {
            "iteration": ${iter},
            "testname": "${testname}",
            "threads":
EOF

            sleep "${looppause}"

            /tmp/run-benchmark.py \
                "${endpoint:?}" \
                "${testconfig:?}" \
                "${testusers:?}" \
                "${usercount:?}" \
                "${delaystart:?}" \
                "${delaynotebook:?}" \
            > "/tmp/results/${testname:?}.txt"

            filter-results "${testname:?}"

cat << EOF
            }
EOF
            comma=','
            ((iter+=1))

        done

cat << EOF
        ]
    }
EOF
        }


# -----------------------------------------------------
# Test with 19 users doing 2 loops until 10am.
#[root@ansibler]

    # local usercount=${1:?'usercount required'}
    # local loopfinish=${2:?'loopfinish required'}
    # local looppause=${3:-10}
    # local delaystart=${4:-1}
    # local delaynotebook=${5:-1}

    long-loop 19 10 10 5 5 \
    | tee /tmp/test-loop.json


    >   ....
    >               {
    >               "iteration": 13,
    >               "testname": "test-20220617T062603-iter-13",
    >               "threads": [
    >                   [
    >                       {
    >                       "name": "GaiaDMPSetup",
    >                       "value": "PASS",
    >                       "time": 36.99,
    >                       "start": "2022-06-17T06:26:13.509228",
    >                       "finish": "2022-06-17T06:26:50.498948"
    >                       },
    >                       ....
    >                       ....
    >                       {
    >                       "name": "Source_counts_over_the_sky.json",
    >                       "value": "PASS",
    >                       "time": 17.83,
    >                       "start": "2022-06-17T06:27:35.418406",
    >                       "finish": "2022-06-17T06:27:53.252655"
    >                       }
    >                   ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >                   [ .... ],
    >               },
    >               {
    >               "iteration": 14,
    >               "testname": "test-20220617T064603-iter-14",
    >               "threads": [
    >                   [
    >                   ....

    #
    # Test reached iteration 14, and stopped around 07:03 am.
    # Everything looks normal .. just stopped.
    #


# -----------------------------------------------------
# Check the Zeppelin logs ...
#[user@zeppelin]

    #
    # The Zeppelin log looks normal ... just stopped.

    less  ${HOME}/zeppelin/logs/zeppelin-$(id -un)-$(hostname).log

    >    ....
    >    ....
    >    WARN [2022-06-17 07:02:57,859] ({Exec Default Executor} ExecRemoteInterpreterProcess.java[onProcessComplete]:226) - Process is exited with exit value 0
    >    INFO [2022-06-17 07:02:57,860] ({Exec Default Executor} ProcessLauncher.java[transition]:109) - Process state is transitioned to COMPLETED
    >    WARN [2022-06-17 07:02:58,055] ({Exec Default Executor} ExecRemoteInterpreterProcess.java[onProcessComplete]:226) - Process is exited with exit value 0
    >    INFO [2022-06-17 07:02:58,056] ({Exec Default Executor} ProcessLauncher.java[transition]:109) - Process state is transitioned to COMPLETED
    >    INFO [2022-06-17 07:03:00,338] ({qtp2128029086-288820} ExecRemoteInterpreterProcess.java[stop]:136) - Remote exec process of interpreter group: spark-Bellgrin is terminated
    >    WARN [2022-06-17 07:03:00,338] ({qtp2128029086-288820} AuthorizationService.java[getOwners]:230) - No noteAuth found for noteId: 2H7RWP7WY
    >    INFO [2022-06-17 07:03:00,344] ({qtp2128029086-288823} ExecRemoteInterpreterProcess.java[stop]:136) - Remote exec process of interpreter group: md-Bellgrin is terminated
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:784) - Start to copy dependencies for interpreter: spark
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:795) - Finish copy dependencies for interpreter: spark
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:784) - Start to copy dependencies for interpreter: spark
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:795) - Finish copy dependencies for interpreter: spark
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:784) - Start to copy dependencies for interpreter: md
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:795) - Finish copy dependencies for interpreter: md
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:784) - Start to copy dependencies for interpreter: spark
    >    INFO [2022-06-17 07:03:00,346] ({qtp2128029086-288823} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:795) - Finish copy dependencies for interpreter: spark
    >    INFO [2022-06-17 07:03:00,345] ({qtp2128029086-288316} ManagedInterpreterGroup.java[close]:102) - Close Session: shared_session for interpreter setting: md
    >    WARN [2022-06-17 07:03:00,346] ({qtp2128029086-288823} AuthorizationService.java[getOwners]:230) - No noteAuth found for noteId: 2H5PXXMKB
    >    INFO [2022-06-17 07:03:00,346] ({qtp2128029086-288316} ManagedInterpreterGroup.java[close]:106) - Remove this InterpreterGroup: md-Bellgrin as all the sessions are closed
    >    INFO [2022-06-17 07:03:00,346] ({qtp2128029086-288316} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:784) - Start to copy dependencies for interpreter: md
    >    INFO [2022-06-17 07:03:00,346] ({qtp2128029086-288316} InterpreterSettingManager.java[copyDependenciesFromLocalPath]:795) - Finish copy dependencies for interpreter: md
    >    WARN [2022-06-17 07:03:00,347] ({qtp2128029086-288316} AuthorizationService.java[getOwners]:230) - No noteAuth found for noteId: 2H78KAXNA
    >
    >    INFO [2022-06-17 07:10:07,760] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:275) - Validating all active sessions...
    >    INFO [2022-06-17 07:10:07,766] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:308) - Finished session validation.  No sessions were stopped.
    >
    >    INFO [2022-06-17 08:10:07,760] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:275) - Validating all active sessions...
    >    INFO [2022-06-17 08:10:07,765] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:308) - Finished session validation.  No sessions were stopped.
    >
    >    INFO [2022-06-17 09:10:07,760] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:275) - Validating all active sessions...
    >    INFO [2022-06-17 09:10:07,763] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:308) - Finished session validation.  No sessions were stopped.
    >
    >    INFO [2022-06-17 10:10:07,760] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:275) - Validating all active sessions...
    >    INFO [2022-06-17 10:10:07,764] ({SessionValidationThread-1} AbstractValidatingSessionManager.java[validateSessions]:308) - Finished session validation.  No sessions were stopped.


    #
    # The Spark interpreter log has some clues.

    less zeppelin-interpreter-spark-Fipa-Fipa-$(id -un)-$(hostname).log

    >    ....
    >    ....
    >    INFO [2022-06-17 06:48:12,591] ({FIFOScheduler-interpreter_238135472-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448486850_407152630 finished by scheduler interpreter_238135472 with status FINISHED
    >    INFO [2022-06-17 06:48:13,651] ({FIFOScheduler-interpreter_238135472-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1655448486850_276207388 started by scheduler interpreter_238135472
    >    INFO [2022-06-17 06:48:13,781] ({FIFOScheduler-interpreter_238135472-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448486850_276207388 finished by scheduler interpreter_238135472 with status FINISHED
    >    INFO [2022-06-17 06:48:14,776] ({FIFOScheduler-interpreter_238135472-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1655448486851_1039163803 started by scheduler interpreter_238135472
    >    INFO [2022-06-17 06:48:14,780] ({FIFOScheduler-interpreter_238135472-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448486851_1039163803 finished by scheduler interpreter_238135472 with status FINISHED
    >    INFO [2022-06-17 06:48:21,314] ({pool-3-thread-1} PySparkInterpreter.java[close]:112) - Close PySparkInterpreter
    >    INFO [2022-06-17 06:48:21,314] ({pool-3-thread-1} PythonInterpreter.java[close]:258) - Kill python process
    >    INFO [2022-06-17 06:48:21,322] ({pool-3-thread-1} RemoteInterpreterServer.java[shutdown]:245) - Unregister interpreter process
    >    WARN [2022-06-17 06:48:21,328] ({Exec Default Executor} ProcessLauncher.java[onProcessFailed]:134) - Process with cmd [python, /tmp/1655448401905-0/zeppelin_python.py, 10.10.2.210, 38015] is failed due to
    >    org.apache.commons.exec.ExecuteException: Process exited with an error: 143 (Exit value: 143)
    >           at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)
    >           at org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)
    >           at org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)
    >           at java.lang.Thread.run(Thread.java:748)
    >    INFO [2022-06-17 06:48:21,330] ({Exec Default Executor} ProcessLauncher.java[transition]:109) - Process state is transitioned to TERMINATED
    >    INFO [2022-06-17 06:48:21,330] ({ShutdownThread} RemoteInterpreterServer.java[run]:646) - Shutting down...
    >    INFO [2022-06-17 06:48:21,330] ({ShutdownThread} RemoteInterpreterServer.java[run]:647) - Shutdown initialized by ShutdownCall
    >    INFO [2022-06-17 06:48:21,330] ({ShutdownThread} SparkInterpreter.java[close]:182) - Close SparkInterpreter
    >    INFO [2022-06-17 06:48:21,349] ({ShutdownThread} BaseSparkScalaInterpreter.scala[cleanupStagingDirInternal]:218) - Deleted staging directory hdfs://master01:9000/albert/Fipa/.sparkStaging/application_1655122472463_1598
    >    INFO [2022-06-17 06:48:21,361] ({ShutdownThread} AbstractConnector.java[doStop]:381) - Stopped Spark@52ba061{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
    >    INFO [2022-06-17 06:48:21,363] ({ShutdownThread} Logging.scala[logInfo]:57) - Stopped Spark web UI at http://zeppelin:4040
    >    INFO [2022-06-17 06:48:21,368] ({YARN application state monitor} Logging.scala[logInfo]:57) - Interrupting monitor thread
    >    INFO [2022-06-17 06:48:21,370] ({ShutdownThread} Logging.scala[logInfo]:57) - Shutting down all executors
    >    INFO [2022-06-17 06:48:21,371] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Asking each executor to shut down
    >    INFO [2022-06-17 06:48:21,377] ({ShutdownThread} Logging.scala[logInfo]:57) - YARN client scheduler backend Stopped
    >    INFO [2022-06-17 06:48:21,385] ({dispatcher-event-loop-34} Logging.scala[logInfo]:57) - MapOutputTrackerMasterEndpoint stopped!
    >    WARN [2022-06-17 06:48:21,393] ({rpc-server-4-5} NioEventLoop.java[unexpectedSelectorWakeup]:554) - Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@78fe626c.
    >    INFO [2022-06-17 06:48:21,394] ({rpc-server-4-5} NioEventLoop.java[rebuildSelector0]:430) - Migrated 1 channel(s) to the new Selector.
    >    INFO [2022-06-17 06:48:21,397] ({ShutdownThread} Logging.scala[logInfo]:57) - MemoryStore cleared
    >    INFO [2022-06-17 06:48:21,397] ({ShutdownThread} Logging.scala[logInfo]:57) - BlockManager stopped
    >    INFO [2022-06-17 06:48:21,400] ({ShutdownThread} Logging.scala[logInfo]:57) - BlockManagerMaster stopped
    >    INFO [2022-06-17 06:48:21,406] ({dispatcher-event-loop-47} Logging.scala[logInfo]:57) - OutputCommitCoordinator stopped!
    >    INFO [2022-06-17 06:48:21,414] ({ShutdownThread} Logging.scala[logInfo]:57) - Successfully stopped SparkContext
    >    INFO [2022-06-17 06:48:21,414] ({ShutdownThread} Logging.scala[logInfo]:57) - SparkContext already stopped.
    >    INFO [2022-06-17 06:48:21,417] ({ShutdownThread} SchedulerFactory.java[destroy]:61) - Destroy all executors
    >    INFO [2022-06-17 06:48:21,418] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler interpreter_238135472
    >    WARN [2022-06-17 06:48:21,418] ({SchedulerFactory2} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    INFO [2022-06-17 06:48:21,418] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler interpreter_1908998913
    >    INFO [2022-06-17 06:48:21,419] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler interpreter_182853103
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory7} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory8} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    INFO [2022-06-17 06:48:21,419] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler interpreter_79060382
    >    INFO [2022-06-17 06:48:21,419] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler org.apache.zeppelin.spark.SparkSqlInterpreter861182032
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory6} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory3} AbstractScheduler.java[run]:91) - ParallelScheduler is interrupted
    >    INFO [2022-06-17 06:48:21,419] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler org.apache.zeppelin.spark.SparkRInterpreter526653806
    >    INFO [2022-06-17 06:48:21,419] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler interpreter_768785502
    >    INFO [2022-06-17 06:48:21,419] ({ShutdownThread} SchedulerFactory.java[destroy]:65) - Stopping Scheduler interpreter_409799365
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory4} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory5} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    WARN [2022-06-17 06:48:21,419] ({SchedulerFactory1} AbstractScheduler.java[run]:91) - FIFOScheduler is interrupted
    >    INFO [2022-06-17 06:48:21,420] ({RemoteInterpreterServer-Thread} RemoteInterpreterServer.java[run]:199) - RemoteInterpreterServer-Thread finished
    >    INFO [2022-06-17 06:48:21,420] ({main} RemoteInterpreterServer.java[main]:317) - RemoteInterpreterServer thread is finished
    >    INFO [2022-06-17 06:48:21,423] ({shutdown-hook-0} Logging.scala[logInfo]:57) - Shutdown hook called
    >    INFO [2022-06-17 06:48:21,424] ({shutdown-hook-0} Logging.scala[logInfo]:57) - Deleting directory /tmp/spark-3a13c1dd-b1a5-421b-8154-9d2f56dfe1d2
    >    INFO [2022-06-17 06:48:21,427] ({shutdown-hook-0} Logging.scala[logInfo]:57) - Deleting directory /mnt/cinder/vdc/spark/temp/spark-5759feb6-4497-4370-a5ca-ddbc6c067218/pyspark-de460db7-1e8a-4134-8ceb-f9d0a912610f
    >    INFO [2022-06-17 06:48:21,430] ({shutdown-hook-0} Logging.scala[logInfo]:57) - Deleting directory /mnt/cinder/vdc/spark/temp/spark-5759feb6-4497-4370-a5ca-ddbc6c067218

    #
    # Some of this looks bad (exit code 143 is associated with out of memory issues).
    # https://stackoverflow.com/questions/42972908/container-killed-by-the-applicationmaster-exit-code-is-143
    # https://stackoverflow.com/a/52403247
    #
    # On the other hand, the Hadoop/Yarn UI shows this application as FINISHED.
    # http://master01:8088/cluster/app/application_1655122472463_1598

    >   User:                       Fipa
    >   Name:                       spark-Fipa
    >   Application Type:           SPARK
    >   Application Tags:           -
    >   Application Priority:       0 (Higher Integer value indicates higher priority)
    >   YarnApplicationState:       FINISHED
    >   Queue:                      default
    >   FinalStatus Reported by AM: SUCCEEDED
    >   Started:                    Fri Jun 17 06:46:27 +0000 2022
    >   Launched:                   Fri Jun 17 06:46:28 +0000 2022
    >   Finished:                   Fri Jun 17 06:48:21 +0000 2022
    >   Elapsed:                    1mins, 53sec
    >   Tracking URL:               History
    >   Log Aggregation Status:     DISABLED
    >   Application Timeout (Remaining Time): Unlimited
    >   Diagnostics:
    >   Unmanaged Application:      false
    >   Application Node Label expression:  <Not set>
    >   AM container Node Label expression: <DEFAULT_PARTITION>

    #
    # There is one application left in the RUNNING state.
    # http://master01:8088/cluster/app/application_1655122472463_1609

    >   User:                       Carrovieus
    >   Name:                       spark-Carrovieus
    >   Application Type:           SPARK
    >   Application Tags:
    >   Application Priority:       0 (Higher Integer value indicates higher priority)
    >   YarnApplicationState:       RUNNING: AM has registered with RM and started running.
    >   Queue:                      default
    >   FinalStatus Reported by AM: Application has not completed yet.
    >   Started:                    Fri Jun 17 06:47:23 +0000 2022
    >   Launched:                   Fri Jun 17 06:51:01 +0000 2022
    >   Finished:                   N/A
    >   Elapsed:                    4hrs, 9mins, 43sec
    >   Tracking URL:               ApplicationMaster
    >   Log Aggregation Status:     DISABLED
    >   Application Timeout (Remaining Time): Unlimited
    >   Diagnostics:
    >   Unmanaged Application:      false
    >   Application Node Label expression:  <Not set>
    >   AM container Node Label expression: <DEFAULT_PARTITION>


# -----------------------------------------------------
# Check the Zeppelin log for Carrovieus ...
#[user@zeppelin]

    less zeppelin-interpreter-spark-Carrovieus-Carrovieus-$(id -un)-$(hostname).log

    #
    # This looks good ..

    >    ....
    >    INFO [2022-06-17 06:55:01,987] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448901045_1404462898 finished by scheduler interpreter_723077581 with status FINISHED
    >    INFO [2022-06-17 06:55:02,083] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1655448901045_635421648 started by scheduler interpreter_723077581
    >    INFO [2022-06-17 06:55:02,090] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448901045_635421648 finished by scheduler interpreter_723077581 with status FINISHED
    >    INFO [2022-06-17 06:55:03,163] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1655448901045_490513828 started by scheduler interpreter_723077581
    >    INFO [2022-06-17 06:55:03,325] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448901045_490513828 finished by scheduler interpreter_723077581 with status FINISHED
    >    INFO [2022-06-17 06:55:04,290] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:127) - Job paragraph_1655448901045_1652921845 started by scheduler interpreter_723077581
    >    INFO [2022-06-17 06:55:04,294] ({FIFOScheduler-interpreter_723077581-Worker-1} AbstractScheduler.java[runJob]:154) - Job paragraph_1655448901045_1652921845 finished by scheduler interpreter_723077581 with status FINISHED
    >    ....
    >    INFO [2022-06-17 06:55:04,389] ({pool-3-thread-1} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_1994647206
    >    INFO [2022-06-17 06:55:04,389] ({pool-3-thread-1} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: org.apache.zeppelin.spark.SparkRInterpreter1459163571
    >    INFO [2022-06-17 06:55:04,390] ({pool-3-thread-1} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_600913622
    >    INFO [2022-06-17 06:55:04,390] ({pool-3-thread-1} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_1255350662
    >    INFO [2022-06-17 06:55:04,390] ({pool-3-thread-1} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_1530179903
    >    ....

    #
    # This does not look good ..

    >    ....
    >    INFO [2022-06-17 06:55:52,336] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Requesting to kill executor(s) 2, 8, 3
    >    INFO [2022-06-17 06:55:52,340] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Actual list of executor(s) to be killed is 2, 8, 3
    >    INFO [2022-06-17 06:55:52,373] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Executors 2,8,3 removed due to idle timeout.
    >    INFO [2022-06-17 06:55:52,473] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Requesting to kill executor(s) 7, 1, 4, 6
    >    INFO [2022-06-17 06:55:52,473] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Actual list of executor(s) to be killed is 7, 1, 4, 6
    >    INFO [2022-06-17 06:55:52,479] ({spark-dynamic-executor-allocation} Logging.scala[logInfo]:57) - Executors 7,1,4,6 removed due to idle timeout.
    >    INFO [2022-06-17 06:55:52,765] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 8.
    >    INFO [2022-06-17 06:55:52,769] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Executor lost: 8 (epoch 2)
    >    INFO [2022-06-17 06:55:52,770] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 2.
    >    INFO [2022-06-17 06:55:52,770] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Trying to remove executor 8 from BlockManagerMaster.
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_16 !
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_32 !
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_8 !
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_24 !
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_0 !
    >    INFO [2022-06-17 06:55:52,772] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removing block manager BlockManagerId(8, worker02, 36583, None)
    >    INFO [2022-06-17 06:55:52,772] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Removed 8 successfully in removeExecutor
    >    INFO [2022-06-17 06:55:52,773] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Executor lost: 2 (epoch 2)
    >    INFO [2022-06-17 06:55:52,773] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Trying to remove executor 2 from BlockManagerMaster.
    >    WARN [2022-06-17 06:55:52,773] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_195 !
    >    WARN [2022-06-17 06:55:52,773] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_23 !
    >    ....
    >    ....
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_24 !
    >    WARN [2022-06-17 06:55:52,771] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_0 !
    >    INFO [2022-06-17 06:55:52,772] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removing block manager BlockManagerId(8, worker02, 36583, None)
    >    INFO [2022-06-17 06:55:52,772] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Removed 8 successfully in removeExecutor
    >    INFO [2022-06-17 06:55:52,773] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Executor lost: 2 (epoch 2)
    >    INFO [2022-06-17 06:55:52,773] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Trying to remove executor 2 from BlockManagerMaster.
    >    WARN [2022-06-17 06:55:52,773] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_195 !
    >    WARN [2022-06-17 06:55:52,773] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_23 !
    >    ....
    >    ....
    >    WARN [2022-06-17 06:55:52,774] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_82 !
    >    WARN [2022-06-17 06:55:52,774] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_159 !
    >    INFO [2022-06-17 06:55:52,774] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removing block manager BlockManagerId(2, worker02, 33739, None)
    >    INFO [2022-06-17 06:55:52,774] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Removed 2 successfully in removeExecutor
    >    INFO [2022-06-17 06:55:52,780] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 6.
    >    INFO [2022-06-17 06:55:52,780] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Executor lost: 6 (epoch 2)
    >    INFO [2022-06-17 06:55:52,780] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 4.
    >    INFO [2022-06-17 06:55:52,780] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Trying to remove executor 6 from BlockManagerMaster.
    >    WARN [2022-06-17 06:55:52,780] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_64 !
    >    WARN [2022-06-17 06:55:52,780] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_38 !
    >    ....
    >    ....
    >    WARN [2022-06-17 06:55:52,841] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_136 !
    >    WARN [2022-06-17 06:55:52,841] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_177 !
    >    INFO [2022-06-17 06:55:52,841] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removing block manager BlockManagerId(3, worker04, 38819, None)
    >    INFO [2022-06-17 06:55:52,841] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Removed 3 successfully in removeExecutor
    >    INFO [2022-06-17 06:55:52,848] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Executor 7 on worker04 killed by driver.
    >    INFO [2022-06-17 06:55:52,870] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Executor 3 on worker04 killed by driver.
    >    INFO [2022-06-17 07:21:13,066] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removed broadcast_9_piece0 on zeppelin:36463 in memory (size: 13.1 KiB, free: 30.5 GiB)
    >    INFO [2022-06-17 07:21:13,067] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removed broadcast_9_piece0 on worker06:37225 in memory (size: 13.1 KiB, free: 3.6 GiB)
    >    INFO [2022-06-17 07:21:13,072] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removed broadcast_10_piece0 on zeppelin:36463 in memory (size: 17.0 KiB, free: 30.5 GiB)
    >    INFO [2022-06-17 07:21:13,072] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removed broadcast_10_piece0 on worker06:37225 in memory (size: 17.0 KiB, free: 3.6 GiB)


# -----------------------------------------------------
# Process list in the client container matches this.
#[root@ansibler]

    ps -ef

    >   UID          PID    PPID  C STIME TTY          TIME CMD
    >   root           1       0  0 Jun13 pts/0    00:00:01 bash
    >   root       21693       0  0 Jun14 pts/1    00:00:00 bash
    >   root       21715       1  0 Jun14 ?        00:02:42 ssh: /root/.ssh/fedora@128.232.222.196:22 [mux]
    >   root       26273   21693  0 Jun15 pts/1    00:00:00 ssh zeppelin
    >   root       36558       0  0 Jun15 pts/2    00:00:00 bash
    >   root       36578   36558  0 Jun15 pts/2    00:00:00 ssh zeppelin
    >   root       44529       1  0 02:38 pts/0    00:00:00 bash
    >   root       44530       1  0 02:38 pts/0    00:00:00 tee /tmp/test-loop.json
    >   root       49242   44529  0 06:46 pts/0    00:00:00 /bin/python3 /tmp/run-benchmark.py http://zeppelin:8080 /deployments/zeppelin/test/config/quick.json
    >   root       49243   49242  0 06:46 pts/0    00:00:00 /bin/python3 /tmp/run-benchmark.py http://zeppelin:8080 /deployments/zeppelin/test/config/quick.json
    >   ....
    >   ....
    >   root       49260   49242  0 06:46 pts/0    00:00:00 /bin/python3 /tmp/run-benchmark.py http://zeppelin:8080 /deployments/zeppelin/test/config/quick.json
    >   root       49261   49242  0 06:46 pts/0    00:00:00 /bin/python3 /tmp/run-benchmark.py http://zeppelin:8080 /deployments/zeppelin/test/config/quick.json
    >   root       49322   49243  0 06:48 pts/0    00:00:00 [zdairi] <defunct>
    >   root       49323   49243  0 06:48 pts/0    00:00:00 [zdairi] <defunct>
    >   ....
    >   ....
    >   root       49447   49250  0 06:54 pts/0    00:00:00 [zdairi] <defunct>
    >   root       49448   49250  0 06:54 pts/0    00:00:00 [zdairi] <defunct>
    >   root       49464   49254  0 06:55 pts/0    00:00:59 /usr/bin/python3 /usr/local/bin/zdairi --config /tmp/user12.yml notebook run --notebook 2H65J1XS4
    >   root       49465   49252  0 06:55 pts/0    00:00:00 [zdairi] <defunct>
    >   root       49466   49252  0 06:55 pts/0    00:00:00 [zdairi] <defunct>
    >   ....
    >   ....
    >   root       49562   49261  0 07:02 pts/0    00:00:00 [zdairi] <defunct>
    >   root       49563   49261  0 07:02 pts/0    00:00:00 [zdairi] <defunct>
    >   root       49564       0  0 10:19 pts/3    00:00:00 bash
    >   root       49582   49564  0 10:20 pts/3    00:00:00 ps -ef


    cat /tmp/user12.yml

    >   zeppelin_url: http://zeppelin:8080
    >   zeppelin_auth: true
    >   zeppelin_user: Carrovieus
    >   zeppelin_password: ########


# -----------------------------------------------------
# Check the memory and disc on the workers.
#[root@ansibler]

    workers=(
        worker01
        worker02
        worker03
        worker04
        worker05
        worker06
        )

    for worker in ${workers[@]}
    do
        echo ""
        echo "worker [${worker}]"
        ssh "${worker}" \
            '
            hostname
            date
            echo
            free -h
            echo
            df -h /
            '
    done

    >   worker [worker01]
    >   iris-gaia-blue-20220613-worker01
    >   Fri Jun 17 11:13:14 UTC 2022
    >
    >                 total        used        free      shared  buff/cache   available
    >   Mem:           42Gi       2.8Gi        13Gi       2.0Mi        25Gi        38Gi
    >   Swap:            0B          0B          0B
    >
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.2G   15G  23% /

    >   worker [worker02]
    >   iris-gaia-blue-20220613-worker02
    >   Fri Jun 17 11:13:14 UTC 2022
    >
    >                 total        used        free      shared  buff/cache   available
    >   Mem:           42Gi       2.7Gi        14Gi       2.0Mi        25Gi        38Gi
    >   Swap:            0B          0B          0B
    >
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.2G   15G  23% /

    >   worker [worker03]
    >   iris-gaia-blue-20220613-worker03
    >   Fri Jun 17 11:13:14 UTC 2022
    >
    >                 total        used        free      shared  buff/cache   available
    >   Mem:           42Gi       3.1Gi        13Gi       2.0Mi        25Gi        38Gi
    >   Swap:            0B          0B          0B
    >
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.2G   15G  23% /

    >   worker [worker04]
    >   iris-gaia-blue-20220613-worker04
    >   Fri Jun 17 11:13:14 UTC 2022
    >
    >                 total        used        free      shared  buff/cache   available
    >   Mem:           42Gi       3.3Gi        14Gi       2.0Mi        24Gi        38Gi
    >   Swap:            0B          0B          0B
    >
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.2G   15G  23% /

    >   worker [worker05]
    >   iris-gaia-blue-20220613-worker05
    >   Fri Jun 17 11:13:14 UTC 2022
    >
    >                 total        used        free      shared  buff/cache   available
    >   Mem:           42Gi       2.8Gi        12Gi       2.0Mi        26Gi        38Gi
    >   Swap:            0B          0B          0B
    >
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.2G   15G  23% /

    >   worker [worker06]
    >   iris-gaia-blue-20220613-worker06
    >   Fri Jun 17 11:13:14 UTC 2022
    >
    >                 total        used        free      shared  buff/cache   available
    >   Mem:           42Gi       5.5Gi        10Gi       2.0Mi        25Gi        36Gi
    >   Swap:            0B          0B          0B
    >
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.2G   15G  23% /


# -----------------------------------------------------
# Kill off the failed application.
#[root@ansibler]

    #
    # Kill all applications on YARN which are in RUNNING state:
    # https://stackoverflow.com/a/56035711
    #

    ssh master01

        for appid in $(
            yarn application -list -appStates RUNNING | awk 'NR > 2 { print $1 }'
            )
        do
            echo ""
            echo "App ID [${appid}]"
            yarn application -kill ${appid}
        done


    >   
    >   2022-06-17 14:54:47,149 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.0.35:8032
    >   
    >   App ID [application_1655122472463_1609]
    >   2022-06-17 14:54:48,486 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.0.35:8032
    >   Killing application application_1655122472463_1609
    >   2022-06-17 14:54:49,087 INFO impl.YarnClientImpl: Killed application application_1655122472463_1609




# -----------------------------------------------------
# Check the Zeppelin interpreter log.
#[user@zepplin]

    tail -f zeppelin-interpreter-spark-Carrovieus-Carrovieus-$(id -un)-$(hostname).log

    >    ....
    >    ....
    >    INFO [2022-06-17 07:21:13,072] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removed broadcast_10_piece0 on zeppelin:36463 in memory (size: 17.0 KiB, free: 30.5 GiB)
    >    INFO [2022-06-17 07:21:13,072] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removed broadcast_10_piece0 on worker06:37225 in memory (size: 17.0 KiB, free: 3.6 GiB)
    >    ....
    >    INFO [2022-06-17 14:54:49,658] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logInfo]:57) - Disabling executor 5.
    >    INFO [2022-06-17 14:54:49,659] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Executor lost: 5 (epoch 2)
    >    INFO [2022-06-17 14:54:49,660] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Trying to remove executor 5 from BlockManagerMaster.
    >    WARN [2022-06-17 14:54:49,660] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_137 !
    >   ERROR [2022-06-17 14:54:49,660] ({rpc-server-4-1} TransportClient.java[operationComplete]:337) - Failed to send RPC RPC 7738039617265107252 to /10.10.1.113:53550: java.nio.channels.ClosedChannelException
    >   java.nio.channels.ClosedChannelException
    >           at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >           at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >           at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >           at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >           at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >           at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)
    >           at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >           at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >           at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >           at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >           at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >           at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >           at java.lang.Thread.run(Thread.java:748)
    >    WARN [2022-06-17 14:54:49,660] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_106 !
    >    WARN [2022-06-17 14:54:49,661] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_17 !
    >    ....
    >    WARN [2022-06-17 14:54:49,662] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_149 !
    >    WARN [2022-06-17 14:54:49,662] ({dispatcher-BlockManagerMaster} Logging.scala[logWarning]:69) - No more replicas available for rdd_15_9 !
    >    INFO [2022-06-17 14:54:49,662] ({dispatcher-BlockManagerMaster} Logging.scala[logInfo]:57) - Removing block manager BlockManagerId(5, worker06, 37225, None)
    >    INFO [2022-06-17 14:54:49,662] ({dag-scheduler-event-loop} Logging.scala[logInfo]:57) - Removed 5 successfully in removeExecutor
    >    WARN [2022-06-17 14:54:49,665] ({rpc-server-4-1} Logging.scala[logWarning]:90) - Attempted to get executor loss reason for executor id 5 at RPC address 10.10.2.198:35570, but got no response. Marking as agent lost.
    >   java.io.IOException: Failed to send RPC RPC 7738039617265107252 to /10.10.1.113:53550: java.nio.channels.ClosedChannelException
    >           at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:363)
    >           at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:340)
    >           ....
    >           ....
    >   Caused by: java.nio.channels.ClosedChannelException
    >           at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >           ... 12 more
    >   ERROR [2022-06-17 14:54:49,667] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logError]:73) - Lost executor 5 on worker06: Executor Process Lost
    >    INFO [2022-06-17 14:54:49,695] ({YARN application state monitor} Logging.scala[logInfo]:57) - Deleted staging directory hdfs://master01:9000/albert/Carrovieus/.sparkStaging/application_1655122472463_1609
    >   ERROR [2022-06-17 14:54:49,696] ({YARN application state monitor} Logging.scala[logError]:73) - YARN application has exited unexpectedly with state KILLED! Check the YARN application logs for more details.
    >   ERROR [2022-06-17 14:54:49,697] ({YARN application state monitor} Logging.scala[logError]:73) - Diagnostics message: Application application_1655122472463_1609 was killed by user fedora at 10.10.0.35
    >    INFO [2022-06-17 14:54:49,705] ({YARN application state monitor} AbstractConnector.java[doStop]:381) - Stopped Spark@1abede1a{HTTP/1.1, (http/1.1)}{0.0.0.0:4051}
    >    INFO [2022-06-17 14:54:49,708] ({YARN application state monitor} Logging.scala[logInfo]:57) - Stopped Spark web UI at http://zeppelin:4051
    >   ERROR [2022-06-17 14:54:49,712] ({rpc-server-4-1} TransportClient.java[operationComplete]:337) - Failed to send RPC RPC 8965578969429977538 to /10.10.1.113:53550: java.nio.channels.ClosedChannelException
    >   java.nio.channels.ClosedChannelException
    >    ....
    >    ....
    >   ERROR [2022-06-17 14:54:49,667] ({dispatcher-CoarseGrainedScheduler} Logging.scala[logError]:73) - Lost executor 5 on worker06: Executor Process Lost
    >    INFO [2022-06-17 14:54:49,695] ({YARN application state monitor} Logging.scala[logInfo]:57) - Deleted staging directory hdfs://master01:9000/albert/Carrovieus/.sparkStaging/application_1655122472463_1609
    >   ERROR [2022-06-17 14:54:49,696] ({YARN application state monitor} Logging.scala[logError]:73) - YARN application has exited unexpectedly with state KILLED! Check the YARN application logs for more details.
    >   ERROR [2022-06-17 14:54:49,697] ({YARN application state monitor} Logging.scala[logError]:73) - Diagnostics message: Application application_1655122472463_1609 was killed by user fedora at 10.10.0.35
    >    INFO [2022-06-17 14:54:49,705] ({YARN application state monitor} AbstractConnector.java[doStop]:381) - Stopped Spark@1abede1a{HTTP/1.1, (http/1.1)}{0.0.0.0:4051}
    >    INFO [2022-06-17 14:54:49,708] ({YARN application state monitor} Logging.scala[logInfo]:57) - Stopped Spark web UI at http://zeppelin:4051
    >   ERROR [2022-06-17 14:54:49,712] ({rpc-server-4-1} TransportClient.java[operationComplete]:337) - Failed to send RPC RPC 8965578969429977538 to /10.10.1.113:53550: java.nio.channels.ClosedChannelException
    >   java.nio.channels.ClosedChannelException
    >           at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >           at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >           ....
    >           ....
    >   ERROR [2022-06-17 14:54:49,713] ({rpc-server-4-1} Logging.scala[logError]:94) - Sending RequestExecutors(Map(),Map(),Map(),Set()) to AM was unsuccessful
    >   java.io.IOException: Failed to send RPC RPC 8965578969429977538 to /10.10.1.113:53550: java.nio.channels.ClosedChannelException
    >           at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:363)
    >           at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:340)
    >           ....
    >           ....
    >           ... 12 more
    >    INFO [2022-06-17 14:54:49,721] ({dispatcher-event-loop-42} Logging.scala[logInfo]:57) - MapOutputTrackerMasterEndpoint stopped!
    >    INFO [2022-06-17 14:54:49,737] ({YARN application state monitor} Logging.scala[logInfo]:57) - MemoryStore cleared
    >    INFO [2022-06-17 14:54:49,737] ({YARN application state monitor} Logging.scala[logInfo]:57) - BlockManager stopped
    >    INFO [2022-06-17 14:54:49,739] ({YARN application state monitor} Logging.scala[logInfo]:57) - BlockManagerMaster stopped
    >    INFO [2022-06-17 14:54:49,742] ({dispatcher-event-loop-46} Logging.scala[logInfo]:57) - OutputCommitCoordinator stopped!
    >    INFO [2022-06-17 14:54:49,753] ({YARN application state monitor} Logging.scala[logInfo]:57) - Successfully stopped SparkContext


    #
    # Our stuck application has gone (killed).
    # Our AglaisBenchmarker test is still stuck.
    #


# -----------------------------------------------------
# Kill off the test with Ctrl^C.
#[root@ansibler]


    >   ....
    >   ....
    >   Process ForkPoolWorker-19:
    >   Process ForkPoolWorker-16:
    >   Process ForkPoolWorker-13:
    >   Process ForkPoolWorker-3:
    >   Process ForkPoolWorker-17:
    >   Process ForkPoolWorker-18:
    >   Process ForkPoolWorker-14:
    >   Process ForkPoolWorker-10:
    >   Process ForkPoolWorker-9:
    >   Process ForkPoolWorker-8:
    >   Traceback (most recent call last):
    >     File "/tmp/run-benchmark.py", line 46, in <module>
    >       AglaisBenchmarker(
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 194, in run
    >       results = self._run_parallel(users, delay_start, delay_notebook, delete)
    >     File "/usr/local/lib/python3.10/site-packages/aglais_benchmark/aglais_benchmark.py", line 231, in _run_parallel
    >       results = pool.starmap(self._run_single, list(zip(range(concurrent_users), [True]*concurrent_users, [delay_start]*concurrent_users, [delay_notebook]*concurrent_users, [delete]*concurrent_users)))
    >     File "/usr/lib64/python3.10/multiprocessing/pool.py", line 372, in starmap
    >       return self._map_async(func, iterable, starmapstar, chunksize).get()
    >     File "/usr/lib64/python3.10/multiprocessing/pool.py", line 765, in get
    >       self.wait(timeout)
    >     File "/usr/lib64/python3.10/multiprocessing/pool.py", line 762, in wait
    >       self._event.wait(timeout)
    >     File "/usr/lib64/python3.10/threading.py", line 600, in wait
    >       signaled = self._cond.wait(timeout)
    >     File "/usr/lib64/python3.10/threading.py", line 320, in wait
    >       waiter.acquire()
    >   KeyboardInterrupt


# -----------------------------------------------------
# Test with 19 users doing 2 loops until 5pm.
#[root@ansibler]

    # local usercount=${1:?'usercount required'}
    # local loopfinish=${2:?'loopfinish required'}
    # local looppause=${3:-10}
    # local delaystart=${4:-1}
    # local delaynotebook=${5:-1}

    long-loop 19 17 10 5 5 \
    | tee /tmp/test-loop.json


        #
        # Test started, but stalled waiting for the 19th user.
        # Applications run by the other users ran and are now FINISHED.
        # No application was created for the user that was stalled in the last run.
        #

    #
    # Looks like SparkInterpreter was created.
    #

    >    ....
    >    INFO [2022-06-16 15:03:11,991] ({qtp686466458-725679} LoginRestApi.java[postLogin]:249) - {"status":"OK","message":"","body":{"principal":"Carrovieus","ticket":"8d1973e0-2796-43f3-82cd-968a6dee765a","roles":"[\"user\"]"}}
    >    INFO [2022-06-16 15:03:12,081] ({SchedulerFactory88} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkInterpreter
    >    INFO [2022-06-16 15:03:12,082] ({qtp686466458-725598} NotebookRestApi.java[getNoteJobStatus]:783) - Get note job status.
    >    INFO [2022-06-16 15:03:12,125] ({qtp686466458-725563} NotebookRestApi.java[runParagraph]:836) - Run paragraph job asynchronously 2H6HKN3WH paragraph_1655391791337_454665494
    >    INFO [2022-06-16 15:03:12,127] ({qtp686466458-725563} NotebookService.java[runParagraph]:346) - Start to run paragraph: paragraph_1655391791337_454665494 of note: 2H6HKN3WH
    >    INFO [2022-06-16 15:03:12,127] ({qtp686466458-725563} VFSNotebookRepo.java[save]:144) - Saving note 2H6HKN3WH to tmp/ZDLBZVW2KC.json_2H6HKN3WH.zpln
    >    INFO [2022-06-16 15:03:12,131] ({qtp686466458-725563} InterpreterSetting.java[getOrCreateInterpreterGroup]:454) - Create InterpreterGroup with groupId: spark-Carrovieus for ExecutionContext{user='Carrovieus', noteId='2H6HKN3WH', interpreterGroupId='null', defaultInterpreterGroup='spark', inIsolatedMode=false, startTime=}
    >    INFO [2022-06-16 15:03:12,131] ({qtp686466458-725563} InterpreterSetting.java[createInterpreters]:823) - Interpreter org.apache.zeppelin.spark.SparkInterpreter created for user: Carrovieus, sessionId: shared_session
    >    INFO [2022-06-16 15:03:12,131] ({qtp686466458-725563} InterpreterSetting.java[createInterpreters]:823) - Interpreter org.apache.zeppelin.spark.SparkSqlInterpreter created for user: Carrovieus, sessionId: shared_session
    >    INFO [2022-06-16 15:03:12,131] ({qtp686466458-725563} InterpreterSetting.java[createInterpreters]:823) - Interpreter org.apache.zeppelin.spark.PySparkInterpreter created for user: Carrovieus, sessionId: shared_session
    >    INFO [2022-06-16 15:03:12,131] ({qtp686466458-725563} InterpreterSetting.java[createInterpreters]:823) - Interpreter org.apache.zeppelin.spark.IPySparkInterpreter created for user: Carrovieus, sessionId: shared_session
    >    INFO [2022-06-16 15:03:12,132] ({qtp686466458-725563} InterpreterSetting.java[createInterpreters]:823) - Interpreter org.apache.zeppelin.spark.SparkRInterpreter created for user: Carrovieus, sessionId: shared_session
    >    INFO [2022-06-16 15:03:12,132] ({qtp686466458-725563} InterpreterSetting.java[createInterpreters]:823) - Interpreter org.apache.zeppelin.spark.SparkIRInterpreter created for user: Carrovieus, sessionId: shared_session
    >    INFO [2022-06-16 15:03:12,132] ({qtp686466458-725563} InterpreterSetting.java[createInterpreters]:823) - Interpreter org.apache.zeppelin.spark.SparkShinyInterpreter created for user: Carrovieus, sessionId: shared_session
    >    INFO [2022-06-16 15:03:12,132] ({qtp686466458-725563} InterpreterSetting.java[createInterpreters]:823) - Interpreter org.apache.zeppelin.spark.KotlinSparkInterpreter created for user: Carrovieus, sessionId: shared_session
    >    INFO [2022-06-16 15:03:12,132] ({qtp686466458-725563} ManagedInterpreterGroup.java[getOrCreateSession]:180) - Create Session: shared_session in InterpreterGroup: spark-Carrovieus for user: Carrovieus
    >    INFO [2022-06-16 15:03:12,133] ({SchedulerFactory93} AbstractScheduler.java[runJob]:127) - Job paragraph_1655391791337_454665494 started by scheduler RemoteInterpreter-spark-Carrovieus-shared_session
    >    INFO [2022-06-16 15:03:12,135] ({SchedulerFactory93} Paragraph.java[jobRun]:416) - Run paragraph [paragraph_id: paragraph_1655391791337_454665494, interpreter: org.apache.zeppelin.spark.PySparkInterpreter, note_id: 2H6HKN3WH, user: Carrovieus]
    >    INFO [2022-06-16 15:03:12,135] ({SchedulerFactory93} ManagedInterpreterGroup.java[getOrCreateInterpreterProcess]:65) - Create InterpreterProcess for InterpreterGroup: spark-Carrovieus
    >    INFO [2022-06-16 15:03:12,137] ({SchedulerFactory93} PluginManager.java[loadInterpreterLauncher]:154) - Loading Interpreter Launcher Plugin: org.apache.zeppelin.interpreter.launcher.SparkInterpreterLauncher
    >    INFO [2022-06-16 15:03:12,137] ({SchedulerFactory93} StandardInterpreterLauncher.java[launchDirectly]:50) - Launching new interpreter process of spark
    >    INFO [2022-06-16 15:03:12,137] ({SchedulerFactory93} SparkInterpreterLauncher.java[buildEnvFromProperties]:213) - Run Spark under non-secure mode as no keytab and principal is specified
    >    INFO [2022-06-16 15:03:12,138] ({SchedulerFactory93} SparkInterpreterLauncher.java[buildEnvFromProperties]:245) - buildEnvFromProperties: {PATH=/home/fedora/.local/bin:/home/fedora/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/aglais/bin:/opt/hadoop/bin:/opt/hadoop/sbin:/opt/spark/python:/opt/spark/bin:/home/fedora/zeppelin/bin, HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop, ZEPPELIN_LOG_DIR=/home/fedora/zeppelin/logs, ZEPPELIN_WAR=/home/fedora/zeppelin/zeppelin-web-0.10.0.war, ZEPPELIN_ENCODING=UTF-8, ZEPPELIN_SPARK_CONF=--proxy-user|Carrovieus|--conf|spark.yarn.dist.archives=/opt/spark/R/lib/sparkr.zip#sparkr|--conf|spark.submit.deployMode=client|--conf|spark.webui.yarn.useProxy=false|--conf|spark.yarn.isPython=true|--conf|spark.app.name=spark-Carrovieus|--conf|spark.master=yarn, ZEPPELIN_NICENESS=0, HADOOP_DATA=/var/hadoop/data, JAVA_OPTS=  -Dfile.encoding=UTF-8 -Xmx1024m -Dlog4j.configuration=file:///home/fedora/zeppelin/conf/log4j.properties -Dzeppelin.log.file=/home/fedora/zeppelin/logs/zeppelin-fedora-iris-gaia-blue-20220613-zeppelin.log, DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus, JAVA_INTP_OPTS= -Dfile.encoding=UTF-8 -Dlog4j.configuration=file:///home/fedora/zeppelin/conf/log4j.properties -Dlog4j.configurationFile=file:///home/fedora/zeppelin/conf/log4j2.properties, ZEPPELIN_CONF_DIR=/home/fedora/zeppelin/conf, LOGNAME=fedora, PWD=/home/fedora, PYTHONPATH=:/opt/spark/python:/opt/spark/python/lib/py4j-0.10.4-src.zip, LESSOPEN=||/usr/bin/lesspipe.sh %s, SHELL=/bin/bash, ZEPPELIN_INTP_MEM=-Xmx1024m, SELINUX_USE_CURRENT_RANGE=, PYSPARK_DRIVER_PYTHON=python, ZEPPELIN_ANGULAR_WAR=/home/fedora/zeppelin/zeppelin-web-angular-0.10.0.war, HADOOP_HOME=/opt/hadoop, SHLVL=1, MASTER=yarn-client, JAVA_HOME=/etc/alternatives/jre, INTERPRETER_GROUP_ID=spark-Carrovieus, LANG=en_US.UTF-8, XDG_SESSION_ID=3, XDG_SESSION_TYPE=tty, SELINUX_LEVEL_REQUESTED=, PYSPARK_PYTHON=python, SELINUX_ROLE_REQUESTED=, SPARK_HOME=/opt/spark, ZEPPELIN_RUNNER=/etc/alternatives/jre/bin/java, _=/usr/bin/nohup, XDG_SESSION_CLASS=user, ZEPPELIN_HOME=/home/fedora/zeppelin, SSH_CLIENT=90.155.51.57 34062 22, USER=fedora, ZEPPELIN_PID_DIR=/home/fedora/zeppelin/run, ZEPPELIN_MEM=-Xmx1024m, SSH_AUTH_SOCK=/tmp/ssh-sdEjU07vpR/agent.1623, SSH_CONNECTION=90.155.51.57 34062 10.10.2.210 22, ZEPPELIN_IDENT_STRING=fedora, PYSPARK_PIN_THREAD=true, HADOOP_LOG_DIR=/var/hadoop/logs, ZEPPELIN_INTERPRETER_REMOTE_RUNNER=bin/interpreter.sh, XDG_RUNTIME_DIR=/run/user/1000, HOME=/home/fedora}
    >    INFO [2022-06-16 15:03:12,160] ({SchedulerFactory88} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkSqlInterpreter
    >    INFO [2022-06-16 15:03:12,162] ({SchedulerFactory88} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
    >    INFO [2022-06-16 15:03:12,166] ({qtp686466458-725682} LoginRestApi.java[postLogin]:249) - {"status":"OK","message":"","body":{"principal":"Pierione","ticket":"a04d17a2-7dcd-4585-9271-96a1cefdfed9","roles":"[\"user\"]"}}
    >    INFO [2022-06-16 15:03:12,167] ({SchedulerFactory88} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.IPySparkInterpreter
    >    INFO [2022-06-16 15:03:12,171] ({SchedulerFactory88} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkRInterpreter
    >    INFO [2022-06-16 15:03:12,196] ({SchedulerFactory88} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkIRInterpreter
    >    INFO [2022-06-16 15:03:12,198] ({SchedulerFactory88} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkShinyInterpreter
    >    INFO [2022-06-16 15:03:12,199] ({SchedulerFactory93} ProcessLauncher.java[transition]:109) - Process state is transitioned to LAUNCHED
    >    INFO [2022-06-16 15:03:12,199] ({SchedulerFactory93} ProcessLauncher.java[launch]:96) - Process is launched: [/home/fedora/zeppelin/bin/interpreter.sh, -d, /home/fedora/zeppelin/interpreter/spark, -c, 10.10.2.210, -p, 35643, -r, :, -i, spark-Carrovieus, -u, Carrovieus, -l, /home/fedora/zeppelin/local-repo/spark, -g, spark]
    >    ....

    #
    # No idea ....
    #


# -----------------------------------------------------
# Restart Zeppelin ...
#[root@ansibler]

    /deployments/hadoop-yarn/bin/restart-zeppelin.sh

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]

    #
    # Test continues as soon as Zeppelin is restarted.
    #


 INFO [2022-06-17 16:29:20,858] ({main} ZeppelinServer.java[main]:265) - Done, zeppelin server started
 INFO [2022-06-17 16:29:21,337] ({Exec Stream Pumper} ProcessLauncher.java[processLine]:189) - [INFO] Interpreter launch command: ssh Carrovieus@localhost source /home/fedora/zeppelin/conf/zeppelin-env.sh; /etc/alternatives/jre/bin/java -Dfile.encoding=UTF-8 -Dlog4j.configuration=file:///home/fedora/zeppelin/conf/log4j.properties -Dlog4j.configurationFile=file:///home/fedora/zeppelin/conf/log4j2.properties -Dzeppelin.log.file=/home/fedora/zeppelin/logs/zeppelin-interpreter-md-Carrovieus-Carrovieus-fedora-iris-gaia-blue-20220613-zeppelin.log -Xmx1024m -cp :/home/fedora/zeppelin/local-repo/md/*:/home/fedora/zeppelin/interpreter/md/*:::/home/fedora/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer 10.10.2.210 39349 md-Carrovieus :
 INFO [2022-06-17 16:29:23,096] ({pool-7-thread-2} RemoteInterpreterEventServer.java[registerInterpreterProcess]:183) - Register interpreter process: 10.10.2.210:43937, interpreterGroup: md-Carrovieus
 INFO [2022-06-17 16:29:23,096] ({pool-7-thread-2} ProcessLauncher.java[transition]:109) - Process state is transitioned to RUNNING
 INFO [2022-06-17 16:29:23,259] ({SchedulerFactory2} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.markdown.Markdown
 INFO [2022-06-17 16:29:23,343] ({SchedulerFactory2} RemoteInterpreter.java[lambda$open$0]:134) - Open RemoteInterpreter org.apache.zeppelin.markdown.Markdown
 INFO [2022-06-17 16:29:23,343] ({SchedulerFactory2} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:393) - Push local angular object registry from ZeppelinServer to remote interpreter group md-Carrovieus
 INFO [2022-06-17 16:29:23,508] ({JobStatusPoller-paragraph_1655483358576_1138661423} NotebookServer.java[onStatusChange]:1989) - Job paragraph_1655483358576_1138661423 starts to RUNNING
 INFO [2022-06-17 16:29:23,509] ({JobStatusPoller-paragraph_1655483358576_1138661423} VFSNotebookRepo.java[save]:144) - Saving note 2H5C33YE1 to tmp/TNHOY4K62F.json_2H5C33YE1.zpln
 INFO [2022-06-17 16:29:23,649] ({SchedulerFactory2} NotebookServer.java[onStatusChange]:1984) - Job paragraph_1655483358576_1138661423 is finished successfully, status: FINISHED
 INFO [2022-06-17 16:29:23,650] ({SchedulerFactory2} VFSNotebookRepo.java[save]:144) - Saving note 2H5C33YE1 to tmp/TNHOY4K62F.json_2H5C33YE1.zpln
 INFO [2022-06-17 16:29:23,653] ({SchedulerFactory2} AbstractScheduler.java[runJob]:154) - Job paragraph_1655483358576_1138661423 finished by scheduler RemoteInterpreter-md-Carrovieus-shared_session with status FINISHED
 INFO [2022-06-17 16:29:24,038] ({qtp2128029086-40} NotebookRestApi.java[runParagraph]:836) - Run paragraph job asynchronously 2H5C33YE1 paragraph_1655483358578_1063825342
 INFO [2022-06-17 16:29:24,040] ({qtp2128029086-40} NotebookService.java[runParagraph]:346) - Start to run paragraph: paragraph_1655483358578_1063825342 of note: 2H5C33YE1
 INFO [2022-06-17 16:29:24,040] ({qtp2128029086-40} VFSNotebookRepo.java[save]:144) - Saving note 2H5C33YE1 to tmp/TNHOY4K62F.json_2H5C33YE1.zpln
 INFO [2022-06-17 16:29:24,044] ({SchedulerFactory3} AbstractScheduler.java[runJob]:127) - Job paragraph_1655483358578_1063825342 started by scheduler RemoteInterpreter-spark-Carrovieus-shared_session
 INFO [2022-06-17 16:29:24,044] ({SchedulerFactory3} Paragraph.java[jobRun]:416) - Run paragraph [paragraph_id: paragraph_1655483358578_1063825342, interpreter: org.apache.zeppelin.spark.PySparkInterpreter, note_id: 2H5C33YE1, user: Carrovieus]

 INFO [2022-06-17 16:29:24,049] ({SchedulerFactory3} ProcessLauncher.java[transition]:109) - Process state is transitioned to LAUNCHED
 INFO [2022-06-17 16:29:24,049] ({SchedulerFactory3} ProcessLauncher.java[launch]:96) - Process is launched: [/home/fedora/zeppelin/bin/interpreter.sh, -d, /home/fedora/zeppelin/interpreter/spark, -c, 10.10.2.210, -p, 39349, -r, :, -i, spark-Carrovieus, -u, Carrovieus, -l, /home/fedora/zeppelin/local-repo/spark, -g, spark]
 INFO [2022-06-17 16:29:24,519] ({Exec Stream Pumper} ProcessLauncher.java[processLine]:189) - [INFO] Interpreter launch command: /opt/spark/bin/spark-submit --class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer --driver-class-path :/home/fedora/zeppelin/local-repo/spark/*:/home/fedora/zeppelin/interpreter/spark/*:::/home/fedora/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar:/home/fedora/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar:/opt/hadoop/etc/hadoop --driver-java-options  -Dfile.encoding=UTF-8 -Dlog4j.configuration=file:///home/fedora/zeppelin/conf/log4j.properties -Dlog4j.configurationFile=file:///home/fedora/zeppelin/conf/log4j2.properties -Dzeppelin.log.file=/home/fedora/zeppelin/logs/zeppelin-interpreter-spark-Carrovieus-Carrovieus-fedora-iris-gaia-blue-20220613-zeppelin.log --proxy-user Carrovieus --conf spark.yarn.dist.archives=/opt/spark/R/lib/sparkr.zip#sparkr --conf spark.submit.deployMode=client --conf spark.webui.yarn.useProxy=false --conf spark.yarn.isPython=true --conf spark.app.name=spark-Carrovieus --conf spark.master=yarn /home/fedora/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar 10.10.2.210 39349 spark-Carrovieus :
 INFO [2022-06-17 16:29:27,413] ({pool-7-thread-5} RemoteInterpreterEventServer.java[registerInterpreterProcess]:183) - Register interpreter process: 10.10.2.210:37509, interpreterGroup: spark-Carrovieus
 INFO [2022-06-17 16:29:27,413] ({pool-7-thread-5} ProcessLauncher.java[transition]:109) - Process state is transitioned to RUNNING
 INFO [2022-06-17 16:29:27,538] ({qtp2128029086-125} NotebookServer.java[onOpen]:246) - New connection from 10.10.2.210:36970
 INFO [2022-06-17 16:29:27,548] ({SchedulerFactory3} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2022-06-17 16:29:27,619] ({SchedulerFactory3} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2022-06-17 16:29:27,621] ({SchedulerFactory3} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2022-06-17 16:29:27,624] ({SchedulerFactory3} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.IPySparkInterpreter
 INFO [2022-06-17 16:29:27,627] ({SchedulerFactory3} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2022-06-17 16:29:27,629] ({SchedulerFactory3} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkIRInterpreter
 INFO [2022-06-17 16:29:27,631] ({SchedulerFactory3} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.SparkShinyInterpreter
 INFO [2022-06-17 16:29:27,632] ({SchedulerFactory3} RemoteInterpreter.java[lambda$internal_create$1]:160) - Create RemoteInterpreter org.apache.zeppelin.spark.KotlinSparkInterpreter

    #
    # Test fails because "UnresolvedRelation [gaia_source]"
    #

 INFO [2022-06-17 16:29:27,662] ({SchedulerFactory3} RemoteInterpreter.java[lambda$open$0]:134) - Open RemoteInterpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2022-06-17 16:29:27,663] ({SchedulerFactory3} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:393) - Push local angular object registry from ZeppelinServer to remote interpreter group spark-Carrovieus
 INFO [2022-06-17 16:29:51,384] ({JobStatusPoller-paragraph_1655483358578_1063825342} NotebookServer.java[onStatusChange]:1989) - Job paragraph_1655483358578_1063825342 starts to RUNNING
 INFO [2022-06-17 16:29:51,385] ({JobStatusPoller-paragraph_1655483358578_1063825342} VFSNotebookRepo.java[save]:144) - Saving note 2H5C33YE1 to tmp/TNHOY4K62F.json_2H5C33YE1.zpln
 WARN [2022-06-17 16:29:53,398] ({SchedulerFactory3} NotebookServer.java[onStatusChange]:1986) - Job paragraph_1655483358578_1063825342 is finished, status: ERROR, exception: null, result: %text Fail to execute line 21: df = spark.sql("SELECT FLOOR(source_id / %d"%(divisor) + ") AS hpx_id, COUNT(*) AS n FROM gaia_source GROUP BY hpx_id")
Traceback (most recent call last):
  File "/tmp/1655483391411-0/zeppelin_python.py", line 158, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 21, in <module>
  File "/opt/spark/python/pyspark/sql/session.py", line 723, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: Table or view not found: gaia_source; line 1 pos 72;
'Aggregate ['hpx_id], ['FLOOR(('source_id / 140737488355328)) AS hpx_id#0, count(1) AS n#1L]
+- 'UnresolvedRelation [gaia_source], [], false

    #
    # Test moves on to next iteration.
    # It recovers, but the test is unreliable.
    #



