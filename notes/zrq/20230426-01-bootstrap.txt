#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Try using the StackHPC helm charts.

    Result:

        Work in progress ...


# -----------------------------------------------------
# Check which platform is live.
#[user@desktop]

    ssh fedora@live.gaia-dmp.uk \
        '
        date
        hostname
        '

    >   Wed 26 Apr 10:54:37 UTC 2023
    >   iris-gaia-green-20230308-zeppelin


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    #
    # Live is green, selecting red for the deployment.
    # Using the 'admin' credentials to allow access to loadbalancers etc.
    #

    source "${HOME:?}/aglais.env"

    agcolour=red

    clientname=ansibler-${agcolour}
    cloudname=iris-gaia-${agcolour}-admin

    podman run \
        --rm \
        --tty \
        --interactive \
        --name     "${clientname:?}" \
        --hostname "${clientname:?}" \
        --env "cloudname=${cloudname:?}" \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK:?}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        ghcr.io/wfau/atolmis/ansible-client:2022.07.25 \
        bash

    >   ....
    >   ....


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

    >   real    2m3.560s
    >   user    0m52.685s
    >   sys     0m5.497s


# -----------------------------------------------------
# Add YAML editor role to our client container.
# TODO Add this to the Ansible client.
# https://github.com/wfau/atolmis/issues/30
#[root@ansibler]

    ansible-galaxy install kwoodson.yedit

    >   ....
    >   ....


# -----------------------------------------------------
# Create our deployment settings.
#[root@ansibler]

    deployname=${cloudname:?}-$(date '+%Y%m%d')
    deploydate=$(date '+%Y%m%dT%H%M%S')

    statusyml='/opt/aglais/aglais-status.yml'
    if [ ! -e "$(dirname ${statusyml})" ]
    then
        mkdir "$(dirname ${statusyml})"
    fi
    rm -f "${statusyml}"
    touch "${statusyml}"

    yq eval \
        --inplace \
        "
        .aglais.deployment.type = \"cluster-api\"   |
        .aglais.deployment.name = \"${deployname}\" |
        .aglais.deployment.date = \"${deploydate}\" |
        .aglais.openstack.cloud.name = \"${cloudname}\"
        " "${statusyml}"

    cat /opt/aglais/aglais-status.yml

    >   aglais:
    >     deployment:
    >       type: cluster-api
    >       name: iris-gaia-red-admin-20230426
    >       date: 20230426T121134
    >     openstack:
    >       cloud:
    >         name: iris-gaia-red-admin


# -----------------------------------------------------
# Create our bootstrap components.
#[root@ansibler]

    inventory=/deployments/cluster-api/bootstrap/ansible/config/inventory.yml

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/01-create-keypair.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/02-create-network.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/03-create-bootstrap.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/04-config-ansible.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/05-install-aglais.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/06-install-docker.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/07-install-kubectl.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/08-install-kind.yml'

#   ansible-playbook \
#       --inventory "${inventory:?}" \
#       '/deployments/cluster-api/bootstrap/ansible/09-install-helm.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/10-install-clusterctl.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/11-install-yq.yml'

    cat /opt/aglais/aglais-status.yml

    >   aglais:
    >     deployment:
    >       date: 20230426T121134
    >       name: iris-gaia-red-admin-20230426
    >       type: cluster-api
    >     openstack:
    >       cloud:
    >         name: iris-gaia-red-admin
    >       keypairs:
    >         team:
    >           fingerprint: 2e:84:98:98:df:70:06:0e:4c:ed:bd:d4:d6:6b:eb:16
    >           id: iris-gaia-red-admin-20230426-keypair
    >           name: iris-gaia-red-admin-20230426-keypair
    >       networks:
    >         external:
    >           network:
    >             id: 57add367-d205-4030-a929-d75617a7c63e
    >             name: CUDN-Internet
    >         internal:
    >           network:
    >             id: 2755c2dc-6d1b-4bc5-a345-7529f90a6a60
    >             name: iris-gaia-red-admin-20230426-internal-network
    >           router:
    >             id: 32d33c31-0f16-41f5-8c25-99f176ca2d60
    >             name: iris-gaia-red-admin-20230426-internal-router
    >           subnet:
    >             cidr: 10.10.0.0/16
    >             id: 69df76ec-716c-4ed5-be53-e307ab782221
    >             name: iris-gaia-red-admin-20230426-internal-subnet
    >       servers:
    >         bootstrap:
    >           float:
    >             external: 128.232.227.65
    >             id: 02f293d2-ea04-47e0-8bb7-44e4dc79b8c0
    >             internal: 10.10.0.221
    >           server:
    >             address:
    >               ipv4: 10.10.0.221
    >             flavor:
    >               name: gaia.vm.cclake.2vcpu
    >             hostname: bootstrap
    >             id: c62b274d-ac19-4049-8ad8-3fbcdcd9933c
    >             image:
    >               id: e5c23082-cc34-4213-ad31-ff4684657691
    >               name: Fedora-34.1.2
    >             name: iris-gaia-red-admin-20230426-bootstrap


# -----------------------------------------------------
# -----------------------------------------------------
# Login to the bootstrap node as root.
#[root@ansibler]

    podman exec \
        -it \
        ansibler-red \
            bash

        ssh bootstrap

            sudo su -

    #
    # We could prefix everything with sudo, but it gets very boring.
    #


# -----------------------------------------------------
# Install Helm on the bootstrap node.
# https://helm.sh/docs/intro/install/
# https://github.com/helm/helm/releases
#[root@bootstrap]

    #
    # We still need to do this because our incude task doesn't handle tar files yet.
    #

    helmarch=linux-amd64
    helmversion=3.11.2
    helmtarfile=helm-v${helmversion}-${helmarch}.tar.gz
    helmtmpfile=/tmp/${helmtarfile:?}
    helmbinary=helm-${helmversion:?}

    curl \
        --location \
        --no-progress-meter \
        --output "${helmtmpfile:?}" \
        "https://get.helm.sh/${helmtarfile:?}"

    tar \
        --gzip \
        --extract \
        --directory /tmp \
        --file "${helmtmpfile:?}"

    pushd /usr/local/bin
        mv "/tmp/${helmarch:?}/helm" "${helmbinary:?}"
        chown 'root:root' "${helmbinary:?}"
        chmod 'u=rwx,g=rx,o=rx' "${helmbinary:?}"
        ln -s "${helmbinary:?}" 'helm'
    popd


# -----------------------------------------------------
# Create our initial Kind cluster.
# https://github.com/kubernetes-sigs/kind/pull/2478#issuecomment-1214656908
#[root@bootstrap]

    kind create cluster --retain

    >   Creating cluster "kind" ...
    >    ✓ Ensuring node image (kindest/node:v1.25.3) 🖼
    >    ✓ Preparing nodes 📦
    >    ✓ Writing configuration 📜
    >    ✓ Starting control-plane 🕹️
    >    ✓ Installing CNI 🔌
    >    ✓ Installing StorageClass 💾
    >   ....
    >   ....


# -----------------------------------------------------
# Check the installed pods.
#[root@bootstrap]

    kubectl get pods --all-namespaces

    >   NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE
    >   kube-system          coredns-565d847f94-mpnp6                     1/1     Running   0          23s
    >   kube-system          coredns-565d847f94-z5qk4                     1/1     Running   0          23s
    >   kube-system          etcd-kind-control-plane                      1/1     Running   0          37s
    >   kube-system          kindnet-jzwvc                                1/1     Running   0          23s
    >   kube-system          kube-apiserver-kind-control-plane            1/1     Running   0          37s
    >   kube-system          kube-controller-manager-kind-control-plane   1/1     Running   0          37s
    >   kube-system          kube-proxy-qm2gw                             1/1     Running   0          23s
    >   kube-system          kube-scheduler-kind-control-plane            1/1     Running   0          37s
    >   local-path-storage   local-path-provisioner-684f458cdd-qh8c4      1/1     Running   0          23s


# -----------------------------------------------------
# Create our Openstack management cluster
# https://cluster-api-openstack.sigs.k8s.io/getting-started.html#initialization-for-common-providers
# https://github.com/stackhpc/capi-helm-charts/tree/main/charts/openstack-cluster#prerequisites
#[root@bootstrap]

    clusterctl init --infrastructure openstack

    >   Fetching providers
    >   Installing cert-manager Version="v1.11.0"
    >   Waiting for cert-manager to be available...
    >   Installing Provider="cluster-api" Version="v1.4.1" TargetNamespace="capi-system"
    >   Installing Provider="bootstrap-kubeadm" Version="v1.4.1" TargetNamespace="capi-kubeadm-bootstrap-system"
    >   Installing Provider="control-plane-kubeadm" Version="v1.4.1" TargetNamespace="capi-kubeadm-control-plane-system"
    >   Installing Provider="infrastructure-openstack" Version="v0.7.1" TargetNamespace="capo-system"
    >   ....
    >   ....


# -----------------------------------------------------
# Check the installed pods.
#[root@bootstrap]

    kubectl get pods --all-namespaces

    >   NAMESPACE                           NAME                                                             READY   STATUS    RESTARTS   AGE
    >   capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-8654485994-r4qf9       1/1     Running   0          86s
    >   capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-5d9d9494d5-pkmqh   1/1     Running   0          84s
    >   capi-system                         capi-controller-manager-746b4f5db4-mk9gq                         1/1     Running   0          87s
    >   capo-system                         capo-controller-manager-775d744795-lqsn4                         1/1     Running   0          80s
    >   cert-manager                        cert-manager-99bb69456-d8zvg                                     1/1     Running   0          105s
    >   cert-manager                        cert-manager-cainjector-ffb4747bb-rfd9s                          1/1     Running   0          105s
    >   cert-manager                        cert-manager-webhook-545bd5d7d8-ktk2d                            1/1     Running   0          105s
    >   kube-system                         coredns-565d847f94-mpnp6                                         1/1     Running   0          10m
    >   kube-system                         coredns-565d847f94-z5qk4                                         1/1     Running   0          10m
    >   kube-system                         etcd-kind-control-plane                                          1/1     Running   0          11m
    >   kube-system                         kindnet-jzwvc                                                    1/1     Running   0          10m
    >   kube-system                         kube-apiserver-kind-control-plane                                1/1     Running   0          11m
    >   kube-system                         kube-controller-manager-kind-control-plane                       1/1     Running   0          11m
    >   kube-system                         kube-proxy-qm2gw                                                 1/1     Running   0          10m
    >   kube-system                         kube-scheduler-kind-control-plane                                1/1     Running   0          11m
    >   local-path-storage                  local-path-provisioner-684f458cdd-qh8c4                          1/1     Running   0          10m


# -----------------------------------------------------
# -----------------------------------------------------
# Extract the settings we need.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        token issue \
            --format json \
    | tee /tmp/ostoken.json   \
    | jq '.'

    >   {
    >     "expires": "2023-04-26T13:17:12+0000",
    >     "id": "gAAAAABk....E2TQzaDh",
    >     "project_id": "0dd8cc5ee5a7455c8748cc06d04c93c3",
    >     "user_id": "5fa0c97a6dd14e01a3c7d91dad5c6b17"
    >   }

    osuserident=$(
        jq -r '.user_id' '/tmp/ostoken.json'
        )

    osprojectid=$(
        jq -r '.project_id' '/tmp/ostoken.json'
        )

    ctrlnodeflavor=gaia.vm.cclake.4vcpu
    nodenodeflavor=gaia.vm.cclake.4vcpu

    keypair=$(
        yq '.aglais.openstack.keypairs.team.name' /opt/aglais/aglais-status.yml
        )

    externalnet=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            network list \
                --external \
                --format json \
        | jq -r ".[] | select(.Name == \"CUDN-Internet\") | .ID"
        )

    cat > /tmp/openstack-settings.env << EOF
export OPENSTACK_CLOUD=${cloudname:?}

export OPENSTACK_USER_ID=${osuserident:?}
export OPENSTACK_PROJECT_ID=${osprojectid:?}

export OPENSTACK_SSH_KEY_NAME=${keypair:?}
export OPENSTACK_EXTERNAL_NETWORK_ID=${externalnet:?}

export OPENSTACK_NODE_MACHINE_FLAVOR=${nodenodeflavor}
export OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR=${ctrlnodeflavor}

export KUBERNETES_VERSION=1.25.4
export OPENSTACK_IMAGE_NAME=gaia-dmp-ubuntu-2004-kube-v1.25.4

export OPENSTACK_FAILURE_DOMAIN=nova

# Use the Cambridge DNS servers.
# https://www.dns.cam.ac.uk/servers/rec.html
export OPENSTACK_DNS_NAMESERVERS=131.111.8.42

EOF


# -----------------------------------------------------
# Transfer the Openstack settings to our bootstrap node.
#[root@ansibler]

    scp \
        /tmp/openstack-settings.env \
        bootstrap:/tmp/openstack-settings.env

    ssh bootstrap \
        '
        sudo mkdir -p \
            /etc/aglais
        sudo install \
            /tmp/openstack-settings.env \
            /etc/aglais/openstack-settings.env
        '


# -----------------------------------------------------
# Transfer a copy of our clouds.yaml file.
#[root@ansibler]

    scp \
        /etc/openstack/clouds.yaml \
        bootstrap:/tmp/openstack-clouds.yaml

    ssh bootstrap \
        '
        sudo mkdir -p \
            /etc/aglais
        sudo install \
            /tmp/openstack-clouds.yaml \
            /etc/aglais/openstack-clouds.yaml
        '


# -----------------------------------------------------
# -----------------------------------------------------
# Load the Openstack settings from our client.
#[root@bootstrap]

    source /etc/aglais/openstack-settings.env

cat << EOF
OPENSTACK_CLOUD [${OPENSTACK_CLOUD}]
OPENSTACK_USER_ID [${OPENSTACK_USER_ID}]
OPENSTACK_PROJECT_ID [${OPENSTACK_PROJECT_ID}]
OPENSTACK_IMAGE_NAME [${OPENSTACK_IMAGE_NAME}]
EOF

    >   OPENSTACK_CLOUD [iris-gaia-red-admin]
    >   OPENSTACK_USER_ID [5fa0c97a6dd14e01a3c7d91dad5c6b17]
    >   OPENSTACK_PROJECT_ID [0dd8cc5ee5a7455c8748cc06d04c93c3]
    >   OPENSTACK_IMAGE_NAME [gaia-dmp-ubuntu-2004-kube-v1.25.4]

# -----------------------------------------------------
# TODO extract single cloud from clouds.yaml so we can edit it.
#

# -----------------------------------------------------
# Edit our clouds.yaml file to disable TLS certificate checks.
# https://docs.openstack.org/os-client-config/latest/user/configuration.html#ssl-settings
#[root@bootstrap]

    yq eval \
        --inplace \
        "
        .clouds.${OPENSTACK_CLOUD}.verify = \"false\"
        " '/etc/aglais/openstack-clouds.yaml'


# -----------------------------------------------------
# Edit our clouds.yaml file to add our project ID.
# https://github.com/stackhpc/capi-helm-charts/tree/main/charts/openstack-cluster#openstack-credentials
#[root@bootstrap]

    yq eval \
        --inplace \
        "
        .clouds.${OPENSTACK_CLOUD}.auth.project_id = \"${OPENSTACK_PROJECT_ID:?}\"
        " '/etc/aglais/openstack-clouds.yaml'


# -----------------------------------------------------
# Check our clouds.yaml file.
#[root@bootstrap]

    yq ".clouds.${OPENSTACK_CLOUD}" '/etc/aglais/openstack-clouds.yaml'

    >   auth:
    >     auth_url: https://arcus.openstack.hpc.cam.ac.uk:5000
    >     application_credential_id: "...."
    >     application_credential_secret: "...."
    >     project_id: 0dd8cc5ee5a7455c8748cc06d04c93c3
    >   region_name: "RegionOne"
    >   interface: "public"
    >   identity_api_version: 3
    >   auth_type: "v3applicationcredential"
    >   verify: "false"


# -----------------------------------------------------
# Create our cluster config.
# https://github.com/stackhpc/capi-helm-charts/tree/main/charts/openstack-cluster#managing-a-workload-cluster
#[root@bootstrap]

    touch '/etc/aglais/workload-cluster.yaml'

    yq eval \
        --inplace \
        "
        .cloudName = \"${OPENSTACK_CLOUD:?}\" |
        .kubernetesVersion = \"${KUBERNETES_VERSION:?}\" |
        .machineImage = \"${OPENSTACK_IMAGE_NAME:?}\" |
        .machineSSHKeyName = \"${OPENSTACK_SSH_KEY_NAME:?}\" |
        .controlPlane.machineFlavor = \"${OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR:?}\" |
        .nodeGroups.[0].name = \"md-0\" |
        .nodeGroups.[0].machineFlavor = \"${OPENSTACK_NODE_MACHINE_FLAVOR:?}\" |
        .nodeGroups.[0].machineCount = 4
        " '/etc/aglais/workload-cluster.yaml'

    yq '/etc/aglais/workload-cluster.yaml'

    >   cloudName: iris-gaia-red-admin
    >   kubernetesVersion: 1.25.4
    >   machineImage: gaia-dmp-ubuntu-2004-kube-v1.25.4
    >   machineSSHKeyName: iris-gaia-red-admin-20230426-keypair
    >   controlPlane:
    >     machineFlavor: gaia.vm.cclake.4vcpu
    >   nodeGroups:
    >     - name: md-0
    >       machineFlavor: gaia.vm.cclake.4vcpu
    >       machineCount: 4


# -----------------------------------------------------
# Add the StackHPC Helm repos.
#[root@bootstrap]

    helm repo add \
        capi \
        https://stackhpc.github.io/capi-helm-charts

    >   "capi" has been added to your repositories


    helm repo add \
        capi-addons \
        https://stackhpc.github.io/cluster-api-addon-provider

    >   "capi-addons" has been added to your repositories


    helm install \
        cluster-api-addon-provider \
        capi-addons/cluster-api-addon-provider

    >   NAME: cluster-api-addon-provider
    >   LAST DEPLOYED: Wed Apr 26 12:31:45 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None

# -----------------------------------------------------
# Initialise our cluster ...
#[root@bootstrap]

    helm install \
        my-cluster \
        capi/openstack-cluster \
            --values '/etc/aglais/workload-cluster.yaml' \
            --values '/etc/aglais/openstack-clouds.yaml'

    >   NAME: my-cluster
    >   LAST DEPLOYED: Wed Apr 26 12:34:57 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None

# -----------------------------------------------------
# Check the installed pods.
#[root@bootstrap]

    kubectl get pods --all-namespaces

    >   NAMESPACE                           NAME                                                             READY   STATUS              RESTARTS   AGE
    >   capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-8654485994-r4qf9       1/1     Running             0          9m36s
    >   capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-5d9d9494d5-pkmqh   1/1     Running             0          9m34s
    >   capi-system                         capi-controller-manager-746b4f5db4-mk9gq                         1/1     Running             0          9m37s
    >   capo-system                         capo-controller-manager-775d744795-lqsn4                         1/1     Running             0          9m30s
    >   cert-manager                        cert-manager-99bb69456-d8zvg                                     1/1     Running             0          9m55s
    >   cert-manager                        cert-manager-cainjector-ffb4747bb-rfd9s                          1/1     Running             0          9m55s
    >   cert-manager                        cert-manager-webhook-545bd5d7d8-ktk2d                            1/1     Running             0          9m55s
    >   default                             cluster-api-addon-provider-5cb78d8945-48jtf                      1/1     Running             0          3m35s
    >   default                             my-cluster-autoscaler-86c49ddb4b-24pct                           0/1     ContainerCreating   0          21s
    >   kube-system                         coredns-565d847f94-mpnp6                                         1/1     Running             0          18m
    >   kube-system                         coredns-565d847f94-z5qk4                                         1/1     Running             0          18m
    >   kube-system                         etcd-kind-control-plane                                          1/1     Running             0          19m
    >   kube-system                         kindnet-jzwvc                                                    1/1     Running             0          18m
    >   kube-system                         kube-apiserver-kind-control-plane                                1/1     Running             0          19m
    >   kube-system                         kube-controller-manager-kind-control-plane                       1/1     Running             0          19m
    >   kube-system                         kube-proxy-qm2gw                                                 1/1     Running             0          18m
    >   kube-system                         kube-scheduler-kind-control-plane                                1/1     Running             0          19m
    >   local-path-storage                  local-path-provisioner-684f458cdd-qh8c4                          1/1     Running             0          18m
    >   [root@iris-gaia-red-admin-20230426-bootstrap ~]#


    kubectl get cluster

    >   NAME         PHASE          AGE    VERSION
    >   my-cluster   Provisioning   104s


    clusterctl describe cluster 'my-cluster'

    >   NAME                                                           READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/my-cluster                                             False  Warning   ScalingUp                    2m30s  Scaling up control plane to 3 replicas (actual 0)
    >   ├─ClusterInfrastructure - OpenStackCluster/my-cluster
    >   ├─ControlPlane - KubeadmControlPlane/my-cluster-control-plane  False  Warning   ScalingUp                    2m30s  Scaling up control plane to 3 replicas (actual 0)
    >   └─Workers
    >     └─MachineDeployment/my-cluster-md-0                          False  Warning   WaitingForAvailableMachines  2m30s  Minimum availability requires 3 replicas, current 0 available
    >       └─4 Machines...                                            False  Info      WaitingForInfrastructure     2m30s  See m--END--
    >   
    >   
    >       kubectl \
    >           --namespace capo-system \
    >           logs \
    >           -l control-plane=capo-controller-manager \
    >           -c manager \
    >           --follow
    >   
    >   ....
    >   E0426 12:36:21.015277       1 controller.go:326] "Reconciler error" err="failed to unmarshal clouds credentials stored in secret my-cluster-cloud-credentials: error unmarshaling JSON: while decoding JSON: json: cannot unmarshal string into Go struct field Cloud.clouds.verify of type bool" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/my-cluster-md-0-d21456b6-c2fsb" namespace="default" name="my-cluster-md-0-d21456b6-c2fsb" reconcileID=afa97050-f08f-4194-9d16-f339b1cea457
    >   E0426 12:36:22.338144       1 controller.go:326] "Reconciler error" err="failed to unmarshal clouds credentials stored in secret my-cluster-cloud-credentials: error unmarshaling JSON: while decoding JSON: json: cannot unmarshal string into Go struct field Cloud.clouds.verify of type bool" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/my-cluster" namespace="default" name="my-cluster" reconcileID=6c9ee371-069a-4734-bbdb-24c2b1cad93e
    >   ....












