#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2024, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: []
#

    Target:

        One final test would be to start with the working deployment and make one change to see if it still needs the extra DNS address.
        If it does, then we need to request the address of a DNS server inside Somerville that we can use.
        Using 8.8.8.8 is more than a bit lame.

    Result:

        Success.
        Functioning dashboard and Zeppelin.
        Without the custom DNS address.


# -----------------------------------------------------
# Start a new branch.
#[user@desktop]

    branchname=jade-dns-test

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        newbranch=$(date '+%Y%m%d')-zrq-${branchname:?}

        git checkout master

        git checkout -b "${newbranch:?}"

    popd


# -----------------------------------------------------
# Check the DNS setting is present.
#[user@laptop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        vi deployments/cluster-api/ansible/templates/clusterapi-config.j2

            clusterNetworking:
              dnsNameservers:
                - "{{ deployments[aglais.openstack.cloud.site].dnsservers }}"

    popd


# -----------------------------------------------------
# Run our local client.
#[user@laptop]

    source "${HOME:?}/aglais.env"
    export PATH=${PATH}:${AGLAIS_CODE}/bin

    agclient jade

    >   ....
    >   ....


# -----------------------------------------------------
# Delete and create everything.
#[root@ansibler]

    export cloudsite=somerville-jade

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    ansible-playbook \
        --inventory 'bootstrap,' \
        '/deployments/cluster-api/ansible/00-create-all.yml'

    >   ....
    >   ....
    >   PLAY RECAP *******************************************************************************************************
    >   bootstrap                  : ok=58   changed=45   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   localhost                  : ok=35   changed=26   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


# -----------------------------------------------------
# Check the cluster status.
#[root@ansibler]

    ssh bootstrap -t \
        '
        source loadconfig
        watch \
            clusterctl \
                --kubeconfig "${kindclusterconf:?}" \
                describe cluster \
                    "${workclustername:?}"
        '

    >   NAME                                                                              READY  SEVERITY  REASON  SINCE  MESSAGE
    >   Cluster/somerville-jade-20240119-work                                             True                     4m54s
    >   ├─ClusterInfrastructure - OpenStackCluster/somerville-jade-20240119-work
    >   ├─ControlPlane - KubeadmControlPlane/somerville-jade-20240119-work-control-plane  True                     4m54s
    >   │ └─Machine/somerville-jade-20240119-work-control-plane-2chzx                     True                     6m46s
    >   └─Workers
    >     └─MachineDeployment/somerville-jade-20240119-work-md-0                          True                     22s
    >       └─3 Machines...                                                               True                     4m22s  See somerville-jade-20240119-work-md-0-cqbbb-9ghc7, somerville-jade-20240119-work-md-0-cqbbb-gb5mj, ...


# -----------------------------------------------------
# Get the location of our cluster config files.
#[root@ansibler]

    # TODO something to put this into the PATH
    export PATH=${PATH}:/deployments/cluster-api/ansible/files/aglais/bin
    source loadconfig


# -----------------------------------------------------
# Check of we can access the kubectl interface.
#[root@ansibler]

    source loadconfig
    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        cluster-info

        #
        # No response - blocked by the University firewall.
        #


# -----------------------------------------------------
# Run a SOCKS proxy linking our client container to our bootstrap node.
# https://unix.stackexchange.com/questions/34004/how-does-tcp-keepalive-work-in-ssh
# https://unix.stackexchange.com/a/34201
#[root@ansibler]

    ssh \
        -n \
        -f \
        -N \
        -D '*:3000' \
        -o ServerAliveInterval=10 \
        -o ServerAliveCountMax=12 \
        bootstrap


# -----------------------------------------------------
# Modify our kubectl config to add a SOCKS proxy.
#[root@ansibler]

    source loadconfig
    vi "${workclusterconf:?}"

        apiVersion: v1
        kind: Config
        clusters:
        - cluster:
          name: somerville-jade-20240118-work
            ....
            server: https://192.41.122.195:6443
    +       proxy-url: socks5://localhost:3000/


# -----------------------------------------------------
# Try again.
#[root@ansibler]

    source loadconfig
    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        cluster-info

    >   Kubernetes control plane is running at https://192.41.122.114:6443
    >   CoreDNS is running at https://192.41.122.114:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy


# -----------------------------------------------------
# Deploy our gaia-dmp Helm chart.
#[root@ansibler]

    source loadconfig

    helm dependency build \
        --kubeconfig "${workclusterconf:?}" \
        '/deployments/cluster-api/helm/gaia-dmp'

    >   Saving 2 charts
    >   Deleting outdated charts


    helm upgrade \
        --wait \
        --debug \
        --kubeconfig "${workclusterconf:?}" \
        'gaia-dmp' \
        '/deployments/cluster-api/helm/gaia-dmp' \
        --install

    >   history.go:56: [debug] getting history for release gaia-dmp
    >   Release "gaia-dmp" does not exist. Installing it now.
    >   install.go:194: [debug] Original chart version: ""
    >   install.go:211: [debug] CHART PATH: /deployments/cluster-api/helm/gaia-dmp
    >   ....
    >   ....
    >   ready.go:277: [debug] Deployment is not ready: default/zeppelin-server. 0 out of 1 expected pods are ready
    >   ready.go:277: [debug] Deployment is not ready: default/zeppelin-server. 0 out of 1 expected pods are ready
    >   Error: timed out waiting for the condition
    >   helm.go:84: [debug] timed out waiting for the condition

    #
    # Had this before, just deploy again.
    #

    helm upgrade \
        --wait \
        --debug \
        --kubeconfig "${workclusterconf:?}" \
        'gaia-dmp' \
        '/deployments/cluster-api/helm/gaia-dmp' \
        --install

    >   history.go:56: [debug] getting history for release gaia-dmp
    >   upgrade.go:144: [debug] preparing upgrade for gaia-dmp
    >   upgrade.go:152: [debug] performing update for gaia-dmp
    >   upgrade.go:324: [debug] creating upgraded release for gaia-dmp
    >   client.go:338: [debug] checking 10 resources for changes
    >   client.go:617: [debug] Looks like there are no changes for Namespace "gaia-dmp"
    >   client.go:617: [debug] Looks like there are no changes for ServiceAccount "dashboard-admin-account"
    >   client.go:617: [debug] Looks like there are no changes for ServiceAccount "zeppelin-server"
    >   client.go:617: [debug] Looks like there are no changes for ConfigMap "zeppelin-server-conf-map"
    >   client.go:617: [debug] Looks like there are no changes for ConfigMap "zeppelin-server-conf"
    >   client.go:617: [debug] Looks like there are no changes for ClusterRole "zeppelin-server-role"
    >   client.go:617: [debug] Looks like there are no changes for ClusterRoleBinding "dashboard-admin-binding"
    >   client.go:617: [debug] Looks like there are no changes for RoleBinding "zeppelin-server-role-binding"
    >   client.go:617: [debug] Looks like there are no changes for Service "zeppelin-server"
    >   client.go:626: [debug] Patch Deployment "zeppelin-server" in namespace default
    >   upgrade.go:396: [debug] waiting for release gaia-dmp resources (created: 0 updated: 10  deleted: 0)
    >   wait.go:48: [debug] beginning wait for 10 resources with timeout of 5m0s
    >   upgrade.go:159: [debug] updating status for upgraded release for gaia-dmp
    >   Release "gaia-dmp" has been upgraded. Happy Helming!
    >   NAME: gaia-dmp
    >   LAST DEPLOYED: Fri Jan 19 16:11:05 2024
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 2
    >   TEST SUITE: None
    >   ....
    >   ....


# -----------------------------------------------------
# Generate a dashboard token.
#[root@ansibler]

    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --namespace "gaia-dmp" \
        create token \
            "dashboard-admin-account"

    >   ....
    >   ....


# -----------------------------------------------------
# Launch a kubectl proxy.
#[root@ansibler]

    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --address 0.0.0.0 \
        proxy \
        &

    >   Starting to serve on [::]:8001


# -----------------------------------------------------
# -----------------------------------------------------
# Get the published port number for our agclient.
#[user@desktop]

    agcolour=jade

    podman container \
        inspect \
            "ansibler-${agcolour:?}" \
            --format json \
    | jq -r '
        .[0]
        | .HostConfig.PortBindings
        '

    >   {
    >     "8001/tcp": [
    >       {
    >         "HostIp": "127.0.0.1",
    >         "HostPort": "34915"
    >       }
    >     ]
    >   }

    kubeport=$(
        podman container \
            inspect \
                "ansibler-${agcolour:?}" \
                --format json \
        | jq -r '
            .[0]
            | .HostConfig.PortBindings
            | ."8001/tcp"
            | .[0].HostPort
            '
        )

    echo "kubeport [${kubeport}]"

    >   kubeport [34915]


# -----------------------------------------------------
# -----------------------------------------------------
# Launch browser pointed at the dashboard.
#[user@desktop]

    firefox \
        --new-window \
        "http://localhost:${kubeport:?}/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/#/login" \
        &

    >   ....
    >   ....

    #
    # Dashboard works OK.
    #

# -----------------------------------------------------
# Launch browser pointed at Zeppelin.
#[user@desktop]

    firefox \
        --new-window \
        "http://localhost:${kubeport:?}/api/v1/namespaces/default/services/http:zeppelin-server:http/proxy/#/" \
        &

    >   ....
    >   ....

    #
    # Zeppelin is there .. but only part of the front page is displayed.
    # Suspect that multiple proxies ontop of proxies is mangling the 'clever' JS UI app.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Remove the DNS setting from the StackHPC Helm chart config.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        vi deployments/cluster-api/ansible/templates/clusterapi-config.j2

            clusterNetworking:
        ~   # dnsNameservers:
        ~   #   - "{{ deployments[aglais.openstack.cloud.site].dnsservers }}"

    popd


# -----------------------------------------------------
# Run our local client.
#[user@laptop]

    source "${HOME:?}/aglais.env"
    export PATH=${PATH}:${AGLAIS_CODE}/bin

    agclient jade

    >   ....
    >   ....


# -----------------------------------------------------
# Delete and create everything.
#[root@ansibler]

    export cloudsite=somerville-jade

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    ansible-playbook \
        --inventory 'bootstrap,' \
        '/deployments/cluster-api/ansible/00-create-all.yml'

    >   ....
    >   ....
    >   PLAY RECAP *******************************************************************************************************
    >   bootstrap                  : ok=58   changed=45   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   localhost                  : ok=35   changed=26   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


# -----------------------------------------------------
# Check the cluster status.
#[root@ansibler]

    ssh bootstrap -t \
        '
        source loadconfig
        watch \
            clusterctl \
                --kubeconfig "${kindclusterconf:?}" \
                describe cluster \
                    "${workclustername:?}"
        '

    >   NAME                                                                              READY  SEVERITY  REASON  SINCE  MESSAGE
    >   Cluster/somerville-jade-20240119-work                                             True                     10m
    >   ├─ClusterInfrastructure - OpenStackCluster/somerville-jade-20240119-work
    >   ├─ControlPlane - KubeadmControlPlane/somerville-jade-20240119-work-control-plane  True                     10m
    >   │ └─Machine/somerville-jade-20240119-work-control-plane-mnz68                     True                     12m
    >   └─Workers
    >     └─MachineDeployment/somerville-jade-20240119-work-md-0                          True                     6m30s
    >       └─3 Machines...                                                               True                     10m    See somerville-jade-20240119-work-md-0-ksgwd-85w25, somerville-jade-20240119-work-md-0-ksgwd-llkp7, ...


# -----------------------------------------------------
# Get the location of our cluster config files.
#[root@ansibler]

    # TODO something to put this into the PATH
    export PATH=${PATH}:/deployments/cluster-api/ansible/files/aglais/bin
    source loadconfig


# -----------------------------------------------------
# Run a SOCKS proxy linking our client container to our bootstrap node.
# https://unix.stackexchange.com/questions/34004/how-does-tcp-keepalive-work-in-ssh
# https://unix.stackexchange.com/a/34201
#[root@ansibler]

    ssh \
        -n \
        -f \
        -N \
        -D '*:3000' \
        -o ServerAliveInterval=10 \
        -o ServerAliveCountMax=12 \
        bootstrap


# -----------------------------------------------------
# Modify our kubectl config to add a SOCKS proxy.
#[root@ansibler]

    source loadconfig
    vi "${workclusterconf:?}"

        apiVersion: v1
        kind: Config
        clusters:
        - cluster:
          name: somerville-jade-20240118-work
            ....
            server: https://192.41.122.195:6443
    +       proxy-url: socks5://localhost:3000/


# -----------------------------------------------------
# Check the cluster info.
#[root@ansibler]

    source loadconfig
    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        cluster-info

    >   Kubernetes control plane is running at https://192.41.122.19:6443
    >   CoreDNS is running at https://192.41.122.19:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy


# -----------------------------------------------------
# Deploy our gaia-dmp Helm chart.
#[root@ansibler]

    source loadconfig

    helm dependency build \
        --kubeconfig "${workclusterconf:?}" \
        '/deployments/cluster-api/helm/gaia-dmp'

    >   Saving 2 charts
    >   Deleting outdated charts


    helm upgrade \
        --wait \
        --debug \
        --kubeconfig "${workclusterconf:?}" \
        'gaia-dmp' \
        '/deployments/cluster-api/helm/gaia-dmp' \
        --install

    >   history.go:56: [debug] getting history for release gaia-dmp
    >   Release "gaia-dmp" does not exist. Installing it now.
    >   install.go:194: [debug] Original chart version: ""
    >   install.go:211: [debug] CHART PATH: /deployments/cluster-api/helm/gaia-dmp
    >   ....
    >   ....
    >   ready.go:277: [debug] Deployment is not ready: default/zeppelin-server. 0 out of 1 expected pods are ready
    >   ready.go:277: [debug] Deployment is not ready: default/zeppelin-server. 0 out of 1 expected pods are ready
    >   Error: timed out waiting for the condition
    >   helm.go:84: [debug] timed out waiting for the condition

    >   history.go:56: [debug] getting history for release gaia-dmp
    >   upgrade.go:144: [debug] preparing upgrade for gaia-dmp
    >   upgrade.go:152: [debug] performing update for gaia-dmp
    >   upgrade.go:324: [debug] creating upgraded release for gaia-dmp
    >   client.go:338: [debug] checking 10 resources for changes
    >   client.go:617: [debug] Looks like there are no changes for Namespace "gaia-dmp"
    >   client.go:617: [debug] Looks like there are no changes for ServiceAccount "dashboard-admin-account"
    >   client.go:617: [debug] Looks like there are no changes for ServiceAccount "zeppelin-server"
    >   client.go:617: [debug] Looks like there are no changes for ConfigMap "zeppelin-server-conf-map"
    >   client.go:617: [debug] Looks like there are no changes for ConfigMap "zeppelin-server-conf"
    >   client.go:617: [debug] Looks like there are no changes for ClusterRole "zeppelin-server-role"
    >   client.go:617: [debug] Looks like there are no changes for ClusterRoleBinding "dashboard-admin-binding"
    >   client.go:617: [debug] Looks like there are no changes for RoleBinding "zeppelin-server-role-binding"
    >   client.go:617: [debug] Looks like there are no changes for Service "zeppelin-server"
    >   client.go:626: [debug] Patch Deployment "zeppelin-server" in namespace default
    >   upgrade.go:396: [debug] waiting for release gaia-dmp resources (created: 0 updated: 10  deleted: 0)
    >   wait.go:48: [debug] beginning wait for 10 resources with timeout of 5m0s
    >   upgrade.go:159: [debug] updating status for upgraded release for gaia-dmp
    >   Release "gaia-dmp" has been upgraded. Happy Helming!
    >   NAME: gaia-dmp
    >   LAST DEPLOYED: Fri Jan 19 17:03:28 2024
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 2
    >   TEST SUITE: None
    >   ....
    >   ....


# -----------------------------------------------------
# Generate a dashboard token.
#[root@ansibler]

    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --namespace "gaia-dmp" \
        create token \
            "dashboard-admin-account"

    >   ....
    >   ....


# -----------------------------------------------------
# Launch a kubectl proxy.
#[root@ansibler]

    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --address 0.0.0.0 \
        proxy \
        &

    >   Starting to serve on [::]:8001


# -----------------------------------------------------
# -----------------------------------------------------
# Get the published port number for our agclient.
#[user@desktop]

    agcolour=jade

    kubeport=$(
        podman container \
            inspect \
                "ansibler-${agcolour:?}" \
                --format json \
        | jq -r '
            .[0]
            | .HostConfig.PortBindings
            | ."8001/tcp"
            | .[0].HostPort
            '
        )

    echo "kubeport [${kubeport}]"

    >   kubeport [38605]


# -----------------------------------------------------
# Launch browser pointed at the dashboard.
#[user@desktop]

    firefox \
        --new-window \
        "http://localhost:${kubeport:?}/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/#/login" \
        &

    >   ....
    >   ....

# -----------------------------------------------------
# Launch browser pointed at Zeppelin.
#[user@desktop]

    firefox \
        --new-window \
        "http://localhost:${kubeport:?}/api/v1/namespaces/default/services/http:zeppelin-server:http/proxy/#/" \
        &

    >   ....
    >   ....

    #
    # Functioning dashboard and Zeppelin.
    # Without the custom DNS address.
    #

