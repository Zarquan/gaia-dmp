#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: []
#


    Target:

        Figure out why the Kubernetes cluster deployment is failing.
        Capture a full set of logs from all the Pods.

    Result:

        Work in progress ...

# -----------------------------------------------------
# Check which platform is live.
#[user@desktop]

    ssh fedora@live.gaia-dmp.uk \
        '
        date
        hostname
        '

    >   Fri 11 Aug 15:31:03 UTC 2023
    >   iris-gaia-green-20230308-zeppelin


# -----------------------------------------------------
# Create our client container.
#[user@desktop]

    agclient blue

    >   ....
    >   ....


# -----------------------------------------------------
# Delete and deploy everything.
#[root@ansibler]

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    ansible-playbook \
        --inventory 'bootstrap,' \
        '/deployments/cluster-api/bootstrap/ansible/00-create-all.yml'

    >   ....
    >   ....
    >   fatal: [bootstrap]: FAILED! => {
    >       "changed": false,
    >       "command": "
    >           /usr/local/bin/helm \
    >               --version=0.1.0 \
    >               upgrade \
    >               -i \
    >               --reset-values \
    >               --wait \
    >               --values=/opt/aglais/clusterapi-config.yml \
    >               --values=/opt/aglais/openstack-clouds.yml \
    >               iris-gaia-blue-20230811-work \
    >               capi/openstack-cluster
    >           ",
    >       "msg": "
    >           Failure when executing Helm command. Exited 1.
    >           stdout: Release \"iris-gaia-blue-20230811-work\" does not exist.
    >           Installing it now.
    >           stderr: Error: context deadline exceeded
    >           ",
    >       "stderr": "Error: context deadline exceeded",
    >       "stderr_lines": [
    >           "Error: context deadline exceeded"
    >           ],
    >       "stdout": "Release \"iris-gaia-blue-20230811-work\" does not exist. Installing it now.",
    >       "stdout_lines": [
    >           "Release \"iris-gaia-blue-20230811-work\" does not exist. Installing it now."
    >           ]
    >       }


# -----------------------------------------------------
# -----------------------------------------------------
# Login to our bootstrap node as root.
#[root@ansibler]

    podman exec -it ansibler-blue bash

    ssh root@bootstrap

    source loadconfig

cat << EOF
kindclustername [${kindclustername}]
kindclusterconf [${kindclusterconf}]
workclustername [${workclustername}]
workclusterconf [${workclusterconf}]
EOF

    >   kindclustername [iris-gaia-blue-20230811-kind]
    >   kindclusterconf [/opt/aglais/iris-gaia-blue-20230811-kind.yml]
    >   workclustername [iris-gaia-blue-20230811-work]
    >   workclusterconf [/opt/aglais/iris-gaia-blue-20230811-work.yml]


# -----------------------------------------------------
# Grab a dump of the logs from all the Pods.
#[user@bootstrap]

    mkdir /tmp/logs

    for podnamespace in $(
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            get pods \
                --output json \
                --all-namespaces \
        | jq -r '.items[].metadata | {namespace, name} | tojson'
        )
    do
        echo ""
        echo "----"

        namespace=$(echo ${podnamespace} | jq -r '.namespace')
        name=$(echo  ${podnamespace} | jq -r '.name')

        echo "Space   [${namespace}]"
        echo "Name    [${name}]"

        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            logs \
                --namespace "${namespace:?}"  \
                "${name:?}" \
        > "/tmp/logs/${name:?}.log"

    done

    >   ----
    >   Space   [capi-kubeadm-bootstrap-system]
    >   Name    [capi-kubeadm-bootstrap-controller-manager-59c54786d5-2bnvs]
    >   
    >   ----
    >   Space   [capi-kubeadm-control-plane-system]
    >   Name    [capi-kubeadm-control-plane-controller-manager-845544c868-7kbjh]
    >   
    >   ----
    >   Space   [capi-system]
    >   Name    [capi-controller-manager-c7bc68d54-sbwhv]
    >   
    >   ----
    >   Space   [capo-system]
    >   Name    [capo-controller-manager-6d9f44548f-b8drd]
    >   
    >   ----
    >   Space   [cert-manager]
    >   Name    [cert-manager-66d9545484-b4ws2]
    >   
    >   ----
    >   Space   [cert-manager]
    >   Name    [cert-manager-cainjector-7d8b6bd6fb-5lkjn]
    >   
    >   ----
    >   Space   [cert-manager]
    >   Name    [cert-manager-webhook-669b96dcfd-2l58m]
    >   
    >   ----
    >   Space   [default]
    >   Name    [cluster-api-addon-provider-66cc76bbbf-tmfgb]
    >   
    >   ----
    >   Space   [default]
    >   Name    [iris-gaia-blue-20230811-work-autoscaler-5579c8d7f8-wrb8r]
    >   Error from server (BadRequest): container "autoscaler" in pod "iris-gaia-blue-20230811-work-autoscaler-5579c8d7f8-wrb8r" is waiting to start: ContainerCreating
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [coredns-5d78c9869d-4grd8]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [coredns-5d78c9869d-f2pcd]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [etcd-iris-gaia-blue-20230811-kind-control-plane]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [kindnet-4b66d]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [kube-apiserver-iris-gaia-blue-20230811-kind-control-plane]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [kube-controller-manager-iris-gaia-blue-20230811-kind-control-plane]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [kube-proxy-gdwsp]
    >   
    >   ----
    >   Space   [kube-system]
    >   Name    [kube-scheduler-iris-gaia-blue-20230811-kind-control-plane]
    >   
    >   ----
    >   Space   [local-path-storage]
    >   Name    [local-path-provisioner-6bc4bddd6b-xm9q5]


# -----------------------------------------------------
# Fold the logs into an archive.
#[user@bootstrap]

    pushd /tmp

        tar -cvzf 20230911-logs.tar.gz logs

# -----------------------------------------------------
# -----------------------------------------------------
# Copy the logs back to our client.
#[root@ansibler]

    scp bootstrap:/tmp/20230911-logs.tar.gz .

    >   20230911-logs.tar.gz    100%   47KB 577.1KB/s   00:00


# -----------------------------------------------------
# -----------------------------------------------------
# Transfer the logs to our desktop.
#[user@desktop]

    pushd /var/local/backups
        pushd aglais/2023

            mkdir 20230911
            pushd 20230911

                podman cp ansibler-blue:/20230911-logs.tar.gz .

