#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-indent
#zrq-notes-ansible
#zrq-notes-osformat
#



    ssh -A fedora@blue.aglais.uk

        cd /home/fedora/zeppelin/logs

        less zeppelin-interpreter-spark-Zeishoo3-Zeishoo3-fedora-iris-gaia-blue-20220525-zeppelin.log


    >    ....
    >    ....
    >    INFO [2022-05-25 07:37:20,448] ({pool-3-thread-1} ZeppelinLocationStrategy.java[locate]:44) - Load configuration from /home/fedora/zeppelin/conf/zeppelin-site.xml
    >    INFO [2022-05-25 07:37:20,448] ({pool-3-thread-1} ZeppelinLocationStrategy.java[locate]:44) - Load configuration from /home/fedora/zeppelin/conf/zeppelin-site.xml
    >    INFO [2022-05-25 07:37:20,505] ({pool-3-thread-1} ZeppelinConfiguration.java[create]:135) - Server Host: 0.0.0.0
    >    INFO [2022-05-25 07:37:20,505] ({pool-3-thread-1} ZeppelinConfiguration.java[create]:139) - Server Port: 8080
    >    INFO [2022-05-25 07:37:20,505] ({pool-3-thread-1} ZeppelinConfiguration.java[create]:141) - Context Path: /
    >    INFO [2022-05-25 07:37:20,506] ({pool-3-thread-1} ZeppelinConfiguration.java[create]:142) - Zeppelin Version: 0.10.0
    >    INFO [2022-05-25 07:37:20,507] ({pool-3-thread-1} RemoteInterpreterServer.java[createLifecycleManager]:287) - Creating interpreter lifecycle manager: org.apache.zeppelin.interpreter.lifecycle.NullLifecycleManager
    >    INFO [2022-05-25 07:37:20,507] ({pool-3-thread-1} RemoteInterpreterServer.java[init]:230) - Creating RemoteInterpreterEventClient with connection pool size: 100
    >    INFO [2022-05-25 07:37:20,575] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:383) - Instantiate interpreter org.apache.zeppelin.spark.SparkInterpreter
    >    INFO [2022-05-25 07:37:20,578] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:383) - Instantiate interpreter org.apache.zeppelin.spark.SparkSqlInterpreter
    >    INFO [2022-05-25 07:37:20,581] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:383) - Instantiate interpreter org.apache.zeppelin.spark.PySparkInterpreter
    >    INFO [2022-05-25 07:37:20,584] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:383) - Instantiate interpreter org.apache.zeppelin.spark.IPySparkInterpreter
    >    INFO [2022-05-25 07:37:20,585] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:383) - Instantiate interpreter org.apache.zeppelin.spark.SparkRInterpreter
    >    INFO [2022-05-25 07:37:20,587] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:383) - Instantiate interpreter org.apache.zeppelin.spark.SparkIRInterpreter
    >    INFO [2022-05-25 07:37:20,588] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:383) - Instantiate interpreter org.apache.zeppelin.spark.SparkShinyInterpreter
    >    INFO [2022-05-25 07:37:20,620] ({pool-3-thread-1} RemoteInterpreterServer.java[createInterpreter]:383) - Instantiate interpreter org.apache.zeppelin.spark.KotlinSparkInterpreter
    >    INFO [2022-05-25 07:37:20,652] ({pool-3-thread-2} SchedulerFactory.java[<init>]:56) - Scheduler Thread Pool Size: 100
    >    INFO [2022-05-25 07:37:20,653] ({pool-3-thread-2} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_38560101
    >    INFO [2022-05-25 07:37:20,654] ({pool-3-thread-2} SchedulerFactory.java[createOrGetParallelScheduler]:88) - Create ParallelScheduler: org.apache.zeppelin.spark.SparkSqlInterpreter814307748 with maxConcurrency: 10
    >    INFO [2022-05-25 07:37:20,655] ({pool-3-thread-2} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_952594110
    >    INFO [2022-05-25 07:37:20,655] ({pool-3-thread-2} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_566313841
    >    INFO [2022-05-25 07:37:20,656] ({pool-3-thread-2} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: org.apache.zeppelin.spark.SparkRInterpreter710436179
    >    INFO [2022-05-25 07:37:20,656] ({pool-3-thread-2} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_1116723329
    >    INFO [2022-05-25 07:37:20,656] ({pool-3-thread-2} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_1224535072
    >    INFO [2022-05-25 07:37:20,656] ({pool-3-thread-2} SchedulerFactory.java[createOrGetFIFOScheduler]:76) - Create FIFOScheduler: interpreter_1808457449
    >    ....
    >    ....
    >    ....
    >    ....
    >    ....
    >    INFO [2022-05-25 07:37:25,251] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logInfo]:57) - Bound SparkUI to 0.0.0.0, and started at http://zeppelin:4040
    >    INFO [2022-05-25 07:37:25,266] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logInfo]:57) - Added JAR file:/home/fedora/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar at s
    >   park://zeppelin:38353/jars/spark-interpreter-0.10.0.jar with timestamp 1653464244490
    >    WARN [2022-05-25 07:37:25,319] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logWarning]:69) - Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order.
    >   To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
    >    INFO [2022-05-25 07:37:25,322] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logInfo]:57) - Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
    >    WARN [2022-05-25 07:37:25,353] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logWarning]:69) - spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, i
    >   gnoring its setting, please update your configs.
    >    INFO [2022-05-25 07:37:25,354] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logInfo]:57) - Using initial executors = 15, max of spark.dynamicAllocation.initialExecutors, spark.dyn
    >   amicAllocation.minExecutors and spark.executor.instances
    >    INFO [2022-05-25 07:37:25,419] ({FIFOScheduler-interpreter_952594110-Worker-1} RMProxy.java[newProxyInstance]:133) - Connecting to ResourceManager at master01/10.10.1.154:8032
    >    INFO [2022-05-25 07:37:25,643] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logInfo]:57) - Requesting a new application from cluster with 6 NodeManagers
    >    INFO [2022-05-25 07:37:26,162] ({FIFOScheduler-interpreter_952594110-Worker-1} Configuration.java[getConfResourceAsInputStream]:2752) - resource-types.xml not found
    >    INFO [2022-05-25 07:37:26,162] ({FIFOScheduler-interpreter_952594110-Worker-1} ResourceUtils.java[addResourcesFileToConf]:419) - Unable to find 'resource-types.xml'.
    >    INFO [2022-05-25 07:37:26,174] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logInfo]:57) - Verifying our application has not requested more than the maximum memory capability of t
    >   he cluster (43008 MB per container)
    >    INFO [2022-05-25 07:37:26,174] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logInfo]:57) - Will allocate AM container, with 2432 MB memory including 384 MB overhead
    >    INFO [2022-05-25 07:37:26,175] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logInfo]:57) - Setting up container launch context for our AM
    >    INFO [2022-05-25 07:37:26,177] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logInfo]:57) - Setting up the launch environment for our AM container
    >    INFO [2022-05-25 07:37:26,181] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logInfo]:57) - Preparing resources for our AM container
    >   ERROR [2022-05-25 07:37:26,252] ({FIFOScheduler-interpreter_952594110-Worker-1} Logging.scala[logError]:94) - Error initializing SparkContext.
    >   org.apache.hadoop.security.AccessControlException: Permission denied: user=Zeishoo3, access=WRITE, inode="/":fedora:supergroup:drwxr-xr-x
    >           at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
    >           at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
    >           at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
    >           at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1879)
    >           at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1863)
    >           at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1822)
    >    ....
    >    ....


# -----------------------------------------------------
# Check the HDFS directory permissions.
# https://community.cloudera.com/t5/Support-Questions/Permission-Error-while-running-spark-shell/m-p/49445/highlight/true#M23315
#[fedora@zeppelin]

    hdfs dfs -ls /

    >   Found 2 items
    >   drwxr-xr-x   - fedora supergroup          0 2022-05-25 07:31 /hdfs-test
    >   drwxr-xr-x   - fedora supergroup          0 2022-05-25 07:08 /spark-log


    hdfs dfs -ls /hdfs-test

    >   Found 1 items
    >   drwxr-xr-x   - Zeishoo3 supergroup          0 2022-05-25 07:31 /hdfs-test/Zeishoo3

    #
    # We need to set the default /tmp dir
    # and we probably need to set the HDFS home dir to /user
    # https://www.informit.com/articles/article.aspx?p=2755708&seqNum=3
    #


https://hadoop.apache.org/docs/r2.4.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml

https://www.informit.com/articles/article.aspx?p=2755708&seqNum=3

https://docs.cloudera.com/HDPDocuments/Ambari-2.6.1.5/bk_ambari-views/content/setup_HDFS_user_directory_pig_view.html

https://hadoop.apache.org/docs/r3.1.1/hadoop-project-dist/hadoop-common/jdiff/xml/Apache_Hadoop_Common_3.1.1.xml

https://www.tabnine.com/code/java/methods/org.apache.hadoop.fs.FileSystem/makeQualified







