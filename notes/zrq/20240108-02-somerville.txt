#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2024, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Getting a deployment at Somerville to work again.

    Result:

        Work in progress ...


# -----------------------------------------------------
# Start the VPN client to put us inside the University network.
#[user@laptop]

    mkdir "${HOME}/Auth/openconnect/"
    cat > "${HOME}/Auth/openconnect/ed.ac.uk.cfg" << EOF
protocol fortinet
server remote.net.ed.ac.uk:8443
user dmorris8
passwd-on-stdin
EOF

    # Local getsecret is broken, but we can use the one on our desktop.

    ssh 10.1.0.2 'getsecret "edinburgh.vpn"' \
    | sudo openconnect \
        --verbose \
        --config "${HOME}/Auth/openconnect/ed.ac.uk.cfg"

    >   ....
    >   Send PPP echo request as DPD
    >   Send PPP echo request as DPD
    >   ....


# -----------------------------------------------------
# Run our local client.
#[user@laptop]

    agclient jade

    >   ....
    >   ....


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


# -----------------------------------------------------
# Create everything.
#[root@ansibler]

    export cloudsite=somerville-jade

    ansible-playbook \
        --inventory 'bootstrap,' \
        '/deployments/cluster-api/bootstrap/ansible/00-create-all.yml'

    >   ....
    >   ....
    >   TASK [Create work cluster [somerville-gaia-jade-20240108-work]] ********************************************************************************
    >   fatal: [bootstrap]: FAILED! => {
    >       "changed": false,
    >       "command": "
    >           /usr/local/bin/helm
    >               --version=0.1.0
    >               upgrade
    >               -i
    >               --reset-values
    >               --wait
    >               --values=/opt/aglais/clusterapi-config.yml
    >               --values=/opt/aglais/openstack-clouds.yml
    >               somerville-gaia-jade-20240108-work
    >               capi/openstack-cluster
    >           ",
    >       "msg": "Failure when executing Helm command.
    >           Exited 1.
    >           stdout: Release \"somerville-gaia-jade-20240108-work\" does not exist. Installing it now.
    >           stderr: Error: context deadline exceeded",
    >       "stderr": "Error: context deadline exceeded",
    >       "stderr_lines": [
    >           "Error: context deadline exceeded"
    >           ],
    >       "stdout": "Release \"somerville-gaia-jade-20240108-work\" does not exist. Installing it now.",
    >       "stdout_lines": [
    >           "Release \"somerville-gaia-jade-20240108-work\" does not exist. Installing it now."
    >           ]
    >       }


# -----------------------------------------------------
# ....
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        server list

    >   +--------------------------------------+----------------------------------------------+--------+-----------------------------------------------------------------------------+------------------------------+---------------+
    >   | ID                                   | Name                                         | Status | Networks                                                                    | Image                        | Flavor        |
    >   +--------------------------------------+----------------------------------------------+--------+-----------------------------------------------------------------------------+------------------------------+---------------+
    >   | 11a36356-d568-4039-8e24-794d62bd22ef | somerville-gaia-jade-20240108-bootstrap-node | ACTIVE | somerville-gaia-jade-20240108-bootstrap-network=10.10.2.246, 192.41.122.106 | gaia-dmp-fedora-cloud-38-1.6 | qserv-jump-v2 |
    >   +--------------------------------------+----------------------------------------------+--------+-----------------------------------------------------------------------------+------------------------------+---------------+


# -----------------------------------------------------
# ....
#[root@ansibler]

    ssh bootstrap

    >   ....
    >   ....


# -----------------------------------------------------
# ....
#[root@bootstrap]

    source loadconfig

    clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        describe cluster \
            "${workclustername:?}"

    >   NAME                                                                                   READY  SEVERITY  REASON                           SINCE  MESSAGE
    >   Cluster/somerville-gaia-jade-20240108-work                                             False  Warning   ScalingUp                        26m    Scaling up control plane to 3 replicas (actual 0)
    >   ├─ClusterInfrastructure - OpenStackCluster/somerville-gaia-jade-20240108-work
    >   ├─ControlPlane - KubeadmControlPlane/somerville-gaia-jade-20240108-work-control-plane  False  Warning   ScalingUp                        26m    Scaling up control plane to 3 replicas (actual 0)
    >   └─Workers
    >     └─MachineDeployment/somerville-gaia-jade-20240108-work-md-0                          False  Warning   WaitingForAvailableMachines      26m    Minimum availability requires 2 replicas, current 0 available
    >       └─3 Machines...                                                                    False  Info      WaitingForClusterInfrastructure  26m    See somerville-gaia-jade-20240108-work-md-0-cs7j7-8skms, somerville-gaia-jade-20240108-work-md-0-cs7j7-bn5xc, ...
    >   [root@somerville-gaia-jade-20240108-bootstrap-node ~]#


# -----------------------------------------------------
# ....
#[root@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get pods \
            --all-namespaces

    >   NAMESPACE                           NAME                                                                       READY   STATUS              RESTARTS   AGE
    >   capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-55d5767547-twxcs                 1/1     Running             0          30m
    >   capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-85fd48fb9b-mj98v             1/1     Running             0          30m
    >   capi-system                         capi-controller-manager-7cb6bcd4db-tmfd7                                   1/1     Running             0          30m
    >   capo-system                         capo-controller-manager-544cb69b9d-26dl9                                   1/1     Running             0          30m
    >   cert-manager                        cert-manager-66d9545484-t7lv5                                              1/1     Running             0          31m
    >   cert-manager                        cert-manager-cainjector-7d8b6bd6fb-q98r5                                   1/1     Running             0          31m
    >   cert-manager                        cert-manager-webhook-669b96dcfd-x872v                                      1/1     Running             0          31m
    >   default                             cluster-api-addon-provider-66cc76bbbf-lhjgk                                1/1     Running             0          30m
    >   default                             somerville-gaia-jade-20240108-work-autoscaler-5d4f9bb689-m4ms9             0/1     ContainerCreating   0          27m
    >   kube-system                         coredns-5d78c9869d-4l6zq                                                   1/1     Running             0          31m
    >   kube-system                         coredns-5d78c9869d-xl49l                                                   1/1     Running             0          31m
    >   kube-system                         etcd-somerville-gaia-jade-20240108-kind-control-plane                      1/1     Running             0          31m
    >   kube-system                         kindnet-fn6b7                                                              1/1     Running             0          31m
    >   kube-system                         kube-apiserver-somerville-gaia-jade-20240108-kind-control-plane            1/1     Running             0          31m
    >   kube-system                         kube-controller-manager-somerville-gaia-jade-20240108-kind-control-plane   1/1     Running             0          31m
    >   kube-system                         kube-proxy-6mp2s                                                           1/1     Running             0          31m
    >   kube-system                         kube-scheduler-somerville-gaia-jade-20240108-kind-control-plane            1/1     Running             0          31m
    >   local-path-storage                  local-path-provisioner-6bc4bddd6b-748jz                                    1/1     Running             0          31m


# -----------------------------------------------------
# ....
#[root@bootstrap]

    namespace=capi-system

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get pods \
            --namespace "${namespace:?}"

    >   NAME                                       READY   STATUS    RESTARTS   AGE
    >   capi-controller-manager-7cb6bcd4db-tmfd7   1/1     Running   0          32m


    podident=$(
        kubectl \
            --output json \
            --kubeconfig "${kindclusterconf:?}" \
            get pods \
                --namespace "${namespace:?}" \
        | jq -r '.items[0] | .metadata.name'
        )


    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        logs \
            --namespace "${namespace:?}" \
            "${podident}"

    >   ....
    >   I0108 19:49:10.109132       1 machine_controller_phases.go:292] "Waiting for infrastructure provider to create machine infrastructure and report status.ready" controller="machine" controllerGroup="cluster.x-k8s.io" controllerKind="Machine" Machine="default/somerville-gaia-jade-20240108-work-md-0-cs7j7-rh8nq" namespace="default" name="somerville-gaia-jade-20240108-work-md-0-cs7j7-rh8nq" reconcileID="1651fc40-ed12-475c-ac84-c2b484627192" MachineSet="default/somerville-gaia-jade-20240108-work-md-0-cs7j7" MachineDeployment="default/somerville-gaia-jade-20240108-work-md-0" Cluster="default/somerville-gaia-jade-20240108-work" OpenStackMachine="default/somerville-gaia-jade-20240108-work-md-0-aa43a292-qzcp9"
    >   I0108 19:49:10.109175       1 machine_controller_noderef.go:58] "Waiting for infrastructure provider to report spec.providerID" controller="machine" controllerGroup="cluster.x-k8s.io" controllerKind="Machine" Machine="default/somerville-gaia-jade-20240108-work-md-0-cs7j7-rh8nq" namespace="default" name="somerville-gaia-jade-20240108-work-md-0-cs7j7-rh8nq" reconcileID="1651fc40-ed12-475c-ac84-c2b484627192" MachineSet="default/somerville-gaia-jade-20240108-work-md-0-cs7j7" MachineDeployment="default/somerville-gaia-jade-20240108-work-md-0" Cluster="default/somerville-gaia-jade-20240108-work" OpenStackMachine="default/somerville-gaia-jade-20240108-work-md-0-aa43a292-qzcp9"
    >   ....


# -----------------------------------------------------
# ....
#[root@bootstrap]

    namespace=capo-system

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get pods \
            --namespace "${namespace:?}"

    >   NAME                                       READY   STATUS    RESTARTS   AGE
    >   capo-controller-manager-544cb69b9d-26dl9   1/1     Running   0          38m


    podident=$(
        kubectl \
            --output json \
            --kubeconfig "${kindclusterconf:?}" \
            get pods \
                --namespace "${namespace:?}" \
        | jq -r '.items[0] | .metadata.name'
        )


    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        logs \
            --namespace "${namespace:?}" \
            "${podident}"

    >   ....
    >   I0108 19:56:14.550842       1 openstackmachine_controller.go:302] "Cluster infrastructure is not ready yet, re-queuing machine" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/somerville-gaia-jade-20240108-work-md-0-aa43a292-qzcp9" namespace="default" name="somerville-gaia-jade-20240108-work-md-0-aa43a292-qzcp9" reconcileID="c5190055-5f36-4e38-94c1-caffdad2db96" openStackMachine="somerville-gaia-jade-20240108-work-md-0-aa43a292-qzcp9" machine="somerville-gaia-jade-20240108-work-md-0-cs7j7-rh8nq" cluster="somerville-gaia-jade-20240108-work" openStackCluster="somerville-gaia-jade-20240108-work"
    >   I0108 19:56:15.790622       1 openstackmachine_controller.go:302] "Cluster infrastructure is not ready yet, re-queuing machine" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/somerville-gaia-jade-20240108-work-md-0-aa43a292-qzcp9" namespace="default" name="somerville-gaia-jade-20240108-work-md-0-aa43a292-qzcp9" reconcileID="c5190055-5f36-4e38-94c1-caffdad2db96" openStackMachine="somerville-gaia-jade-20240108-work-md-0-aa43a292-qzcp9" machine="somerville-gaia-jade-20240108-work-md-0-cs7j7-rh8nq" cluster="somerville-gaia-jade-20240108-work" openStackCluster="somerville-gaia-jade-20240108-work"
    >   ....


# -----------------------------------------------------
# ....
#[root@bootstrap]

    namespace=capi-system

    podident=$(
        kubectl \
            --output json \
            --kubeconfig "${kindclusterconf:?}" \
            get pods \
                --namespace "${namespace:?}" \
        | jq -r '.items[0] | .metadata.name'
        )


    kubectl \
        --output json \
        --kubeconfig "${kindclusterconf:?}" \
        get pod \
            --namespace "${namespace:?}" \
            "${podident}" \
    | jq '.'

    >   {
    >     "apiVersion": "v1",
    >     "kind": "Pod",
    >     "metadata": {
    >       "creationTimestamp": "2024-01-08T19:16:43Z",
    >       "generateName": "capi-controller-manager-7cb6bcd4db-",
    >       "labels": {
    >         "cluster.x-k8s.io/provider": "cluster-api",
    >         "control-plane": "controller-manager",
    >         "pod-template-hash": "7cb6bcd4db"
    >       },
    >       "name": "capi-controller-manager-7cb6bcd4db-tmfd7",
    >       "namespace": "capi-system",
    >       "ownerReferences": [
    >         {
    >           "apiVersion": "apps/v1",
    >           "blockOwnerDeletion": true,
    >           "controller": true,
    >           "kind": "ReplicaSet",
    >           "name": "capi-controller-manager-7cb6bcd4db",
    >           "uid": "56f3a464-fb75-41fe-b0c9-43cb738f8c01"
    >         }
    >       ],
    >       "resourceVersion": "1085",
    >       "uid": "478d9645-a75c-4aa2-865f-7ea5cfc016ea"
    >     },
    >     "spec": {
    >       "containers": [
    >         {
    >           "args": [
    >             "--leader-elect",
    >             "--diagnostics-address=:8443",
    >             "--insecure-diagnostics=false",
    >             "--feature-gates=MachinePool=false,ClusterResourceSet=false,ClusterTopology=false,RuntimeSDK=false,MachineSetPreflightChecks=false"
    >           ],
    >           "command": [
    >             "/manager"
    >           ],
    >           "env": [
    >             {
    >               "name": "POD_NAMESPACE",
    >               "valueFrom": {
    >                 "fieldRef": {
    >                   "apiVersion": "v1",
    >                   "fieldPath": "metadata.namespace"
    >                 }
    >               }
    >             },
    >             {
    >               "name": "POD_NAME",
    >               "valueFrom": {
    >                 "fieldRef": {
    >                   "apiVersion": "v1",
    >                   "fieldPath": "metadata.name"
    >                 }
    >               }
    >             },
    >             {
    >               "name": "POD_UID",
    >               "valueFrom": {
    >                 "fieldRef": {
    >                   "apiVersion": "v1",
    >                   "fieldPath": "metadata.uid"
    >                 }
    >               }
    >             }
    >           ],
    >           "image": "registry.k8s.io/cluster-api/cluster-api-controller:v1.6.0",
    >           "imagePullPolicy": "IfNotPresent",
    >           "livenessProbe": {
    >             "failureThreshold": 3,
    >             "httpGet": {
    >               "path": "/healthz",
    >               "port": "healthz",
    >               "scheme": "HTTP"
    >             },
    >             "periodSeconds": 10,
    >             "successThreshold": 1,
    >             "timeoutSeconds": 1
    >           },
    >           "name": "manager",
    >           "ports": [
    >             {
    >               "containerPort": 9443,
    >               "name": "webhook-server",
    >               "protocol": "TCP"
    >             },
    >             {
    >               "containerPort": 9440,
    >               "name": "healthz",
    >               "protocol": "TCP"
    >             },
    >             {
    >               "containerPort": 8443,
    >               "name": "metrics",
    >               "protocol": "TCP"
    >             }
    >           ],
    >           "readinessProbe": {
    >             "failureThreshold": 3,
    >             "httpGet": {
    >               "path": "/readyz",
    >               "port": "healthz",
    >               "scheme": "HTTP"
    >             },
    >             "periodSeconds": 10,
    >             "successThreshold": 1,
    >             "timeoutSeconds": 1
    >           },
    >           "resources": {},
    >           "securityContext": {
    >             "allowPrivilegeEscalation": false,
    >             "capabilities": {
    >               "drop": [
    >                 "ALL"
    >               ]
    >             },
    >             "privileged": false,
    >             "runAsGroup": 65532,
    >             "runAsUser": 65532
    >           },
    >           "terminationMessagePath": "/dev/termination-log",
    >           "terminationMessagePolicy": "File",
    >           "volumeMounts": [
    >             {
    >               "mountPath": "/tmp/k8s-webhook-server/serving-certs",
    >               "name": "cert",
    >               "readOnly": true
    >             },
    >             {
    >               "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
    >               "name": "kube-api-access-tkvvm",
    >               "readOnly": true
    >             }
    >           ]
    >         }
    >       ],
    >       "dnsPolicy": "ClusterFirst",
    >       "enableServiceLinks": true,
    >       "nodeName": "somerville-gaia-jade-20240108-kind-control-plane",
    >       "preemptionPolicy": "PreemptLowerPriority",
    >       "priority": 0,
    >       "restartPolicy": "Always",
    >       "schedulerName": "default-scheduler",
    >       "securityContext": {
    >         "runAsNonRoot": true,
    >         "seccompProfile": {
    >           "type": "RuntimeDefault"
    >         }
    >       },
    >       "serviceAccount": "capi-manager",
    >       "serviceAccountName": "capi-manager",
    >       "terminationGracePeriodSeconds": 10,
    >       "tolerations": [
    >         {
    >           "effect": "NoSchedule",
    >           "key": "node-role.kubernetes.io/master"
    >         },
    >         {
    >           "effect": "NoSchedule",
    >           "key": "node-role.kubernetes.io/control-plane"
    >         },
    >         {
    >           "effect": "NoExecute",
    >           "key": "node.kubernetes.io/not-ready",
    >           "operator": "Exists",
    >           "tolerationSeconds": 300
    >         },
    >         {
    >           "effect": "NoExecute",
    >           "key": "node.kubernetes.io/unreachable",
    >           "operator": "Exists",
    >           "tolerationSeconds": 300
    >         }
    >       ],
    >       "volumes": [
    >         {
    >           "name": "cert",
    >           "secret": {
    >             "defaultMode": 420,
    >             "secretName": "capi-webhook-service-cert"
    >           }
    >         },
    >         {
    >           "name": "kube-api-access-tkvvm",
    >           "projected": {
    >             "defaultMode": 420,
    >             "sources": [
    >               {
    >                 "serviceAccountToken": {
    >                   "expirationSeconds": 3607,
    >                   "path": "token"
    >                 }
    >               },
    >               {
    >                 "configMap": {
    >                   "items": [
    >                     {
    >                       "key": "ca.crt",
    >                       "path": "ca.crt"
    >                     }
    >                   ],
    >                   "name": "kube-root-ca.crt"
    >                 }
    >               },
    >               {
    >                 "downwardAPI": {
    >                   "items": [
    >                     {
    >                       "fieldRef": {
    >                         "apiVersion": "v1",
    >                         "fieldPath": "metadata.namespace"
    >                       },
    >                       "path": "namespace"
    >                     }
    >                   ]
    >                 }
    >               }
    >             ]
    >           }
    >         }
    >       ]
    >     },
    >     "status": {
    >       "conditions": [
    >         {
    >           "lastProbeTime": null,
    >           "lastTransitionTime": "2024-01-08T19:16:43Z",
    >           "status": "True",
    >           "type": "Initialized"
    >         },
    >         {
    >           "lastProbeTime": null,
    >           "lastTransitionTime": "2024-01-08T19:17:07Z",
    >           "status": "True",
    >           "type": "Ready"
    >         },
    >         {
    >           "lastProbeTime": null,
    >           "lastTransitionTime": "2024-01-08T19:17:07Z",
    >           "status": "True",
    >           "type": "ContainersReady"
    >         },
    >         {
    >           "lastProbeTime": null,
    >           "lastTransitionTime": "2024-01-08T19:16:43Z",
    >           "status": "True",
    >           "type": "PodScheduled"
    >         }
    >       ],
    >       "containerStatuses": [
    >         {
    >           "containerID": "containerd://07c97c19f9985c711115b7f859dfb5eebb931031553af5bf473e0f9549f2f002",
    >           "image": "registry.k8s.io/cluster-api/cluster-api-controller:v1.6.0",
    >           "imageID": "registry.k8s.io/cluster-api/cluster-api-controller@sha256:211632c5b695212bce78e0d35da5eb7b7672a3b2ff598883f8c60ebb557a7185",
    >           "lastState": {},
    >           "name": "manager",
    >           "ready": true,
    >           "restartCount": 0,
    >           "started": true,
    >           "state": {
    >             "running": {
    >               "startedAt": "2024-01-08T19:17:06Z"
    >             }
    >           }
    >         }
    >       ],
    >       "hostIP": "172.18.0.2",
    >       "phase": "Running",
    >       "podIP": "10.244.0.8",
    >       "podIPs": [
    >         {
    >           "ip": "10.244.0.8"
    >         }
    >       ],
    >       "qosClass": "BestEffort",
    >       "startTime": "2024-01-08T19:16:43Z"
    >     }
    >   }




















