#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: []
#

    Target:

        Add disc space to the worker nodes.
        Modifed the Helm chart to add 100G Cinder volume to each woker node.

    Result:

        Stuck:
            Issue with stuck Cinder volumes blocking the creation of worker nodes.
            Cinder volumes would be stuck at 'creating' for 5-10min and then get deleted.
            Cycle repeats with no clear error messages in the ClusterAPI Pod logs.

        Good:
            Issue seems to be fixed.
            Created a cluster of worker nodes with 100G Cinder boot discs.
            ....


# -----------------------------------------------------
# Edit our cluster config to add disc space.
#[user@desktop]

    source "${HOME}/aglais.env"

    pushd "${AGLAIS_CODE:?}"
        pushd deployments/cluster-api/bootstrap
            pushd ansible/templates

                gedit clusterapi-config.j2 &

            popd
        popd
    popd

    >   ....
    >   ....
    >   # The worker node groups for the cluster
    >   nodeGroups:
    >     - # The name of the node group
    >       name: md-0
    >       ....
    >       ....
    >       # The root volume spec for machines in the node group
    >       machineRootVolume:
    >         # The size of the disk to use
    >         # If not given, the ephemeral root disk from the flavor is used
    >         diskSize: 100


# -----------------------------------------------------
# -----------------------------------------------------
# Delete and deploy everything.
#[root@ansibler]

    >   ....
    >   ....
    >   fatal: [bootstrap]: FAILED! => {
    >       "changed": false,
    >       "command": "
    >           /usr/local/bin/helm \
    >               --version=0.1.0 \
    >               upgrade \
    >                   -i \
    >                   --reset-values \
    >                   --wait \
    >                   --values=/opt/aglais/clusterapi-config.yml \
    >                   --values=/opt/aglais/openstack-clouds.yml \
    >                   iris-gaia-blue-20230822-work \
    >                   capi/openstack-cluster
    >           ",
    >       "msg": "
    >           Failure when executing Helm command. Exited 1.
    >           stdout: Release \"iris-gaia-blue-20230822-work\" does not exist. Installing it now.
    >           stderr: Error: context deadline exceeded
    >           ",
    >       "stderr": "Error: context deadline exceeded",
    >       "stderr_lines": [
    >           "Error: context deadline exceeded"
    >           ],
    >       "stdout": "Release \"iris-gaia-blue-20230822-work\" does not exist. Installing it now.",
    >       "stdout_lines": [
    >           "Release \"iris-gaia-blue-20230822-work\" does not exist. Installing it now."
    >           ]
    >       }


# -----------------------------------------------------
# -----------------------------------------------------
# Edit our cluster config to remove the extra discs.
#[user@desktop]

    source "${HOME}/aglais.env"

    pushd "${AGLAIS_CODE:?}"
        pushd deployments/cluster-api/bootstrap
            pushd ansible/templates

                gedit clusterapi-config.j2 &

            popd
        popd
    popd

    >   ....
    >   ....
    >   # The worker node groups for the cluster
    >   nodeGroups:
    >     - # The name of the node group
    >       name: md-0
    >       ....
    >       ....
    >       # The root volume spec for machines in the node group
    >       machineRootVolume:
    >         # The size of the disk to use
    >         # If not given, the ephemeral root disk from the flavor is used
    >         # diskSize: 100


# -----------------------------------------------------
# -----------------------------------------------------
# Delete and deploy everything.
#[root@ansibler]

    >   ....
    >   ....
    >   Deleting volumes
    >   - Deleting volume [5b694e39-358e-4bd4-8632-d80c2156222b]
    >   Failed to delete volume with name or ID '5b694e39-358e-4bd4-8632-d80c2156222b':
    >       Invalid volume:
    >           Volume status must be available or error or error_restoring or error_extending or error_managing
    >           and must not be migrating, attached, belong to a group, have snapshots or be disassociated from
    >           snapshots after volume transfer. (HTTP 400) (Request-ID: req-1dd097d8-7532-432e-8249-1422b460c721)
    >   1 of 1 volumes failed to delete.
    >   - Deleting volume [3bb32a5c-5118-4629-a27f-2581bc26a9c3]
    >   Failed to delete volume with name or ID '3bb32a5c-5118-4629-a27f-2581bc26a9c3':
    >       Invalid volume:
    >           Volume status must be available or error or error_restoring or error_extending or error_managing
    >           and must not be migrating, attached, belong to a group, have snapshots or be disassociated from
    >           snapshots after volume transfer. (HTTP 400) (Request-ID: req-205b9aff-7e26-48ea-aed2-4425f3316694)
    >   1 of 1 volumes failed to delete.
    >   - Deleting volume [7e95e352-26d1-4242-9feb-9821cebd96e2]
    >   Failed to delete volume with name or ID '7e95e352-26d1-4242-9feb-9821cebd96e2':
    >       Invalid volume:
    >           Volume status must be available or error or error_restoring or error_extending or error_managing
    >           and must not be migrating, attached, belong to a group, have snapshots or be disassociated from
    >           snapshots after volume transfer. (HTTP 400) (Request-ID: req-120bd961-245e-4df8-9227-3d1f4372e469)
    >   1 of 1 volumes failed to delete.
    >   ....
    >   ....

    >   ....
    >   ....
    >   List volumes
    >   +--------------------------------------+-------------------------------------------------------+----------+------+-------------+
    >   | ID                                   | Name                                                  | Status   | Size | Attached to |
    >   +--------------------------------------+-------------------------------------------------------+----------+------+-------------+
    >   | 5b694e39-358e-4bd4-8632-d80c2156222b | iris-gaia-blue-20230822-work-md-0-f9de9d2c-jxq9w-root | creating |  100 |             |
    >   | 3bb32a5c-5118-4629-a27f-2581bc26a9c3 | iris-gaia-blue-20230822-work-md-0-f9de9d2c-gqd6v-root | creating |  100 |             |
    >   | 7e95e352-26d1-4242-9feb-9821cebd96e2 | iris-gaia-blue-20230822-work-md-0-f9de9d2c-hklt5-root | creating |  100 |             |
    >   +--------------------------------------+-------------------------------------------------------+----------+------+-------------+
    >   ....
    >   ....

    >   ....
    >   ....
    >   TASK [Create work cluster [iris-gaia-blue-20230823-work]] ***********************************************************
    >   fatal: [bootstrap]: FAILED! => {
    >       "changed": false,
    >       "command": "
    >           /usr/local/bin/helm \
    >               --version=0.1.0 \
    >               upgrade \
    >                   -i \
    >                   --reset-values \
    >                   --wait \
    >                   --values=/opt/aglais/clusterapi-config.yml \
    >                   --values=/opt/aglais/openstack-clouds.yml \
    >                   iris-gaia-blue-20230823-work \
    >                   capi/openstack-cluster
    >           ",
    >       "msg": "
    >           Failure when executing Helm command. Exited 1.
    >           stdout: Release \"iris-gaia-blue-20230823-work\" does not exist. Installing it now.
    >           stderr: Error: context deadline exceeded
    >           ",
    >       "stderr": "Error: context deadline exceeded",
    >       "stderr_lines": [
    >           "Error: context deadline exceeded"
    >           ],
    >       "stdout": "Release \"iris-gaia-blue-20230823-work\" does not exist. Installing it now.",
    >       "stdout_lines": [
    >           "Release \"iris-gaia-blue-20230823-work\" does not exist. Installing it now."
    >           ]
    >       }


# -----------------------------------------------------
# List everything.
#[root@ansibler]

    /deployments/openstack/bin/list-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....
    >   Nova servers
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | ID                                   | Name                                                      | Status | Networks                                                                  | Image                             | Flavor               |
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | 4ecc1286-7dca-4779-a2f2-a5085666b3b1 | iris-gaia-blue-20230823-work-control-plane-e7d7a7c7-nmwnr | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.142 | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 1700bd44-9964-447a-8f94-e3f08cc778fd | iris-gaia-blue-20230823-work-control-plane-e7d7a7c7-bxr2q | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.94  | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 1905edca-807d-4cef-8775-a735e12e8357 | iris-gaia-blue-20230823-work-control-plane-e7d7a7c7-vhmdd | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.52  | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 0eede3cb-c68c-41e6-8034-16f6a6e9af6f | iris-gaia-blue-20230823-bootstrap-node                    | ACTIVE | iris-gaia-blue-20230823-bootstrap-network=10.10.2.86, 128.232.226.244     | Fedora-34.1.2                     | gaia.vm.cclake.2vcpu |
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >
    >   ---- ----
    >   Cinder volumes
    >   +--------------------------------------+-------------------------------------------------------+-----------+------+-------------+
    >   | ID                                   | Name                                                  | Status    | Size | Attached to |
    >   +--------------------------------------+-------------------------------------------------------+-----------+------+-------------+
    >   | d167e53b-57a6-40a2-8343-066047fe5a7a | iris-gaia-blue-20230823-work-md-0-7df2dbba-4ngxm-root | creating  |  100 |             |
    >   | 1802c902-28e6-4024-a3ed-bf467e6b4e8c | iris-gaia-blue-20230823-work-md-0-7df2dbba-4djz8-root | creating  |  100 |             |
    >   | 5517d894-88f2-401a-918e-68106424bfac | iris-gaia-blue-20230823-work-md-0-7df2dbba-dbtw9-root | creating  |  100 |             |
    >   | 5b694e39-358e-4bd4-8632-d80c2156222b | iris-gaia-blue-20230822-work-md-0-f9de9d2c-jxq9w-root | available |  100 |             |
    >   | 3bb32a5c-5118-4629-a27f-2581bc26a9c3 | iris-gaia-blue-20230822-work-md-0-f9de9d2c-gqd6v-root | available |  100 |             |
    >   | 7e95e352-26d1-4242-9feb-9821cebd96e2 | iris-gaia-blue-20230822-work-md-0-f9de9d2c-hklt5-root | available |  100 |             |
    >   +--------------------------------------+-------------------------------------------------------+-----------+------+-------------+
    >   ....
    >   ....

    #
    # Looks like the create worked ... just took longer than the timeout.
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        volume show \
            'd167e53b-57a6-40a2-8343-066047fe5a7a'

    >   +------------------------------+------------------------------------------------------------------+
    >   | Field                        | Value                                                            |
    >   +------------------------------+------------------------------------------------------------------+
    >   | attachments                  | []                                                               |
    >   | availability_zone            | nova                                                             |
    >   | bootable                     | false                                                            |
    >   | consistencygroup_id          | None                                                             |
    >   | created_at                   | 2023-08-23T00:35:46.000000                                       |
    >   | description                  | Root volume for iris-gaia-blue-20230823-work-md-0-7df2dbba-4ngxm |
    >   | encrypted                    | False                                                            |
    >   | id                           | d167e53b-57a6-40a2-8343-066047fe5a7a                             |
    >   | multiattach                  | False                                                            |
    >   | name                         | iris-gaia-blue-20230823-work-md-0-7df2dbba-4ngxm-root            |
    >   | os-vol-tenant-attr:tenant_id | e918a13fed2648758175a15fac083569                                 |
    >   | properties                   |                                                                  |
    >   | replication_status           | None                                                             |
    >   | size                         | 100                                                              |
    >   | snapshot_id                  | None                                                             |
    >   | source_volid                 | None                                                             |
    >   | status                       | creating                                                         |
    >   | type                         | arcus-ceph01-rbd                                                 |
    >   | updated_at                   | 2023-08-23T00:35:47.000000                                       |
    >   | user_id                      | 5fa0c97a6dd14e01a3c7d91dad5c6b17                                 |
    >   +------------------------------+------------------------------------------------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        volume show \
            '5b694e39-358e-4bd4-8632-d80c2156222b'

    >   +------------------------------+---------------------------------------------------------------------------------------+
    >   | Field                        | Value                                                                                 |
    >   +------------------------------+---------------------------------------------------------------------------------------+
    >   | attachments                  | []                                                                                    |
    >   | availability_zone            | nova                                                                                  |
    >   | bootable                     | true                                                                                  |
    >   | consistencygroup_id          | None                                                                                  |
    >   | created_at                   | 2023-08-23T00:04:11.000000                                                            |
    >   | description                  | Root volume for iris-gaia-blue-20230822-work-md-0-f9de9d2c-jxq9w                      |
    >   | encrypted                    | False                                                                                 |
    >   | id                           | 5b694e39-358e-4bd4-8632-d80c2156222b                                                  |
    >   | multiattach                  | False                                                                                 |
    >   | name                         | iris-gaia-blue-20230822-work-md-0-f9de9d2c-jxq9w-root                                 |
    >   | os-vol-tenant-attr:tenant_id | e918a13fed2648758175a15fac083569                                                      |
    >   | properties                   |                                                                                       |
    >   | replication_status           | None                                                                                  |
    >   | size                         | 100                                                                                   |
    >   | snapshot_id                  | None                                                                                  |
    >   | source_volid                 | None                                                                                  |
    >   | status                       | available                                                                             |
    >   | type                         | arcus-ceph01-rbd                                                                      |
    >   | updated_at                   | 2023-08-23T00:33:12.000000                                                            |
    >   | user_id                      | 5fa0c97a6dd14e01a3c7d91dad5c6b17                                                      |
    >   | volume_image_metadata        | {                                                                                     |
    >   |                              |  'signature_verified': 'False',                                                       |
    >   |                              |  'owner_specified.openstack.md5': '',                                                 |
    >   |                              |  'owner_specified.openstack.object': 'images/gaia-dmp-ubuntu-2004-kube-v1.25.4',      |
    >   |                              |  'owner_specified.openstack.sha256': '',                                              |
    >   |                              |  'image_id': '6ac13e0f-fee8-4cfc-9b88-fe94b3237f9a',                                  |
    >   |                              |  'image_name': 'gaia-dmp-ubuntu-2004-kube-v1.25.4',                                   |
    >   |                              |  'checksum': '225a4fec21b30be3fbb4121554baa8f4',                                      |
    >   |                              |  'container_format': 'bare',                                                          |
    >   |                              |  'disk_format': 'qcow2',                                                              |
    >   |                              |  'min_disk': '0',                                                                     |
    >   |                              |  'min_ram': '0',                                                                      |
    >   |                              |  'size': '4441047040'                                                                 |
    >   |                              |  }                                                                                    |
    >   +------------------------------+---------------------------------------------------------------------------------------+

    #
    # See if we actually got a functioning cluster.
    #

# -----------------------------------------------------
# -----------------------------------------------------
# Check the cluster status.
#[user@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get OpenStackClusters

    >   NAME                           CLUSTER                        READY   NETWORK                                SUBNET                                 BASTION IP   AGE
    >   iris-gaia-blue-20230823-work   iris-gaia-blue-20230823-work   true    dd602967-3fa8-4a63-9225-7f6a526bb98c   a2896739-145f-4ec8-b002-09108aae0544                25m


    clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        describe cluster \
            "${workclustername:?}"

    >   NAME                                                                             READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/iris-gaia-blue-20230823-work                                             True                                          15m
    >   ├─ClusterInfrastructure - OpenStackCluster/iris-gaia-blue-20230823-work
    >   ├─ControlPlane - KubeadmControlPlane/iris-gaia-blue-20230823-work-control-plane  True                                          15m
    >   │  └─3 Machines...                                                                True                                          20m    See iris-gaia-blue-20230823-work-control-plane-g5nmv, iris-gaia-blue-20230823-work-control-plane-ljg76, ...
    >   └─Workers
    >     └─MachineDeployment/iris-gaia-blue-20230823-work-md-0                          False  Warning   WaitingForAvailableMachines  25m    Minimum availability requires 2 replicas, current 0 available
    >       └─3 Machines...                                                              False  Error     InstanceDeleteFailed         8m45s  See iris-gaia-blue-20230823-work-md-0-964459f77xc6qsf-j7npp, iris-gaia-blue-20230823-work-md-0-964459f77xc6qsf-p9hxj, ...

    #
    # Very boring.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Try again.
#[root@ansibler]

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   List servers
    >
    >   ---- ----
    >   List volumes
    >
    >   ....

    #
    # Create all, with no extra discs.
    # To check that it is the discs causing the problem ...
    #

    ansible-playbook \
        --inventory 'bootstrap,' \
        '/deployments/cluster-api/bootstrap/ansible/00-create-all.yml'

    >   ....
    >   TASK [Create work cluster [iris-gaia-blue-20230823-work]] *****************************
    >   fatal: [bootstrap]: FAILED! => {
    >       "changed": false,
    >       "command": "
    >           /usr/local/bin/helm \
    >               --version=0.1.0 \
    >               upgrade \
    >                   -i \
    >                   --reset-values \
    >                   --wait \
    >                   --values=/opt/aglais/clusterapi-config.yml \
    >                   --values=/opt/aglais/openstack-clouds.yml \
    >                   iris-gaia-blue-20230823-work \
    >                   capi/openstack-cluster
    >           ",
    >       "msg": "
    >           Failure when executing Helm command. Exited 1.
    >           stdout: Release \"iris-gaia-blue-20230823-work\" does not exist. Installing it now.
    >           stderr: Error: context deadline exceeded
    >           ",
    >       "stderr": "Error: context deadline exceeded",
    >       "stderr_lines": [
    >           "Error: context deadline exceeded"
    >           ],
    >       "stdout": "Release \"iris-gaia-blue-20230823-work\" does not exist. Installing it now.",
    >       "stdout_lines": [
    >           "Release \"iris-gaia-blue-20230823-work\" does not exist. Installing it now."
    >           ]
    >       }
    >   ....


    /deployments/openstack/bin/list-all.sh \
        "${cloudname:?}"

    >   ....
    >   Nova servers
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | ID                                   | Name                                                      | Status | Networks                                                                  | Image                             | Flavor               |
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | 3368ea6f-a645-44b9-9848-dd206ca75b40 | iris-gaia-blue-20230823-work-md-0-e7d7a7c7-9wxsj          | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.117 | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 80274bb7-27b1-4da8-9d12-4bbbe31678e4 | iris-gaia-blue-20230823-work-md-0-e7d7a7c7-4l6gw          | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.162 | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 299fd52b-63bd-4154-83fa-a285fa105593 | iris-gaia-blue-20230823-work-md-0-e7d7a7c7-klnml          | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.26  | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 3d6bbcf2-4165-4579-8564-d3603afe6ff1 | iris-gaia-blue-20230823-work-control-plane-e7d7a7c7-9s4q8 | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.21  | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 22ee4dda-73f7-4f2a-9578-3ada7bc56f98 | iris-gaia-blue-20230823-bootstrap-node                    | ACTIVE | iris-gaia-blue-20230823-bootstrap-network=10.10.2.40, 128.232.227.9       | Fedora-34.1.2                     | gaia.vm.cclake.2vcpu |
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >
    >   ---- ----
    >   Cinder volumes
    >
    >
    >   ---- ----
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------
# Check the cluster status.
#[user@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get OpenStackClusters

    >   NAME                           CLUSTER                        READY   NETWORK                                SUBNET                                 BASTION IP   AGE
    >   iris-gaia-blue-20230823-work   iris-gaia-blue-20230823-work   true    e4213304-870d-4ca8-8c35-c53a1cab7edd   405ea08e-6ce4-4205-8352-1fae9ff96626                9m31s


    clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        describe cluster \
            "${workclustername:?}"

    >   NAME                                                                             READY  SEVERITY  REASON                   SINCE  MESSAGE
    >   Cluster/iris-gaia-blue-20230823-work                                             False  Warning   ScalingUp                2m17s  Scaling up control plane to 3 replicas (actual 2)
    >   ├─ClusterInfrastructure - OpenStackCluster/iris-gaia-blue-20230823-work
    >   ├─ControlPlane - KubeadmControlPlane/iris-gaia-blue-20230823-work-control-plane  False  Warning   ScalingUp                2m17s  Scaling up control plane to 3 replicas (actual 2)
    >   │ ├─2 Machines...                                                                True                                      4m39s  See iris-gaia-blue-20230823-work-control-plane-smwfz, iris-gaia-blue-20230823-work-control-plane-zs4lb
    >   │ └─Machine/iris-gaia-blue-20230823-work-control-plane-q5v8h                     False  Info      WaitingForBootstrapData  21s    1 of 2 completed
    >   └─Workers
    >     └─MachineDeployment/iris-gaia-blue-20230823-work-md-0                          True                                      81s
    >       └─3 Machines...                                                              True                                      2m22s  See iris-gaia-blue-20230823-work-md-0-7bcd78d668xh8q8b-5jqsf, iris-gaia-blue-20230823-work-md-0-7bcd78d668xh8q8b-q2vr8, ...

    >   NAME                                                                             READY  SEVERITY  REASON  SINCE  MESSAGE
    >   Cluster/iris-gaia-blue-20230823-work                                             True                     34s
    >   ├─ClusterInfrastructure - OpenStackCluster/iris-gaia-blue-20230823-work
    >   ├─ControlPlane - KubeadmControlPlane/iris-gaia-blue-20230823-work-control-plane  True                     34s
    >   │ └─3 Machines...                                                                True                     2m18s  See iris-gaia-blue-20230823-work-control-plane-q5v8h, iris-gaia-blue-20230823-work-control-plane-smwfz, ...
    >   └─Workers
    >     └─MachineDeployment/iris-gaia-blue-20230823-work-md-0                          True                     2m31s
    >       └─3 Machines...                                                              True                     3m32s  See iris-gaia-blue-20230823-work-md-0-7bcd78d668xh8q8b-5jqsf, iris-gaia-blue-20230823-work-md-0-7bcd78d668xh8q8b-q2vr8, ...

    #
    # OK, so we have a healthy cluster ..
    # Even though the create-all failed with 'context deadline exceeded'.
    #

# -----------------------------------------------------
# -----------------------------------------------------
# Try again.
#[root@ansibler]

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   List servers
    >
    >   ---- ----
    >   List volumes
    >
    >   ....

    #
    # Create all, with no extra discs.
    # To check that it is the discs causing the problem ...
    # This should NOT fail.
    #

    ansible-playbook \
        --inventory 'bootstrap,' \
        '/deployments/cluster-api/bootstrap/ansible/00-create-all.yml'

    >   ....
    >   ....
    >   TASK [Create work cluster [iris-gaia-blue-20230823-work]] ************************************************************************************************************************************
    >   fatal: [bootstrap]: FAILED! => {
    >       "changed": false,
    >       "command": "
    >           /usr/local/bin/helm \
    >               --version=0.1.0 \
    >               upgrade \
    >                   -i \
    >                   --reset-values \
    >                   --wait \
    >                   --values=/opt/aglais/clusterapi-config.yml \
    >                   --values=/opt/aglais/openstack-clouds.yml \
    >                   iris-gaia-blue-20230823-work capi/openstack-cluster
    >           ",
    >       "msg": "
    >           Failure when executing Helm command. Exited 1.
    >           stdout: Release \"iris-gaia-blue-20230823-work\" does not exist. Installing it now.
    >           stderr: Error: context deadline exceeded
    >           ",
    >       "stderr": "Error: context deadline exceeded",
    >       "stderr_lines": [
    >           "Error: context deadline exceeded"
    >           ],
    >       "stdout": "Release \"iris-gaia-blue-20230823-work\" does not exist. Installing it now.",
    >       "stdout_lines": [
    >           "Release \"iris-gaia-blue-20230823-work\" does not exist. Installing it now."
    >           ]
    >       }


# -----------------------------------------------------
# -----------------------------------------------------
# Check the cluster status.
#[user@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get OpenStackClusters

    >   NAME                           CLUSTER                        READY   NETWORK                                SUBNET                                 BASTION IP   AGE
    >   iris-gaia-blue-20230823-work   iris-gaia-blue-20230823-work   true    2c90ddcc-439a-402c-b7ce-ac12e850281c   dba4a264-21cb-40bb-aa98-b960dcdde849                12m


    clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        describe cluster \
            "${workclustername:?}"

    >   NAME                                                                             READY  SEVERITY  REASON  SINCE  MESSAGE
    >   Cluster/iris-gaia-blue-20230823-work                                             True                     3m8s
    >   ├─ClusterInfrastructure - OpenStackCluster/iris-gaia-blue-20230823-work
    >   ├─ControlPlane - KubeadmControlPlane/iris-gaia-blue-20230823-work-control-plane  True                     3m8s
    >   │ └─3 Machines...                                                                True                     4m56s  See iris-gaia-blue-20230823-work-control-plane-jxtq5, iris-gaia-blue-20230823-work-control-plane-rnrcl, ...
    >   └─Workers
    >     └─MachineDeployment/iris-gaia-blue-20230823-work-md-0                          True                     5m13s
    >       └─3 Machines...                                                              True                     6m13s  See iris-gaia-blue-20230823-work-md-0-7bcd78d668x996qj-dddkw, iris-gaia-blue-20230823-work-md-0-7bcd78d668x996qj-tj9z5, ...

    #
    # The Helm chart fails with 'context deadline exceeded',
    # but the resulting cluster seems healthy.
    # So the timeout errors were NOT caused by the extra disc space.
    #


# -----------------------------------------------------
# Edit our cluster config to add disc space.
#[user@desktop]

    source "${HOME}/aglais.env"

    pushd "${AGLAIS_CODE:?}"
        pushd deployments/cluster-api/bootstrap
            pushd ansible/templates

                gedit clusterapi-config.j2 &

            popd
        popd
    popd

    >   ....
    >   ....
    >   # The worker node groups for the cluster
    >   nodeGroups:
    >     - # The name of the node group
    >       name: md-0
    >       ....
    >       ....
    >       # The root volume spec for machines in the node group
    >       machineRootVolume:
    >         # The size of the disk to use
    >         # If not given, the ephemeral root disk from the flavor is used
    >         diskSize: 100


# -----------------------------------------------------
# -----------------------------------------------------
# Delete and deploy everything.
#[root@ansibler]

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


    ansible-playbook \
        --inventory 'bootstrap,' \
        '/deployments/cluster-api/bootstrap/ansible/00-create-all.yml'

    >   ....
    >   ....
    >   TASK [Create work cluster [iris-gaia-blue-20230823-work]] ************************************************************************************************************************************
    >   fatal: [bootstrap]: FAILED! => {
    >       "changed": false,
    >       "command": "
    >           /usr/local/bin/helm \
    >               --version=0.1.0 \
    >               upgrade \
    >                   -i \
    >                   --reset-values \
    >                   --wait \
    >                   --values=/opt/aglais/clusterapi-config.yml \
    >                   --values=/opt/aglais/openstack-clouds.yml \
    >                   iris-gaia-blue-20230823-work \
    >                   capi/openstack-cluster
    >           ",
    >       "msg": "
    >           Failure when executing Helm command. Exited 1.
    >           stdout: Release \"iris-gaia-blue-20230823-work\" does not exist. Installing it now.
    >           stderr: Error: context deadline exceeded
    >           ",
    >       "stderr": "Error: context deadline exceeded",
    >       "stderr_lines": [
    >           "Error: context deadline exceeded"
    >           ],
    >       "stdout": "Release \"iris-gaia-blue-20230823-work\" does not exist. Installing it now.",
    >       "stdout_lines": [
    >           "Release \"iris-gaia-blue-20230823-work\" does not exist. Installing it now."
    >           ]
    >       }


# -----------------------------------------------------
# -----------------------------------------------------
# Check the cluster status.
#[user@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get OpenStackClusters

    >   NAME                           CLUSTER                        READY   NETWORK                                SUBNET                                 BASTION IP   AGE
    >   iris-gaia-blue-20230823-work   iris-gaia-blue-20230823-work   true    13fe0808-05fa-4602-bcdf-4d4b1316f251   493eda49-8c54-495b-a277-04ee99a5f138                8m55s


    clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        describe cluster \
            "${workclustername:?}"

    >   NAME                                                                             READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/iris-gaia-blue-20230823-work                                             False  Warning   ScalingUp                    2m33s  Scaling up control plane to 3 replicas (actual 2)
    >   ├─ClusterInfrastructure - OpenStackCluster/iris-gaia-blue-20230823-work
    >   ├─ControlPlane - KubeadmControlPlane/iris-gaia-blue-20230823-work-control-plane  False  Warning   ScalingUp                    2m33s  Scaling up control plane to 3 replicas (actual 2)
    >   │ ├─2 Machines...                                                                True                                          4m44s  See iris-gaia-blue-20230823-work-control-plane-5ppkz, iris-gaia-blue-20230823-work-control-plane-kc6sw
    >   │ └─Machine/iris-gaia-blue-20230823-work-control-plane-ff44f                     False  Info      WaitingForBootstrapData      54s    1 of 2 completed
    >   └─Workers
    >     └─MachineDeployment/iris-gaia-blue-20230823-work-md-0                          False  Warning   WaitingForAvailableMachines  10m    Minimum availability requires 2 replicas, current 0 available
    >       └─3 Machines...                                                              False  Info      WaitingForBootstrapData      3m22s  See iris-gaia-blue-20230823-work-md-0-964459f77xblkc5-54rrd, iris-gaia-blue-20230823-work-md-0-964459f77xblkc5-nf2cr, ...


    >   NAME                                                                             READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/iris-gaia-blue-20230823-work                                             True                                          119s
    >   ├─ClusterInfrastructure - OpenStackCluster/iris-gaia-blue-20230823-work
    >   ├─ControlPlane - KubeadmControlPlane/iris-gaia-blue-20230823-work-control-plane  True                                          119s
    >   │ └─3 Machines...                                                                True                                          6m46s  See iris-gaia-blue-20230823-work-control-plane-5ppkz, iris-gaia-blue-20230823-work-control-plane-ff44f, ...
    >   └─Workers
    >     └─MachineDeployment/iris-gaia-blue-20230823-work-md-0                          False  Warning   WaitingForAvailableMachines  12m    Minimum availability requires 2 replicas, current 0 available
    >       └─3 Machines...                                                              False  Error     InstanceCreateFailed         1s     See iris-gaia-blue-20230823-work-md-0-964459f77xblkc5-54rrd, iris-gaia-blue-20230823-work-md-0-964459f77xblkc5-nf2cr, ...

    #
    # Very very boring.
    #

# -----------------------------------------------------
# Check the addon-provider logs.
#[root@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get pods

    >   NAME                                                       READY   STATUS    RESTARTS   AGE
    >   cluster-api-addon-provider-66cc76bbbf-z2896                1/1     Running   0          19m
    >   iris-gaia-blue-20230823-work-autoscaler-7897bbf6f7-vqvdl   1/1     Running   0          18m


    podname=$(
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            get pods \
                --output json \
        | jq -r '.items[].metadata.name | select(test("cluster-api-addon-provider")) '
        )

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        logs "${podname:?}" \
    | tee /tmp/"${podname:?}".log

    >   ....
    >   ....

    #
    # No real error messaages, just lots of "node not ready".
    #

# -----------------------------------------------------
# Check the controller-manager logs.
#[root@bootstrap]

    podname=$(
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            get pods \
                --output json \
                --namespace capi-kubeadm-control-plane-system \
        | jq -r '.items[0] | .metadata.name'
        )

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        logs "${podname:?}" \
            --namespace capi-kubeadm-control-plane-system \
    | tee /tmp/"${podname:?}".log

    >   ....
    >   ....

    #
    # Loads and loads of SPAM and no real information.
    #


# -----------------------------------------------------
# Check the cluster status.
#[user@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get OpenStackClusters

    >   NAME                           CLUSTER                        READY   NETWORK                                SUBNET                                 BASTION IP   AGE
    >   iris-gaia-blue-20230823-work   iris-gaia-blue-20230823-work   true    13fe0808-05fa-4602-bcdf-4d4b1316f251   493eda49-8c54-495b-a277-04ee99a5f138                35m


    clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        describe cluster \
            "${workclustername:?}"

    >   NAME                                                                             READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/iris-gaia-blue-20230823-work                                             True                                          25m
    >   ├─ClusterInfrastructure - OpenStackCluster/iris-gaia-blue-20230823-work
    >   ├─ControlPlane - KubeadmControlPlane/iris-gaia-blue-20230823-work-control-plane  True                                          25m
    >   │ └─3 Machines...                                                                True                                          29m    See iris-gaia-blue-20230823-work-control-plane-5ppkz, iris-gaia-blue-20230823-work-control-plane-ff44f, ...
    >   └─Workers
    >     └─MachineDeployment/iris-gaia-blue-20230823-work-md-0                          False  Warning   WaitingForAvailableMachines  35m    Minimum availability requires 2 replicas, current 0 available
    >       └─3 Machines...                                                              False  Error     InstanceDeleteFailed         17m    See iris-gaia-blue-20230823-work-md-0-964459f77xblkc5-54rrd, iris-gaia-blue-20230823-work-md-0-964459f77xblkc5-nf2cr, ...

    #
    # Just CRAP.
    # A waste of time and energy.
    #


# -----------------------------------------------------
# Check what we have in Openstack.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        server list

    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | ID                                   | Name                                                      | Status | Networks                                                                  | Image                             | Flavor               |
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | 190cba9d-5763-4a07-822f-7dac0ba9c23f | iris-gaia-blue-20230823-work-control-plane-e7d7a7c7-llhwp | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.232 | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | d2a7e2e3-27fd-4c51-8a1e-c61c6b60a537 | iris-gaia-blue-20230823-work-control-plane-e7d7a7c7-r46cs | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.33  | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 65d52cc4-ddad-4512-b4e8-d1fb61ae4f37 | iris-gaia-blue-20230823-work-control-plane-e7d7a7c7-f577h | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.193 | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 3ac64e89-1d92-4f6a-b849-68ce62cf4d8d | iris-gaia-blue-20230823-bootstrap-node                    | ACTIVE | iris-gaia-blue-20230823-bootstrap-network=10.10.1.34, 128.232.226.135     | Fedora-34.1.2                     | gaia.vm.cclake.2vcpu |
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+

    #
    # We have full set of control plane nodes, but no worker nodes.
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        volume list

    >   +--------------------------------------+-------------------------------------------------------+----------+------+-------------+
    >   | ID                                   | Name                                                  | Status   | Size | Attached to |
    >   +--------------------------------------+-------------------------------------------------------+----------+------+-------------+
    >   | 4427d08d-66b8-4094-9102-cf712b90869e | iris-gaia-blue-20230823-work-md-0-7df2dbba-gq8pm-root | creating |  100 |             |
    >   | 539c8ca2-cedd-46d0-9897-d0b9dabc2481 | iris-gaia-blue-20230823-work-md-0-7df2dbba-66btj-root | creating |  100 |             |
    >   | 7c0e6090-7c42-4a7c-9bc9-11a444ecfd42 | iris-gaia-blue-20230823-work-md-0-7df2dbba-7rb44-root | creating |  100 |             |
    >   +--------------------------------------+-------------------------------------------------------+----------+------+-------------+

    #
    # Three volumes, stuck in 'creating'.
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        volume show \
            '4427d08d-66b8-4094-9102-cf712b90869e'

    >   +------------------------------+------------------------------------------------------------------+
    >   | Field                        | Value                                                            |
    >   +------------------------------+------------------------------------------------------------------+
    >   | attachments                  | []                                                               |
    >   | availability_zone            | nova                                                             |
    >   | bootable                     | false                                                            |
    >   | consistencygroup_id          | None                                                             |
    >   | created_at                   | 2023-08-23T03:26:53.000000                                       |
    >   | description                  | Root volume for iris-gaia-blue-20230823-work-md-0-7df2dbba-gq8pm |
    >   | encrypted                    | False                                                            |
    >   | id                           | 4427d08d-66b8-4094-9102-cf712b90869e                             |
    >   | multiattach                  | False                                                            |
    >   | name                         | iris-gaia-blue-20230823-work-md-0-7df2dbba-gq8pm-root            |
    >   | os-vol-tenant-attr:tenant_id | e918a13fed2648758175a15fac083569                                 |
    >   | properties                   |                                                                  |
    >   | replication_status           | None                                                             |
    >   | size                         | 100                                                              |
    >   | snapshot_id                  | None                                                             |
    >   | source_volid                 | None                                                             |
    >   | status                       | creating                                                         |
    >   | type                         | arcus-ceph01-rbd                                                 |
    >   | updated_at                   | 2023-08-23T03:26:55.000000                                       |
    >   | user_id                      | 5fa0c97a6dd14e01a3c7d91dad5c6b17                                 |
    >   +------------------------------+------------------------------------------------------------------+

    #
    # Text description says :
    # Root volume for iris-gaia-blue-20230823-work-md-0-7df2dbba-gq8pm
    #
    # .. but the VM doesn't exist yet.
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        server show \
            'iris-gaia-blue-20230823-work-md-0-7df2dbba-gq8pm'

    >   No server with a name or ID of 'iris-gaia-blue-20230823-work-md-0-7df2dbba-gq8pm' exists.

    #
    # Creating the server is blocked because the root disc hasn't been created yet.
    # Dumb shit K8s Helm chart just sits there waiting for stuff to be ready.
    # The disc creation hasn't failed, it is just stalled.
    # Helm deploy just sits there and waits.
    #

# -----------------------------------------------------
# -----------------------------------------------------

    #
    # Logged the issue with Cambridge HPC support.
    # [HPCSSUP-60234 Arcus cloud - Cinder root volumes stuck at 'creating']
    # Paul Browne resolved the stuck Cinder volumes, but couldn't reproduce it in their tests.
    # So it might happen again.
    #

# -----------------------------------------------------
# -----------------------------------------------------
# Delete and deploy everything.
#[root@ansibler]

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


    ansible-playbook \
        --inventory 'bootstrap,' \
        '/deployments/cluster-api/bootstrap/ansible/00-create-all.yml'

    >   ....
    >   ....
    >   PLAY RECAP **********************************************************************************************************
    >   bootstrap                  : ok=55   changed=45   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   localhost                  : ok=34   changed=26   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >


    /deployments/openstack/bin/list-all.sh \
        "${cloudname:?}"

    >   ....
    >   Nova servers
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | ID                                   | Name                                                      | Status | Networks                                                                  | Image                             | Flavor               |
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | 248bafa3-cb4e-4bcf-898f-fbd648d5cb7a | iris-gaia-blue-20230823-work-control-plane-e7d7a7c7-d2zsv | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.166 | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | da7984d2-14ab-4b82-8079-65d58c34786c | iris-gaia-blue-20230823-work-control-plane-e7d7a7c7-mwwb4 | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.235 | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 0aab585f-76e4-404d-ba64-8a80b65f7b55 | iris-gaia-blue-20230823-bootstrap-node                    | ACTIVE | iris-gaia-blue-20230823-bootstrap-network=10.10.1.213, 128.232.226.183    | Fedora-34.1.2                     | gaia.vm.cclake.2vcpu |
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >
    >   Cinder volumes
    >   +--------------------------------------+-------------------------------------------------------+----------+------+-------------+
    >   | ID                                   | Name                                                  | Status   | Size | Attached to |
    >   +--------------------------------------+-------------------------------------------------------+----------+------+-------------+
    >   | c6361699-2958-45db-88f0-fe024f9384f3 | iris-gaia-blue-20230823-work-md-0-7df2dbba-lw2x6-root | reserved |  100 |             |
    >   | a52cab54-e04c-436f-9560-570d2798f399 | iris-gaia-blue-20230823-work-md-0-7df2dbba-bc7w8-root | reserved |  100 |             |
    >   | 486e6836-1782-4183-af97-bdc91ab481c5 | iris-gaia-blue-20230823-work-md-0-7df2dbba-svhqr-root | reserved |  100 |             |
    >   +--------------------------------------+-------------------------------------------------------+----------+------+-------------+
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------
# Check the cluster status.
#[user@bootstrap]

    source loadconfig

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get OpenStackClusters

    >   NAME                           CLUSTER                        READY   NETWORK                                SUBNET                                 BASTION IP   AGE
    >   iris-gaia-blue-20230823-work   iris-gaia-blue-20230823-work   true    e7615c08-82ff-431a-83b0-2ea54f31048a   08712ea6-bf6c-40e4-aaef-e02a36ee1a06                6m20s


    clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        describe cluster \
            "${workclustername:?}"

    >   NAME                                                                             READY  SEVERITY  REASON  SINCE  MESSAGE
    >   Cluster/iris-gaia-blue-20230823-work                                             True                     34s
    >   ├─ClusterInfrastructure - OpenStackCluster/iris-gaia-blue-20230823-work
    >   ├─ControlPlane - KubeadmControlPlane/iris-gaia-blue-20230823-work-control-plane  True                     34s
    >   │ └─3 Machines...                                                                True                     4m57s  See iris-gaia-blue-20230823-work-control-plane-dg7p6, iris-gaia-blue-20230823-work-control-plane-k9ztg, ...
    >   └─Workers
    >     └─MachineDeployment/iris-gaia-blue-20230823-work-md-0                          True                     73s
    >       └─3 Machines...                                                              True                     2m     See iris-gaia-blue-20230823-work-md-0-96--END--
    >
    >
    >   # -----------------------------------------------------
    >   # -----------------------------------------------------
    >   # List our Openstack resources.
    >   #[root@ansibler]
    >
    >       /deployments/openstack/bin/list-all.sh \
    >           "${cloudname:?}"
    >
    >   ....
    >   Nova servers
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | ID                                   | Name                                                      | Status | Networks                                                                  | Image                             | Flavor               |
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | 9be2b9bd-1171-41a6-a024-ad5bc170619c | iris-gaia-blue-20230823-work-control-plane-e7d7a7c7-nljhl | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.236 | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 262eef8c-d13b-4686-b4d6-ef532d2e6596 | iris-gaia-blue-20230823-work-md-0-7df2dbba-lw2x6          | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.184 | N/A (booted from volume)          | gaia.vm.cclake.4vcpu |
    >   | 4494f60e-0482-4153-96a6-c5b4149e1979 | iris-gaia-blue-20230823-work-md-0-7df2dbba-bc7w8          | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.21  | N/A (booted from volume)          | gaia.vm.cclake.4vcpu |
    >   | ec8ef44e-94aa-46a5-af35-f52d7648baed | iris-gaia-blue-20230823-work-md-0-7df2dbba-svhqr          | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.33  | N/A (booted from volume)          | gaia.vm.cclake.4vcpu |
    >   | 248bafa3-cb4e-4bcf-898f-fbd648d5cb7a | iris-gaia-blue-20230823-work-control-plane-e7d7a7c7-d2zsv | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.166 | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | da7984d2-14ab-4b82-8079-65d58c34786c | iris-gaia-blue-20230823-work-control-plane-e7d7a7c7-mwwb4 | ACTIVE | k8s-clusterapi-cluster-default-iris-gaia-blue-20230823-work=192.168.3.235 | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 0aab585f-76e4-404d-ba64-8a80b65f7b55 | iris-gaia-blue-20230823-bootstrap-node                    | ACTIVE | iris-gaia-blue-20230823-bootstrap-network=10.10.1.213, 128.232.226.183    | Fedora-34.1.2                     | gaia.vm.cclake.2vcpu |
    >   +--------------------------------------+-----------------------------------------------------------+--------+---------------------------------------------------------------------------+-----------------------------------+----------------------+
    >
    >   Cinder volumes
    >   +--------------------------------------+-------------------------------------------------------+--------+------+---------------------------------------------------------------------------+
    >   | ID                                   | Name                                                  | Status | Size | Attached to                                                               |
    >   +--------------------------------------+-------------------------------------------------------+--------+------+---------------------------------------------------------------------------+
    >   | c6361699-2958-45db-88f0-fe024f9384f3 | iris-gaia-blue-20230823-work-md-0-7df2dbba-lw2x6-root | in-use |  100 | Attached to iris-gaia-blue-20230823-work-md-0-7df2dbba-lw2x6 on /dev/vda  |
    >   | a52cab54-e04c-436f-9560-570d2798f399 | iris-gaia-blue-20230823-work-md-0-7df2dbba-bc7w8-root | in-use |  100 | Attached to iris-gaia-blue-20230823-work-md-0-7df2dbba-bc7w8 on /dev/vda  |
    >   | 486e6836-1782-4183-af97-bdc91ab481c5 | iris-gaia-blue-20230823-work-md-0-7df2dbba-svhqr-root | in-use |  100 | Attached to iris-gaia-blue-20230823-work-md-0-7df2dbba-svhqr on /dev/vda  |
    >   +--------------------------------------+-------------------------------------------------------+--------+------+---------------------------------------------------------------------------+
    >   ....

    #
    # Issue with stuck Cinder volumes seems fixed :-)
    # Time to try it with Zeppelin deployment.
    #

