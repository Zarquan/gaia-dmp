#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: []
#


    Target:

        Figure out what enabling the Ingress controller actually does.

    Result:

        Work in progress ...

# -----------------------------------------------------
# Check which platform is live.
#[user@desktop]

    ssh fedora@live.gaia-dmp.uk \
        '
        date
        hostname
        '

    >   Fri 18 Aug 03:58:21 UTC 2023
    >   iris-gaia-green-20230308-zeppelin


# -----------------------------------------------------
# Edit client container script to publish port 8001.
#[user@desktop]

    gedit "${HOME}/bin/agclient" &

        ....
        ....

    +   kubectlproxy=8001:8001

        podman run \
            --rm \
            --tty \
            --interactive \
            --name     "${clientname:?}" \
            --hostname "${clientname:?}" \
    +       --publish  "${kubectlproxy:?}" \
            --env "cloudname=${cloudname:?}" \
            --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
            --volume "${SSH_AUTH_SOCK:?}:/mnt/ssh_auth_sock:rw,z" \
            --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
            --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
            "${containerfull:?}" \
            bash


# -----------------------------------------------------
# Create our client container.
#[user@desktop]

    agclient blue

    >   ....
    >   ....


# -----------------------------------------------------
# Delete and deploy everything.
#[root@ansibler]

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    ansible-playbook \
        --inventory 'bootstrap,' \
        '/deployments/cluster-api/bootstrap/ansible/00-create-all.yml'

    >   ....
    >   ....
    >   PLAY RECAP ***************************************************************************************************
    >   bootstrap                  : ok=55   changed=45   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   localhost                  : ok=32   changed=24   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


# -----------------------------------------------------
# -----------------------------------------------------
# Login to our bootstrap node.
#[user@desktop]

    podman exec \
        --tty \
        --interactive \
        ansibler-blue \
            ssh bootstrap

    >   Last login: Fri Aug 18 05:10:36 2023 from 90.155.51.57


# -----------------------------------------------------
# Load the cluster config.
#[user@bootstrap]

    source loadconfig

cat << EOF
kindclustername [${kindclustername}]
kindclusterconf [${kindclusterconf}]
workclustername [${workclustername}]
workclusterconf [${workclusterconf}]
EOF

    >   kindclustername [iris-gaia-blue-20230818-kind]
    >   kindclusterconf [/opt/aglais/iris-gaia-blue-20230818-kind.yml]
    >   workclustername [iris-gaia-blue-20230818-work]
    >   workclusterconf [/opt/aglais/iris-gaia-blue-20230818-work.yml]


# -----------------------------------------------------
# Check the cluster status.
#[user@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get OpenStackClusters

    >   NAME                           CLUSTER                        READY   NETWORK                                SUBNET                                 BASTION IP   AGE
    >   iris-gaia-blue-20230818-work   iris-gaia-blue-20230818-work   true    048d2726-2ac4-4a87-94bc-a02f29b6cfca   9d6849a5-6924-4dd2-a247-16406de7d7a0                10m


    clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        describe cluster \
            "${workclustername:?}"

    >   NAME                                                                             READY  SEVERITY  REASON  SINCE  MESSAGE
    >   Cluster/iris-gaia-blue-20230818-work                                             True                     5m26s
    >   ├─ClusterInfrastructure - OpenStackCluster/iris-gaia-blue-20230818-work
    >   ├─ControlPlane - KubeadmControlPlane/iris-gaia-blue-20230818-work-control-plane  True                     5m26s
    >   │ └─3 Machines...                                                                True                     8m36s  See iris-gaia-blue-20230818-work-control-plane-2hfpv, iris-gaia-blue-20230818-work-control-plane-2pq2p, ...
    >   └─Workers
    >     └─MachineDeployment/iris-gaia-blue-20230818-work-md-0                          True                     6m47s
    >       └─3 Machines...                                                              True                     7m27s  See iris-gaia-blue-20230818-work-md-0-59bccb97f9xbwcn8-9s27p, iris-gaia-blue-20230818-work-md-0-59bccb97f9xbwcn8-stffr, ...


# -----------------------------------------------------
# -----------------------------------------------------
# Run a kubectl proxy.
#[user@desktop]

    podman exec \
        --tty \
        --interactive \
        ansibler-blue \
            bash

    source /deployments/cluster-api/bootstrap/ansible/files/aglais/bin/loadconfig

    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        cluster-info

    >   Kubernetes control plane is running at https://128.232.227.77:6443
    >   CoreDNS is running at https://128.232.227.77:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy


    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --address 0.0.0.0 \
        proxy

    >   Starting to serve on [::]:8001


# -----------------------------------------------------
# -----------------------------------------------------
# Try access our dashboard via the proxy.
#[user@desktop]

    curl --head \
         --insecure \
         http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/

    >   HTTP/1.1 200 OK
    >   ....
    >   ....
    >   Content-Type: text/html; charset=utf-8
    >   Date: Fri, 18 Aug 2023 05:34:27 GMT
    >   Last-Modified: Fri, 12 Aug 2022 13:32:05 GMT


    firefox \
        --new-window \
        'http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/#/login' \
        &

    >   Dashboard login screen asking for a token.


# -----------------------------------------------------
# -----------------------------------------------------
# Create a service account and role binding for dashboard-admin.
# https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md#creating-a-service-account
# https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md#creating-a-clusterrolebinding
#[root@bootstrap]

    dashaccount=dashboard-admin
    dashnamespace=kube-system

    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --namespace "${dashnamespace:?}" \
        create serviceaccount \
            "${dashaccount:?}"

    >   serviceaccount/dashboard-admin created


    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --namespace "${dashnamespace:?}" \
        get serviceaccount \
            --output json \
            "${dashaccount:?}"

    >   {
    >       "apiVersion": "v1",
    >       "kind": "ServiceAccount",
    >       "metadata": {
    >           "creationTimestamp": "2023-08-18T12:06:42Z",
    >           "name": "dashboard-admin",
    >           "namespace": "kube-system",
    >           "resourceVersion": "17252",
    >           "uid": "65b9ea71-d232-4c5b-b859-6b2a1fbd77e4"
    >       }
    >   }


    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --namespace "${dashnamespace:?}" \
        create clusterrolebinding \
            "${dashaccount:?}-role" \
            --clusterrole=cluster-admin \
            --serviceaccount "${dashnamespace:?}:${dashaccount:?}"

    >   clusterrolebinding.rbac.authorization.k8s.io/dashboard-admin-role created


    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --namespace "${dashnamespace:?}" \
        get clusterrolebinding \
            --output json \
            "${dashaccount:?}-role"

    >   {
    >       "apiVersion": "rbac.authorization.k8s.io/v1",
    >       "kind": "ClusterRoleBinding",
    >       "metadata": {
    >           "creationTimestamp": "2023-08-18T12:07:11Z",
    >           "name": "dashboard-admin-role",
    >           "resourceVersion": "17407",
    >           "uid": "d9e4b8fb-1c59-476a-b038-456f1fa15e28"
    >       },
    >       "roleRef": {
    >           "apiGroup": "rbac.authorization.k8s.io",
    >           "kind": "ClusterRole",
    >           "name": "cluster-admin"
    >       },
    >       "subjects": [
    >           {
    >               "kind": "ServiceAccount",
    >               "name": "dashboard-admin",
    >               "namespace": "kube-system"
    >           }
    >       ]
    >   }

# -----------------------------------------------------
# Create a token for the dashboard-admin user.
# https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md#getting-a-bearer-token-for-serviceaccount
#[root@bootstrap]

    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --namespace "${dashnamespace:?}" \
        create token \
            "${dashaccount:?}"

    >   ........


# -----------------------------------------------------
# -----------------------------------------------------
# Use the token to login to the dashboard.
#[user@desktop]

    firefox \
        --new-window \
        'http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/#/login' \
        &

    #
    # Yay - works :-)
    #


# -----------------------------------------------------
# Create a secret with the token for the dashboard-admin user.
# https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md#getting-a-long-lived-bearer-token-for-serviceaccount
# https://gist.github.com/zulhfreelancer/dc909185dde3a5ed5373ba0fd4ebb952?permalink_comment_id=4012048#gistcomment-4012048
#[root@bootstrap]

    #
    # This isn't needed to login, we can create new tokens each time.
    # Left this in for reference.
    #

    cat << EOF | kubectl apply \
        --kubeconfig "${workclusterconf:?}" \
        --filename -
apiVersion: v1
kind: Secret
type: kubernetes.io/service-account-token
metadata:
  name: ${dashaccount}-token
  namespace: ${dashnamespace:?}
  annotations:
    kubernetes.io/service-account.name: ${dashaccount}
EOF

    >   secret/dashboard-admin-token created


# -----------------------------------------------------
# Extract the token from the secret.
# https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md#getting-a-long-lived-bearer-token-for-serviceaccount
#[root@bootstrap]

    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --namespace "${dashnamespace:?}" \
        get secret \
            "${dashaccount}-token" \
            --output json \
    | jq '.'

    >   {
    >     "apiVersion": "v1",
    >     "data": {
    >       "ca.crt": "........",
    >       "namespace": "........",
    >       "token": "........"
    >     },
    >     "kind": "Secret",
    >     "metadata": {
    >       "annotations": {
    >         "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"Secret\",\"metadata\":{\"annotations\":{\"kubernetes.io/service-account.name\":\"dashboard-admin\"},\"name\":\"dashboard-admin-token\",\"namespace\":\"kube-system\"},\"type\":\"kubernetes.io/service-account-token\"}\n",
    >         "kubernetes.io/service-account.name": "dashboard-admin",
    >         "kubernetes.io/service-account.uid": "65b9ea71-d232-4c5b-b859-6b2a1fbd77e4"
    >       },
    >       "creationTimestamp": "2023-08-18T12:11:27Z",
    >       "name": "dashboard-admin-token",
    >       "namespace": "kube-system",
    >       "resourceVersion": "18768",
    >       "uid": "29abb9eb-aeb0-4254-9a33-467fea5c68e2"
    >     },
    >     "type": "kubernetes.io/service-account-token"
    >   }


    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --namespace "${dashnamespace:?}" \
        get secret \
            "${dashaccount}-token" \
            --output json \
    | jq -r '.data.token'

    >   ........


    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --namespace "${dashnamespace:?}" \
        get secret \
            "${dashaccount}-token" \
            --output json \
    | jq -r '.data.token' \
    | base64 -d

    >   ........


# -----------------------------------------------------
# -----------------------------------------------------
# Use the token to login to the dashboard.
#[user@desktop]

    firefox \
        --new-window \
        'http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/#/login' \
        &

    #
    # Yay - works :-)
    #

