#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: [{"name": "ChatGPT","contribution": {"value": 0,"units": "%"}}]
#


    Target:

        Test our updated application credentials (with load-balancer_member).
        Test scaling the K8s cluster.

    Result:

        Work in progress ...

# -----------------------------------------------------
# Check which platform is live.
#[user@desktop]

    ssh fedora@live.gaia-dmp.uk \
        '
        date
        hostname
        '

    >   Thu 20 Jul 12:43:19 UTC 2023
    >   iris-gaia-green-20230308-zeppelin


# -----------------------------------------------------
# Create our client container.
#[user@desktop]

    agclient blue

    >   ....
    >   ....


# -----------------------------------------------------
# Check our application credentials.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        application credential \
            list

    >   +----------------------------------+------------------+----------------------------------+------------------+------------+
    >   | ID                               | Name             | Project ID                       | Description      | Expires At |
    >   +----------------------------------+------------------+----------------------------------+------------------+------------+
    >   |                         ........ | ........         |                         ........ | ........         | ....       |
    >   | ef6688feaded416685a529d5b0b6c598 | gaia-arcus-blue  | e918a13fed2648758175a15fac083569 | None             | None       |
    >   |                         ........ | ........         |                         ........ | ........         | ....       |
    >   +----------------------------------+------------------+----------------------------------+------------------+------------+

    credname=gaia-arcus-blue

    credid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            application credential \
                list \
                    --format json \
        | jq -r \
            --arg credname "${credname:?}" \
            '
            .[]
            | select(.Name == $credname)
            | .ID
            '
        )

    openstack \
        --os-cloud "${cloudname:?}" \
        application credential \
            show \
                "${credid}"

    >   +--------------+----------------------------------+
    >   | Field        | Value                            |
    >   +--------------+----------------------------------+
    >   | description  | None                             |
    >   | expires_at   | None                             |
    >   | id           | ef6688feaded416685a529d5b0b6c598 |
    >   | name         | gaia-arcus-blue                  |
    >   | project_id   | e918a13fed2648758175a15fac083569 |
    >   | roles        | reader member                    |
    >   | system       | None                             |
    >   | unrestricted | False                            |
    >   | user_id      | 5fa0c97a6dd14e01a3c7d91dad5c6b17 |
    >   +--------------+----------------------------------+


# -----------------------------------------------------

    #
    # Use the Horizon UI to create new application credentials with the load-balancer_member option.
    # ...
    #

# -----------------------------------------------------
# Update our clouds.yml file.
#[user@desktop]

    gedit "${HOME}/~/clouds.yaml" &

        ....

        iris-gaia-blue:
          auth:
            auth_url: https://arcus.openstack.hpc.cam.ac.uk:5000
    ~       application_credential_id: "........"
    ~       application_credential_secret: "........"
          region_name: "RegionOne"
          interface: "public"
          identity_api_version: 3
          auth_type: "v3applicationcredential"

        ....


# -----------------------------------------------------
# Create our client container.
#[user@desktop]

    agclient blue

    >   ....
    >   ....

# -----------------------------------------------------
# Check our application credentials.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        application credential \
            list

    >   +----------------------------------+------------------+----------------------------------+-----------------------------------------------+------------+
    >   | ID                               | Name             | Project ID                       | Description                                   | Expires At |
    >   +----------------------------------+------------------+----------------------------------+-----------------------------------------------+------------+
    >   |                         ........ | ........         |                         ........ | ........                                      | ....       |
    >   | 557c2c5b93a14973aff62662173a4b5d | gaia-arcus-blue  | e918a13fed2648758175a15fac083569 | User credential for the iris-gaia-blue cloud. | None       |
    >   |                         ........ | ........         |                         ........ | ........                                      | ....       |
    >   +----------------------------------+------------------+----------------------------------+-----------------------------------------------+------------+

    credname=gaia-arcus-blue

    credid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            application credential \
                list \
                    --format json \
        | jq -r \
            --arg credname "${credname:?}" \
            '
            .[]
            | select(.Name == $credname)
            | .ID
            '
        )

    openstack \
        --os-cloud "${cloudname:?}" \
        application credential \
            show \
                "${credid}"

    >   +--------------+-----------------------------------------------+
    >   | Field        | Value                                         |
    >   +--------------+-----------------------------------------------+
    >   | description  | User credential for the iris-gaia-blue cloud. |
    >   | expires_at   | None                                          |
    >   | id           | 557c2c5b93a14973aff62662173a4b5d              |
    >   | name         | gaia-arcus-blue                               |
    >   | project_id   | e918a13fed2648758175a15fac083569              |
    >   | roles        | reader load-balancer_member member            |
    >   | system       | None                                          |
    >   | unrestricted | False                                         |
    >   | user_id      | 5fa0c97a6dd14e01a3c7d91dad5c6b17              |
    >   +--------------+-----------------------------------------------+


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....

    #
    # No errors with deleting or listing the load balancers :-)
    #

    #
    # Having problems deleting some of the CephFS shares though :-(
    #

    >   ....
    >   Manila shares
    >   +--------------------------------------+------------------------------+------+-------------+----------------+-----------+-----------------+------+-------------------+
    >   | ID                                   | Name                         | Size | Share Proto | Status         | Is Public | Share Type Name | Host | Availability Zone |
    >   +--------------------------------------+------------------------------+------+-------------+----------------+-----------+-----------------+------+-------------------+
    >   | dcfa8904-6c11-453c-a82d-dafca87380f6 | blue-test                    |   10 | CEPHFS      | available      | False     | ceph01_cephfs   |      | nova              |
    >   | 5b97c1a5-da70-4417-ba63-80ed2c1301bf | iris-gaia-blue-home-Evison   |    1 | CEPHFS      | deleting       | False     | ceph01_cephfs   |      | nova              |
    >   | 02ad6c76-06d8-4a51-a898-8fb3cd1e2f52 | iris-gaia-blue-home-Reyesfan |    1 | CEPHFS      | error_deleting | False     | ceph01_cephfs   |      | nova              |
    >   | bcc5edd9-dabc-4a18-8c70-dd86ae8e404c | iris-gaia-blue-user-Evison   |    1 | CEPHFS      | deleting       | False     | ceph01_cephfs   |      | nova              |
    >   | 8e108df6-746e-4882-9449-cc11740c1dad | iris-gaia-blue-user-Reyesfan |    1 | CEPHFS      | deleting       | False     | ceph01_cephfs   |      | nova              |
    >   +--------------------------------------+------------------------------+------+-------------+----------------+-----------+-----------------+------+-------------------+
    >   ....


# -----------------------------------------------------
# List everything.
#[root@ansibler]

    /deployments/openstack/bin/list-all.sh \
        "${cloudname:?}"

    >   ....
    >   Manila shares
    >   +--------------------------------------+------------------------------+------+-------------+----------------+-----------+-----------------+------+-------------------+
    >   | ID                                   | Name                         | Size | Share Proto | Status         | Is Public | Share Type Name | Host | Availability Zone |
    >   +--------------------------------------+------------------------------+------+-------------+----------------+-----------+-----------------+------+-------------------+
    >   | dcfa8904-6c11-453c-a82d-dafca87380f6 | blue-test                    |   10 | CEPHFS      | available      | False     | ceph01_cephfs   |      | nova              |
    >   | 5b97c1a5-da70-4417-ba63-80ed2c1301bf | iris-gaia-blue-home-Evison   |    1 | CEPHFS      | deleting       | False     | ceph01_cephfs   |      | nova              |
    >   | 02ad6c76-06d8-4a51-a898-8fb3cd1e2f52 | iris-gaia-blue-home-Reyesfan |    1 | CEPHFS      | error_deleting | False     | ceph01_cephfs   |      | nova              |
    >   | bcc5edd9-dabc-4a18-8c70-dd86ae8e404c | iris-gaia-blue-user-Evison   |    1 | CEPHFS      | deleting       | False     | ceph01_cephfs   |      | nova              |
    >   | 8e108df6-746e-4882-9449-cc11740c1dad | iris-gaia-blue-user-Reyesfan |    1 | CEPHFS      | deleting       | False     | ceph01_cephfs   |      | nova              |
    >   +--------------------------------------+------------------------------+------+-------------+----------------+-----------+-----------------+------+-------------------+
    >   ....


# -----------------------------------------------------
# Initialise our status file.
#[root@ansibler]

    /deployments/cluster-api/bootstrap/bin/init-status.sh \
        "${cloudname:?}"

    >   aglais:
    >     deployment:
    >       type: cluster-api
    >       name: iris-gaia-blue-20230720
    >       date: 20230720T142407
    >     openstack:
    >       cloud:
    >         name: iris-gaia-blue
    >       user:
    >         id: 5fa0c97a6dd14e01a3c7d91dad5c6b17
    >         name: dmorris_gaia
    >       project:
    >         id: e918a13fed2648758175a15fac083569
    >         name: iris-gaia-blue


# -----------------------------------------------------
# Deploy our bootstrap node.
#[root@ansibler]

    ansible-playbook \
        --inventory \
        '/deployments/cluster-api/bootstrap/ansible/config/inventory.yml' \
        '/deployments/cluster-api/bootstrap/ansible/00-create-all.yml'

    >   ....
    >   ....


# -----------------------------------------------------
# Login to our bootstrap node as root.
#[root@ansibler]

    ssh root@bootstrap


# -----------------------------------------------------
# Create the initial Kubernetes in Docker (KinD) cluster.
#[root@bootstrap]

    kindclustername=bootstrap
    kindclusterfull=${kindclustername:?}-$(date '+%Y%m%d')
    kindclusterpath=/opt/aglais/${kindclustername:?}
    kindclusterconf=${kindclusterpath:?}/${kindclusterfull:?}-kubeconfig.yml

    mkdir -p "${kindclusterpath}"

    kind create cluster \
        --name "${kindclusterfull:?}" \
        --kubeconfig "${kindclusterconf:?}"

    >   Creating cluster "bootstrap-20230720" ...
    >    ✓ Ensuring node image (kindest/node:v1.25.3) 🖼
    >    ✓ Preparing nodes 📦
    >    ✓ Writing configuration 📜
    >    ✓ Starting control-plane 🕹️
    >    ✓ Installing CNI 🔌
    >    ✓ Installing StorageClass 💾
    >   Set kubectl context to "kind-bootstrap-20230720"
    >   ....
    >   ....


# -----------------------------------------------------
# Install the Openstack ClusterAPI components.
#[root@bootstrap]

    clusterctl init \
        --kubeconfig "${kindclusterconf:?}" \
        --infrastructure openstack

    >   Fetching providers
    >   Installing cert-manager Version="v1.12.2"
    >   Waiting for cert-manager to be available...
    >   Installing Provider="cluster-api" Version="v1.4.4" TargetNamespace="capi-system"
    >   Installing Provider="bootstrap-kubeadm" Version="v1.4.4" TargetNamespace="capi-kubeadm-bootstrap-system"
    >   Installing Provider="control-plane-kubeadm" Version="v1.4.4" TargetNamespace="capi-kubeadm-control-plane-system"
    >   Installing Provider="infrastructure-openstack" Version="v0.7.3" TargetNamespace="capo-system"
    >
    >   Your management cluster has been initialized successfully!
    >   ....
    >   ....


# -----------------------------------------------------
# Install the StackHPC Helm charts.
#[root@bootstrap]

    helm repo add \
        capi \
        https://stackhpc.github.io/capi-helm-charts

    helm repo add \
        capi-addons \
        https://stackhpc.github.io/cluster-api-addon-provider

    helm upgrade \
        --kubeconfig "${kindclusterconf:?}" \
        cluster-api-addon-provider \
        capi-addons/cluster-api-addon-provider \
            --install \
            --version "0.1.0"

# -----------------------------------------------------
# Wait for the StackHPC components to be ready.
#[root@bootstrap]

    api_resources_check()
        {
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            api-resources \
                --api-group addons.stackhpc.com \
        | awk '
            BEGIN {
                found = 0
                }
            {
            if ($1 == "helmreleases" || $1 == "manifests") {
                found++
                }
            }
            END {
                if (found == 2) {
                    print "match"
                    }
                else {
                    print "no match"
                    }
                }
            '
        }

    loop=120
    while [[ ${loop} > 0 ]]
    do
        result=$(
            api_resources_check
            )

        echo "Loop [${loop}] [${result}]"

        if [[ "${result}" == "match" ]]
        then
            break
        else
            sleep 1
            ((loop--))
        fi
    done


    >   "capi" has been added to your repositories
    >   "capi-addons" has been added to your repositories

    >   Release "cluster-api-addon-provider" does not exist. Installing it now.
    >   NAME: cluster-api-addon-provider
    >   LAST DEPLOYED: Thu Jul 20 14:34:30 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None

    >   Loop [120] [no match]
    >   Loop [119] [no match]
    >   ....
    >   ....
    >   Loop [103] [no match]
    >   Loop [102] [match]


    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        api-resources \
            --api-group addons.stackhpc.com

    >   NAME           SHORTNAMES   APIVERSION                     NAMESPACED   KIND
    >   helmreleases                addons.stackhpc.com/v1alpha1   true         HelmRelease
    >   manifests                   addons.stackhpc.com/v1alpha1   true         Manifests

# -----------------------------------------------------
# Deploy our target cluster.
#[root@bootstrap]

    workclustername=gaia-dmp-one
    workclusterfull=${workclustername:?}-$(date '+%Y%m%d')

    workclusterpath=/opt/aglais/${workclustername:?}
    workclustertext=${workclusterpath:?}/${workclusterfull:?}.txt
    workclusterconf=${workclusterpath:?}/${workclusterfull:?}-kubeconfig.yml

    helm upgrade \
        --kubeconfig "${kindclusterconf:?}" \
        "${workclusterfull:?}" \
        capi/openstack-cluster \
            --install \
            --version "0.1.0" \
            --values '/opt/aglais/clusterapi-config.yml' \
            --values '/opt/aglais/openstack-clouds.yaml'

    >   Release "gaia-dmp-one-20230720" does not exist. Installing it now.
    >   NAME: gaia-dmp-one-20230720
    >   LAST DEPLOYED: Thu Jul 20 14:35:52 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None


# -----------------------------------------------------
# Watch the events log.
#[root@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get events \
            --watch

    >   LAST SEEN   TYPE      REASON                    OBJECT                                                   MESSAGE
    >   4m46s       Normal    NodeHasSufficientMemory   node/bootstrap-20230720-control-plane                    Node bootstrap-20230720-control-plane status is now: NodeHasSufficientMemory
    >   4m46s       Normal    NodeHasNoDiskPressure     node/bootstrap-20230720-control-plane                    Node bootstrap-20230720-control-plane status is now: NodeHasNoDiskPressure
    >   4m46s       Normal    NodeHasSufficientPID      node/bootstrap-20230720-control-plane                    Node bootstrap-20230720-control-plane status is now: NodeHasSufficientPID
    >   4m34s       Normal    Starting                  node/bootstrap-20230720-control-plane                    Starting kubelet.
    >   4m34s       Normal    NodeAllocatableEnforced   node/bootstrap-20230720-control-plane                    Updated Node Allocatable limit across pods
    >   ....
    >   ....

    >   ....
    >   ....
    >   0s          Normal    Successfulcreatelistener        openstackcluster/gaia-dmp-one-20230720                        Created listener k8s-clusterapi-cluster-default-gaia-dmp-one-20230720-kubeapi-6443 with id a3fa92dc-ec2e-4dfd-a371-52c2c92050fc
    >   0s          Normal    Successfulcreatepool            openstackcluster/gaia-dmp-one-20230720                        Created pool k8s-clusterapi-cluster-default-gaia-dmp-one-20230720-kubeapi-6443 with id 15b89494-2241-42d3-b0cd-e64dbf105243
    >   0s          Normal    Successfulcreatemonitor         openstackcluster/gaia-dmp-one-20230720                        Created monitor k8s-clusterapi-cluster-default-gaia-dmp-one-20230720-kubeapi-6443 with id e71b09e1-0b67-4e4b-978e-ea86e3d78456
    >   0s          Normal    InfrastructureReady             cluster/gaia-dmp-one-20230720                                 Cluster gaia-dmp-one-20230720 InfrastructureReady is now true
    >   0s          Normal    Provisioned                     cluster/gaia-dmp-one-20230720                                 Cluster gaia-dmp-one-20230720 is Provisioned
    >   0s          Normal    InfrastructureReady             cluster/gaia-dmp-one-20230720                                 Cluster gaia-dmp-one-20230720 InfrastructureReady is now true
    >   0s          Normal    Provisioned                     cluster/gaia-dmp-one-20230720                                 Cluster gaia-dmp-one-20230720 is Provisioned
    >   ....
    >   ....

    >   ....
    >   ....
    >   0s          Normal    DetectedUnhealthy               machine/gaia-dmp-one-20230720-control-plane-q2hvr             Machine default/gaia-dmp-one-20230720-control-plane/gaia-dmp-one-20230720-control-plane-q2hvr/ has unhealthy node
    >   0s          Normal    DetectedUnhealthy               machine/gaia-dmp-one-20230720-control-plane-q2hvr             Machine default/gaia-dmp-one-20230720-control-plane/gaia-dmp-one-20230720-control-plane-q2hvr/ has unhealthy node
    >   ....
    >   0s          Warning   FailedMount                     pod/gaia-dmp-one-20230720-autoscaler-57dfcc4865-mkwb5         Unable to attach or mount volumes: unmounted volumes=[kubeconfig], unattached volumes=[kubeconfig kube-api-access-jkt8q]: timed out waiting for the condition
    >   0s          Warning   ControlPlaneUnhealthy           kubeadmcontrolplane/gaia-dmp-one-20230720-control-plane       Waiting for control plane to pass preflight checks to continue reconciliation: [machine gaia-dmp-one-20230720-control-plane-q2hvr does not have APIServerPodHealthy condition, machine gaia-dmp-one-20230720-control-plane-q2hvr does not have ControllerManagerPodHealthy condition, machine gaia-dmp-one-20230720-control-plane-q2hvr does not have SchedulerPodHealthy condition, machine gaia-dmp-one-20230720-control-plane-q2hvr does not have EtcdPodHealthy condition, machine gaia-dmp-one-20230720-control-plane-q2hvr does not have EtcdMemberHealthy condition]
    >   0s          Warning   ControlPlaneUnhealthy           kubeadmcontrolplane/gaia-dmp-one-20230720-control-plane       Waiting for control plane to pass preflight checks to continue reconciliation: [machine gaia-dmp-one-20230720-control-plane-q2hvr does not have APIServerPodHealthy condition, machine gaia-dmp-one-20230720-control-plane-q2hvr does not have ControllerManagerPodHealthy condition, machine gaia-dmp-one-20230720-control-plane-q2hvr does not have SchedulerPodHealthy condition, machine gaia-dmp-one-20230720-control-plane-q2hvr does not have EtcdPodHealthy condition, machine gaia-dmp-one-20230720-control-plane-q2hvr does not have EtcdMemberHealthy condition]
    >   ....
    >   ....

    >   ....
    >   ....
    >   0s          Warning   ControlPlaneUnhealthy           kubeadmcontrolplane/gaia-dmp-one-20230720-control-plane       Waiting for control plane to pass preflight checks to continue reconciliation: [machine gaia-dmp-one-20230720-control-plane-tzknr does not have APIServerPodHealthy condition, machine gaia-dmp-one-20230720-control-plane-tzknr does not have ControllerManagerPodHealthy condition, machine gaia-dmp-one-20230720-control-plane-tzknr does not have SchedulerPodHealthy condition, machine gaia-dmp-one-20230720-control-plane-tzknr does not have EtcdPodHealthy condition, machine gaia-dmp-one-20230720-control-plane-tzknr does not have EtcdMemberHealthy condition]
    >   ....
    >   ....


# -----------------------------------------------------
# Login to our client and list the resources.
#[user@desktop]

    podman ps

    >   CONTAINER ID  IMAGE                                              COMMAND     CREATED            STATUS                PORTS       NAMES
    >   c6acae1695db  ghcr.io/wfau/atolmis/kubernetes-client:2023.06.15  bash        About an hour ago  Up About an hour ago              ansibler-blue


    podman exec \
        --tty \
        --interactive \
        'ansibler-blue' \
            'bash'


        /deployments/openstack/bin/list-all.sh \
            "${cloudname:?}"


    >   ....
    >   ....
    >   ---- ----
    >   Nova servers
    >   +--------------------------------------+-----------------------------------+--------+----------------------------------------------------------------------+---------------+----------------------+
    >   | ID                                   | Name                              | Status | Networks                                                             | Image         | Flavor               |
    >   +--------------------------------------+-----------------------------------+--------+----------------------------------------------------------------------+---------------+----------------------+
    >   | f39bbd9e-df4d-4c6b-b139-0901bda75ad6 | iris-gaia-blue-20230720-bootstrap | ACTIVE | iris-gaia-blue-20230720-internal-network=10.10.0.175, 128.232.227.33 | Fedora-34.1.2 | gaia.vm.cclake.2vcpu |
    >   +--------------------------------------+-----------------------------------+--------+----------------------------------------------------------------------+---------------+----------------------+
    >
    >   ....
    >   ....
    >
    >   ---- ----
    >   Floating addresses
    >   +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+
    >   | ID                                   | Floating IP Address | Fixed IP Address | Port                                 | Floating Network                     | Project                          |
    >   +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+
    >   | 1e7ac701-8ba3-4b51-9807-f67a5f949b79 | 128.232.227.33      | 10.10.0.175      | e2a98239-e3e8-403f-9a7b-727c83f34bcb | 57add367-d205-4030-a929-d75617a7c63e | e918a13fed2648758175a15fac083569 |
    >   | 435b4348-f655-49ce-a505-63f2fc813ed1 | 128.232.226.136     | 192.168.3.76     | a13b1ccb-33e1-429b-93ea-df96dd6dbd9b | 57add367-d205-4030-a929-d75617a7c63e | e918a13fed2648758175a15fac083569 |
    >   +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+
    >
    >   ---- ----
    >   Load balancers
    >   +--------------------------------------+--------------------------------------------------------------+----------------------------------+--------------+---------------------+------------------+----------+
    >   | id                                   | name                                                         | project_id                       | vip_address  | provisioning_status | operating_status | provider |
    >   +--------------------------------------+--------------------------------------------------------------+----------------------------------+--------------+---------------------+------------------+----------+
    >   | 25e5dfa6-7fd8-4b81-81ab-07a0e9949db7 | k8s-clusterapi-cluster-default-gaia-dmp-one-20230720-kubeapi | e918a13fed2648758175a15fac083569 | 192.168.3.76 | ACTIVE              | ONLINE           | amphora  |
    >   +--------------------------------------+--------------------------------------------------------------+----------------------------------+--------------+---------------------+------------------+----------+
    >
    >   ---- ----
    >   Routers
    >   +--------------------------------------+------------------------------------------------------+--------+-------+----------------------------------+
    >   | ID                                   | Name                                                 | Status | State | Project                          |
    >   +--------------------------------------+------------------------------------------------------+--------+-------+----------------------------------+
    >   | c10c05bf-03b1-4ddd-9436-db286c500a3c | iris-gaia-blue-20230720-internal-router              | ACTIVE | UP    | e918a13fed2648758175a15fac083569 |
    >   | eaec089f-4cdc-4775-b560-ec5ef329cbd8 | k8s-clusterapi-cluster-default-gaia-dmp-one-20230720 | ACTIVE | UP    | e918a13fed2648758175a15fac083569 |
    >   +--------------------------------------+------------------------------------------------------+--------+-------+----------------------------------+
    >
    >   ---- ----
    >   Networks
    >   +--------------------------------------+------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
    >   | ID                                   | Name                                                 | Subnets                                                                                                                                                |
    >   +--------------------------------------+------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
    >   | 3d6fc0a6-06f6-48ea-8e66-16102ffea3f7 | k8s-clusterapi-cluster-default-gaia-dmp-one-20230720 | ad970d24-0daa-4ac1-8c63-6b8b49f336d5                                                                                                                   |
    >   | 410920fb-5714-4447-b26a-e7b06092fc62 | cephfs                                               | 5699fb5d-8316-4b88-b889-b05c8a1ec975                                                                                                                   |
    >   | 57add367-d205-4030-a929-d75617a7c63e | CUDN-Internet                                        | 1847b14d-b974-4f78-959d-44d18d4485b8, 3fcaa5a5-ba8e-49a9-bf94-d87fbb0afc42, 5f1388b3-a0c7-463e-bb58-5532c38e4b40, a79eb610-eca3-4ee8-aaf1-88f4fef5a4e7 |
    >   | da8c73b4-f055-497b-9232-2bb27c64b89d | iris-gaia-blue-20230720-internal-network             | 75044be7-c70c-47e4-b8f7-7d8c24c9f337                                                                                                                   |
    >   +--------------------------------------+------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
    >
    >   ---- ----
    >   Subnets
    >   +--------------------------------------+------------------------------------------------------+--------------------------------------+----------------+
    >   | ID                                   | Name                                                 | Network                              | Subnet         |
    >   +--------------------------------------+------------------------------------------------------+--------------------------------------+----------------+
    >   | 5699fb5d-8316-4b88-b889-b05c8a1ec975 | cephfs                                               | 410920fb-5714-4447-b26a-e7b06092fc62 | 10.9.0.0/16    |
    >   | 75044be7-c70c-47e4-b8f7-7d8c24c9f337 | iris-gaia-blue-20230720-internal-subnet              | da8c73b4-f055-497b-9232-2bb27c64b89d | 10.10.0.0/16   |
    >   | ad970d24-0daa-4ac1-8c63-6b8b49f336d5 | k8s-clusterapi-cluster-default-gaia-dmp-one-20230720 | 3d6fc0a6-06f6-48ea-8e66-16102ffea3f7 | 192.168.3.0/24 |
    >   +--------------------------------------+------------------------------------------------------+--------------------------------------+----------------+
    >
    >   ---- ----
    >   Security groups
    >   +--------------------------------------+-----------------------------------------------------------------+---------------------------+----------------------------------+------+
    >   | ID                                   | Name                                                            | Description               | Project                          | Tags |
    >   +--------------------------------------+-----------------------------------------------------------------+---------------------------+----------------------------------+------+
    >   | 2e80b118-0f58-49c9-862b-86816b83c901 | iris-gaia-blue-20230720-bootstrap-security                      |                           | e918a13fed2648758175a15fac083569 | []   |
    >   | 99e050b6-34b8-4c27-a685-05db1cc69b4e | k8s-cluster-default-gaia-dmp-one-20230720-secgroup-worker       | Cluster API managed group | e918a13fed2648758175a15fac083569 | []   |
    >   | bf1131d5-d4d7-4ea3-bbec-bb0b824d0e0f | k8s-cluster-default-gaia-dmp-one-20230720-secgroup-controlplane | Cluster API managed group | e918a13fed2648758175a15fac083569 | []   |
    >   | e1c6a1db-3caf-47f5-91e2-51a3e1967dc6 | default                                                         | Default security group    | e918a13fed2648758175a15fac083569 | []   |
    >   +--------------------------------------+-----------------------------------------------------------------+---------------------------+----------------------------------+------+

    #
    # Everything *apart* from the virtual machines.
    # Guess - is this an issue with accessing the VM image ?
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        image list

    >   +--------------------------------------+------------------------------------------------+--------+
    >   | ID                                   | Name                                           | Status |
    >   +--------------------------------------+------------------------------------------------+--------+
    >   | 0f242b58-0563-46c5-b357-64b46cf46030 | AlmaLinux-8.5-20211119                         | active |
    >   |                             ........ | ........                                       | ...... |
    >   |                             ........ | ........                                       | ...... |
    >   | a42aef9d-a910-4a5e-b2f4-9a3e9d8cfc7e | Ubuntu-Focal-20.04-20220124                    | active |
    >   | 796cc099-e302-4e9b-b5cf-d99a633da092 | os_migrate_conv                                | active |
    >   +--------------------------------------+------------------------------------------------+--------+

    #
    # No sign of the 'kube' images we were using ..
    #

    openstack \
        --os-cloud 'iris-gaia-red' \
        image list

    >   +--------------------------------------+------------------------------------------------+--------+
    >   | ID                                   | Name                                           | Status |
    >   +--------------------------------------+------------------------------------------------+--------+
    >   | 0f242b58-0563-46c5-b357-64b46cf46030 | AlmaLinux-8.5-20211119                         | active |
    >   |                             ........ | ........                                       | ...... |
    >   |                             ........ | ........                                       | ...... |
    >   | a42aef9d-a910-4a5e-b2f4-9a3e9d8cfc7e | Ubuntu-Focal-20.04-20220124                    | active |
    >   | 686c415b-c5a6-419e-8c46-4732498582e8 | gaia-dmp-ubuntu-2004-kube-v1.25.4              | active |
    >   | 796cc099-e302-4e9b-b5cf-d99a633da092 | os_migrate_conv                                | active |
    >   +--------------------------------------+------------------------------------------------+--------+

    #
    # So we need to bring the 'kube' images into this project too.
    # We can see the  'kube' images in the list of community images.
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        image list \
            --community

    >   +--------------------------------------+----------------------------------------------+--------+
    >   | ID                                   | Name                                         | Status |
    >   +--------------------------------------+----------------------------------------------+--------+
    >   | 948b088f-f15a-4082-9fa2-8d6ec46d63fc | CentOS8-2004                                 | active |
    >   | ........                             | ........                                     | ...... |
    >   | aba8be34-d96e-4276-863f-af76ebab71d5 | ubuntu-2004-kube-v1.22.12                    | active |
    >   | 932a8846-ce6d-49a9-b926-f200c322d237 | ubuntu-2004-kube-v1.22.15                    | active |
    >   | d248843e-f1ea-49b2-b3c8-fca8e0ecb3fe | ubuntu-2004-kube-v1.23.14                    | active |
    >   | 6d2fe080-a9fe-4fde-a8d3-cd101c5310a3 | ubuntu-2004-kube-v1.23.9                     | active |
    >   | a83b5cfc-eb5e-4ca3-8d5c-cef95f41bcce | ubuntu-2004-kube-v1.24.2                     | active |
    >   | 0350c998-23c7-44c5-8782-e29ac9640527 | ubuntu-2004-kube-v1.24.8                     | active |
    >   | 75f1e989-d2e3-4f31-b660-8a8a9e5fa967 | ubuntu-2004-kube-v1.25.4                     | active |
    >   | ........                             | ........                                     | ...... |
    >   +--------------------------------------+----------------------------------------------+--------+

    #
    # Last time we tried this we couldn't grant access to the community images.
    # The easiest way to get access was to download the image and upload it ourselves.
    #

# -----------------------------------------------------
# Check the image properties.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        image show \
            '75f1e989-d2e3-4f31-b660-8a8a9e5fa967'

    >   +------------------+------------------------------------------------------------------------------------------------------------------------+
    >   | Field            | Value                                                                                                                  |
    >   +------------------+------------------------------------------------------------------------------------------------------------------------+
    >   | checksum         | 225a4fec21b30be3fbb4121554baa8f4                                                                                       |
    >   | container_format | bare                                                                                                                   |
    >   | created_at       | 2022-11-15T18:47:31Z                                                                                                   |
    >   | disk_format      | qcow2                                                                                                                  |
    >   | file             | /v2/images/75f1e989-d2e3-4f31-b660-8a8a9e5fa967/file                                                                   |
    >   | id               | 75f1e989-d2e3-4f31-b660-8a8a9e5fa967                                                                                   |
    >   | min_disk         | 0                                                                                                                      |
    >   | min_ram          | 0                                                                                                                      |
    >   | name             | ubuntu-2004-kube-v1.25.4                                                                                               |
    >   | owner            | 49ebd4f338824999a04114d13dfd8caa                                                                                       |
    >   | properties       | direct_url='rbd://a900cf30-f8a3-42bf-98d6-af7ce92f1a1a/arcus-images/75f1e989-d2e3-4f31-b660-8a8a9e5fa967/snap',        |
    >   |                  | locations='                                                                                                            |
    >   |                  |     [                                                                                                                  |
    >   |                  |         {                                                                                                              |
    >   |                  |         'url': 'rbd://a900cf30-f8a3-42bf-98d6-af7ce92f1a1a/arcus-images/75f1e989-d2e3-4f31-b660-8a8a9e5fa967/snap',    |
    >   |                  |         'metadata': {                                                                                                  |
    >   |                  |             'store': 'rbd'                                                                                             |
    >   |                  |             }                                                                                                          |
    >   |                  |         }                                                                                                              |
    >   |                  |     ]',                                                                                                                |
    >   |                  | os_hash_algo='sha512',                                                                                                 |
    >   |                  | os_hash_value='3b00....8961',                                                                                          |
    >   |                  | os_hidden='False',                                                                                                     |
    >   |                  | owner_specified.openstack.md5='',                                                                                      |
    >   |                  | owner_specified.openstack.object='images/ubuntu-2004-kube-v1.25.4',                                                    |
    >   |                  | owner_specified.openstack.sha256='',                                                                                   |
    >   |                  | stores='rbd'                                                                                                           |
    >   | protected        | False                                                                                                                  |
    >   | schema           | /v2/schemas/image                                                                                                      |
    >   | size             | 4441047040                                                                                                             |
    >   | status           | active                                                                                                                 |
    >   | tags             |                                                                                                                        |
    >   | updated_at       | 2023-01-19T14:59:48Z                                                                                                   |
    >   | visibility       | community                                                                                                              |
    >   +------------------+------------------------------------------------------------------------------------------------------------------------+


# -----------------------------------------------------
# Try downloading the image and uploading our own copy.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        image save \
            '75f1e989-d2e3-4f31-b660-8a8a9e5fa967' \
            --file /tmp/ubuntu-2004-kube-v1.25.4

    ls -alh /tmp/

    >   ....
    >   -rw-r--r--.  1 root root 4.2G Jul 20 21:12 ubuntu-2004-kube-v1.25.4
    >   ....


    openstack \
        --os-cloud "${cloudname:?}" \
        image create \
            --disk-format qcow2 \
            --container-format bare \
            --file /tmp/ubuntu-2004-kube-v1.25.4 \
            gaia-dmp-ubuntu-2004-kube-v1.25.4

    >   ....
    >   ....

    #
    # Uploading 4G of data will take too long ...
    #

# -----------------------------------------------------
# Tried different options ...
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        image create \
            --disk-format qcow2 \
            --container-format bare \
            --copy-from 'rbd://a900cf30-f8a3-42bf-98d6-af7ce92f1a1a/arcus-images/75f1e989-d2e3-4f31-b660-8a8a9e5fa967/snap' \
            gaia-dmp-ubuntu-2004-kube-v1.25.4

    >   ERROR: --copy-from was given, which is an Image v1 option that is no longer supported in Image v2


 openstack \
        --os-cloud "${cloudname:?}" \
        image create \
            --disk-format qcow2 \
            --container-format bare \
            --location 'rbd://a900cf30-f8a3-42bf-98d6-af7ce92f1a1a/arcus-images/75f1e989-d2e3-4f31-b660-8a8a9e5fa967/snap' \
            gaia-dmp-ubuntu-2004-kube-v1.25.4

    >   ERROR: --location was given, which is an Image v1 option that is no longer supported in Image v2


  openstack \
        --os-cloud "${cloudname:?}" \
        image create \
            --volume '75f1e989-d2e3-4f31-b660-8a8a9e5fa967' \
            gaia-xyz-ubuntu-2004-kube-v1.25.4

    >   No volume with a name or ID of '75f1e989-d2e3-4f31-b660-8a8a9e5fa967' exists.


    openstack \
        --os-cloud "${cloudname:?}" \
        volume create \
        --image '75f1e989-d2e3-4f31-b660-8a8a9e5fa967' \
         gaia-dmp-tempvol

    >   --size is a required option if snapshot or source volume is not specified.


    openstack \
        --os-cloud "${cloudname:?}" \
        volume create \
        --image '75f1e989-d2e3-4f31-b660-8a8a9e5fa967' \
        --size 4441047040 \
         gaia-dmp-tempvol

    >   Invalid input for field/attribute size.
    >       Value: 4441047040.
    >           4441047040 is greater than the maximum of 2147483647 (HTTP 400)


# -----------------------------------------------------
# Create our bootstrap VM, and tun our client container from there ....
#[user@desktop]

    agclient blue

    >   ....
    >   ....

# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    #
    # Still has problems with undeletable shares.
    #

    >   ....
    >   Deleting shares
    >   - Deleting share [iris-gaia-blue-home-Evison]
    >   Failed to delete share with name or ID 'iris-gaia-blue-home-Evison':
    >       Invalid share: Share status must be one of ('available', 'error', 'inactive').
    >       (HTTP 403) (Request-ID: req-5dc0b40d-1ecf-4d37-a3cb-40bb04ac8cbb)
    >   1 of 1 shares failed to delete.
    >   - Deleting share [iris-gaia-blue-home-Reyesfan]
    >   Failed to delete share with name or ID 'iris-gaia-blue-home-Reyesfan':
    >       Invalid share: Share status must be one of ('available', 'error', 'inactive').
    >       (HTTP 403) (Request-ID: req-b109f2f0-a996-410b-a463-cbb6708d2cd6)
    >   1 of 1 shares failed to delete.
    >   - Deleting share [iris-gaia-blue-user-Evison]
    >   Failed to delete share with name or ID 'iris-gaia-blue-user-Evison':
    >       Invalid share: Share status must be one of ('available', 'error', 'inactive').
    >       (HTTP 403) (Request-ID: req-d2a54fd6-dd7c-4fe7-ba35-2c54cc86dbdd)
    >   1 of 1 shares failed to delete.
    >   - Deleting share [iris-gaia-blue-user-Reyesfan]
    >   Failed to delete share with name or ID 'iris-gaia-blue-user-Reyesfan':
    >       Invalid share: Share status must be one of ('available', 'error', 'inactive').
    >       (HTTP 403) (Request-ID: req-eb9fe0e9-117d-41f9-b23d-d1380eace197)
    >   1 of 1 shares failed to delete.
    >   ....


# -----------------------------------------------------
# Initialise our status file.
#[root@ansibler]

    /deployments/cluster-api/bootstrap/bin/init-status.sh \
        "${cloudname:?}"

    >   aglais:
    >     deployment:
    >       type: cluster-api
    >       name: iris-gaia-blue-20230721
    >       date: 20230721T023338
    >     openstack:
    >       cloud:
    >         name: iris-gaia-blue
    >       user:
    >         id: 5fa0c97a6dd14e01a3c7d91dad5c6b17
    >         name: dmorris_gaia
    >       project:
    >         id: e918a13fed2648758175a15fac083569
    >         name: iris-gaia-blue


# -----------------------------------------------------
# Deploy our bootstrap node.
#[root@ansibler]

    ansible-playbook \
        --inventory \
        '/deployments/cluster-api/bootstrap/ansible/config/inventory.yml' \
        '/deployments/cluster-api/bootstrap/ansible/00-create-all.yml'

    >   ....
    >   ....


# -----------------------------------------------------
# Login to our bootstrap node as root.
#[root@ansibler]

    ssh root@bootstrap


# -----------------------------------------------------
# Create our Openstack client container.
#[root@bootstrap]

    cloudname=iris-gaia-blue

    docker run \
        --rm \
        --tty \
        --interactive \
        --name     "openstack-client" \
        --hostname "openstack-client" \
        --env "cloudname=${cloudname:?}" \
        --volume "/opt/aglais/openstack-clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        ghcr.io/wfau/atolmis/ansible-client:2022.07.25 \
        bash

    >   Unable to find image 'ghcr.io/wfau/atolmis/ansible-client:2022.07.25' locally
    >   2022.07.25: Pulling from wfau/atolmis/ansible-client
    >   862b97fea715: Pull complete
    >   ....
    >   ....
    >   b6f80f56ffa7: Pull complete
    >   Digest: sha256:ea63039dfd5a69e776f2474f59bd72b17b231189f98ff437d8a6d9bb052b95f1
    >   Status: Downloaded newer image for ghcr.io/wfau/atolmis/ansible-client:2022.07.25


# -----------------------------------------------------
# Try downloading the VM image and uploading our own copy.
#[root@openstack-client]

    openstack \
        --os-cloud "${cloudname:?}" \
        image save \
            '75f1e989-d2e3-4f31-b660-8a8a9e5fa967' \
            --file /tmp/ubuntu-2004-kube-v1.25.4

    >   Error authenticating with application credential:
    >       Application credentials cannot request a scope.
    >       (HTTP 401) (Request-ID: req-9f742ec5-57c6-485e-acdc-bfe02b55affa)

    #
    # This is the same command that works when run from my desktop.
    # Using the same client container and application credentials.
    #
    # Actually, this isn't the same clouds.yaml file.
    # The clouds.yaml on the bootstrap node was created using an Ansible template.
    # Which suggests there is something wrong with the template.
    #
    # Try again, removing the --volume option from the container launch and
    # manually copying our clouds.yaml file using copy/paste from desktop.
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        image save \
            '75f1e989-d2e3-4f31-b660-8a8a9e5fa967' \
            --file /tmp/ubuntu-2004-kube-v1.25.4

    >   ....
    >   ....


    ls -alh /tmp/

    >   ....
    >   ....
    >   -rw-r--r--. 1 root root 4.2G Jul 21 03:13 ubuntu-2004-kube-v1.25.4


    openstack \
        --os-cloud "${cloudname:?}" \
        image create \
            --disk-format qcow2 \
            --container-format bare \
            --file /tmp/ubuntu-2004-kube-v1.25.4 \
            gaia-dmp-ubuntu-2004-kube-v1.25.4

    >   +------------------+--------------------------------------------------------------------------------+
    >   | Field            | Value                                                                          |
    >   +------------------+--------------------------------------------------------------------------------+
    >   | container_format | bare                                                                           |
    >   | created_at       | 2023-07-21T03:14:26Z                                                           |
    >   | disk_format      | qcow2                                                                          |
    >   | file             | /v2/images/6ac13e0f-fee8-4cfc-9b88-fe94b3237f9a/file                           |
    >   | id               | 6ac13e0f-fee8-4cfc-9b88-fe94b3237f9a                                           |
    >   | min_disk         | 0                                                                              |
    >   | min_ram          | 0                                                                              |
    >   | name             | gaia-dmp-ubuntu-2004-kube-v1.25.4                                              |
    >   | owner            | e918a13fed2648758175a15fac083569                                               |
    >   | properties       | locations='[]',                                                                |
    >   |                  | os_hidden='False',                                                             |
    >   |                  | owner_specified.openstack.md5='',                                              |
    >   |                  | owner_specified.openstack.object='images/gaia-dmp-ubuntu-2004-kube-v1.25.4',   |
    >   |                  | owner_specified.openstack.sha256=''                                            |
    >   | protected        | False                                                                          |
    >   | schema           | /v2/schemas/image                                                              |
    >   | status           | queued                                                                         |
    >   | tags             |                                                                                |
    >   | updated_at       | 2023-07-21T03:14:26Z                                                           |
    >   | visibility       | shared                                                                         |
    >   +------------------+--------------------------------------------------------------------------------+

    #
    # Yay - image installed :-)
    #

# -----------------------------------------------------
# -----------------------------------------------------
# Create the initial Kubernetes in Docker (KinD) cluster.
#[root@bootstrap]

    kindclustername=bootstrap
    kindclusterfull=${kindclustername:?}-$(date '+%Y%m%d')
    kindclusterpath=/opt/aglais/${kindclustername:?}
    kindclusterconf=${kindclusterpath:?}/${kindclusterfull:?}-kubeconfig.yml

    mkdir -p "${kindclusterpath}"

    kind create cluster \
        --name "${kindclusterfull:?}" \
        --kubeconfig "${kindclusterconf:?}"

    >   Creating cluster "bootstrap-20230721" ...
    >    ✓ Ensuring node image (kindest/node:v1.25.3) 🖼
    >    ✓ Preparing nodes 📦
    >    ✓ Writing configuration 📜
    >    ✓ Starting control-plane 🕹️
    >    ✓ Installing CNI 🔌
    >    ✓ Installing StorageClass 💾
    >   Set kubectl context to "kind-bootstrap-20230721"
    >   ....
    >   ....


# -----------------------------------------------------
# Install the Openstack ClusterAPI components.
#[root@bootstrap]

    clusterctl init \
        --kubeconfig "${kindclusterconf:?}" \
        --infrastructure openstack

    >   Fetching providers
    >   Installing cert-manager Version="v1.12.2"
    >   Waiting for cert-manager to be available...
    >   Installing Provider="cluster-api" Version="v1.4.4" TargetNamespace="capi-system"
    >   Installing Provider="bootstrap-kubeadm" Version="v1.4.4" TargetNamespace="capi-kubeadm-bootstrap-system"
    >   Installing Provider="control-plane-kubeadm" Version="v1.4.4" TargetNamespace="capi-kubeadm-control-plane-system"
    >   Installing Provider="infrastructure-openstack" Version="v0.7.3" TargetNamespace="capo-system"
    >   ....
    >   ....


# -----------------------------------------------------
# Install the StackHPC Helm charts.
#[root@bootstrap]

    helm repo add \
        capi \
        https://stackhpc.github.io/capi-helm-charts

    helm repo add \
        capi-addons \
        https://stackhpc.github.io/cluster-api-addon-provider

    helm upgrade \
        --kubeconfig "${kindclusterconf:?}" \
        cluster-api-addon-provider \
        capi-addons/cluster-api-addon-provider \
            --install \
            --version "0.1.0"

    >   Release "cluster-api-addon-provider" does not exist. Installing it now.
    >   NAME: cluster-api-addon-provider
    >   LAST DEPLOYED: Fri Jul 21 03:27:45 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None


# -----------------------------------------------------
# Wait for the StackHPC components to be ready.
#[root@bootstrap]

    api_resources_check()
        {
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            api-resources \
                --api-group addons.stackhpc.com \
        | awk '
            BEGIN {
                found = 0
                }
            {
            if ($1 == "helmreleases" || $1 == "manifests") {
                found++
                }
            }
            END {
                if (found == 2) {
                    print "match"
                    }
                else {
                    print "no match"
                    }
                }
            '
        }

    loop=120
    while [[ ${loop} > 0 ]]
    do
        result=$(
            api_resources_check
            )

        echo "Loop [${loop}] [${result}]"

        if [[ "${result}" == "match" ]]
        then
            break
        else
            sleep 1
            ((loop--))
        fi
    done

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        api-resources \
            --api-group addons.stackhpc.com

    >   NAME           SHORTNAMES   APIVERSION                     NAMESPACED   KIND
    >   helmreleases                addons.stackhpc.com/v1alpha1   true         HelmRelease
    >   manifests                   addons.stackhpc.com/v1alpha1   true         Manifests

# -----------------------------------------------------
# Deploy our target cluster.
#[root@bootstrap]

    workclustername=gaia-dmp-one
    workclusterfull=${workclustername:?}-$(date '+%Y%m%d')

    workclusterpath=/opt/aglais/${workclustername:?}
    workclustertext=${workclusterpath:?}/${workclusterfull:?}.txt
    workclusterconf=${workclusterpath:?}/${workclusterfull:?}-kubeconfig.yml

    helm upgrade \
        --kubeconfig "${kindclusterconf:?}" \
        "${workclusterfull:?}" \
        capi/openstack-cluster \
            --install \
            --version "0.1.0" \
            --values '/opt/aglais/clusterapi-config.yml' \
            --values '/opt/aglais/openstack-clouds.yaml'

    >   Release "gaia-dmp-one-20230721" does not exist. Installing it now.
    >   NAME: gaia-dmp-one-20230721
    >   LAST DEPLOYED: Fri Jul 21 03:28:57 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None


# -----------------------------------------------------
# Watch the events log.
#[root@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get events \
            --watch

    >   LAST SEEN   TYPE      REASON                    OBJECT                                                        MESSAGE
    >   5m4s        Normal    Starting                  node/bootstrap-20230721-control-plane                         Starting kubelet.
    >   5m4s        Normal    NodeHasSufficientMemory   node/bootstrap-20230721-control-plane                         Node bootstrap-20230721-control-plane status is now: NodeHasSufficientMemory
    >   5m4s        Normal    NodeHasNoDiskPressure     node/bootstrap-20230721-control-plane                         Node bootstrap-20230721-control-plane status is now: NodeHasNoDiskPressure
    >   5m4s        Normal    NodeHasSufficientPID      node/bootstrap-20230721-control-plane                         Node bootstrap-20230721-control-plane status is now: NodeHasSufficientPID
    >   5m4s        Normal    NodeAllocatableEnforced   node/bootstrap-20230721-control-plane                         Updated Node Allocatable limit across pods
    >   4m53s       Normal    Starting                  node/bootstrap-20230721-control-plane                         Starting kubelet.
    >   ....
    >   ....

    >   ....
    >   0s          Warning   ControlPlaneUnhealthy           kubeadmcontrolplane/gaia-dmp-one-20230721-control-plane       Waiting for control plane to pass preflight checks to continue reconciliation: [machine gaia-dmp-one-20230721-control-plane-nxsnp does not have APIServerPodHealthy condition, machine gaia-dmp-one-20230721-control-plane-nxsnp does not have ControllerManagerPodHealthy condition, machine gaia-dmp-one-20230721-control-plane-nxsnp does not have SchedulerPodHealthy condition, machine gaia-dmp-one-20230721-control-plane-nxsnp does not have EtcdPodHealthy condition, machine gaia-dmp-one-20230721-control-plane-nxsnp does not have EtcdMemberHealthy condition]
    >   0s          Warning   BackOff                         pod/gaia-dmp-one-20230721-autoscaler-5c674b557d-fwhfk         Back-off restarting failed container
    >   ....

    #
    # The Heml chart still fails to create our VMs.
    # Not found out how to debug these charts easily yet ..
    #

# -----------------------------------------------------
# -----------------------------------------------------
# Login to our client and check the images.
#[user@desktop]

    podman exec \
        --tty \
        --interactive \
        'ansibler-blue' \
            'bash'

    /deployments/openstack/bin/list-all.sh \
        "${cloudname:?}"

        #
        # Again, everything apart from the VMs.
        #

    openstack \
        --os-cloud "${cloudname:?}" \
        image list

    >   +--------------------------------------+------------------------------------------------+--------+
    >   | ID                                   | Name                                           | Status |
    >   +--------------------------------------+------------------------------------------------+--------+
    >   | 0f242b58-0563-46c5-b357-64b46cf46030 | AlmaLinux-8.5-20211119                         | active |
    >   | ........                             | ........                                       | active |
    >   | ........                             | ........                                       | active |
    >   | 6ac13e0f-fee8-4cfc-9b88-fe94b3237f9a | gaia-dmp-ubuntu-2004-kube-v1.25.4              | active |
    >   | 8c05a17d-9f49-4b6e-bf37-ba0382652210 | gaia-dmp-ubuntu-2004-kube-v1.25.4              | queued |
    >   +--------------------------------------+------------------------------------------------+--------+

    #
    # OK, two images with the same name ... is broken.
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        image show \
            '6ac13e0f-fee8-4cfc-9b88-fe94b3237f9a'

    >   +------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    >   | Field            | Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
    >   +------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    >   | checksum         | 225a4fec21b30be3fbb4121554baa8f4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    >   | container_format | bare                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    >   | created_at       | 2023-07-21T03:14:26Z                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    >   | disk_format      | qcow2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
    >   | file             | /v2/images/6ac13e0f-fee8-4cfc-9b88-fe94b3237f9a/file                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    >   | id               | 6ac13e0f-fee8-4cfc-9b88-fe94b3237f9a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    >   | min_disk         | 0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
    >   | min_ram          | 0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
    >   | name             | gaia-dmp-ubuntu-2004-kube-v1.25.4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
    >   | owner            | e918a13fed2648758175a15fac083569                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    >   | properties       | direct_url='rbd://a900cf30-f8a3-42bf-98d6-af7ce92f1a1a/arcus-images/6ac13e0f-fee8-4cfc-9b88-fe94b3237f9a/snap', locations='[{'url': 'rbd://a900cf30-f8a3-42bf-98d6-af7ce92f1a1a/arcus-images/6ac13e0f-fee8-4cfc-9b88-fe94b3237f9a/snap', 'metadata': {'store': 'rbd'}}]', os_hash_algo='sha512', os_hash_value='3b0009effa8127cfa60053e7158113e7498d3fcdb4f87fae18329854d808ec3c029fe36fbc53d03e9190fa3fa2f5d036fe80f5db8c2421518e41e097d6888961', os_hidden='False', owner_specified.openstack.md5='', owner_specified.openstack.object='images/gaia-dmp-ubuntu-2004-kube-v1.25.4', owner_specified.openstack.sha256='', stores='rbd' |
    >   | protected        | False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
    >   | schema           | /v2/schemas/image                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
    >   | size             | 4441047040                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
    >   | status           | active                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
    >   | tags             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
    >   | updated_at       | 2023-07-21T03:16:35Z                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    >   | virtual_size     | 21474836480                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    >   | visibility       | shared                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
    >   +------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        image show \
            '8c05a17d-9f49-4b6e-bf37-ba0382652210'

    >   +------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    >   | Field            | Value                                                                                                                                                                                 |
    >   +------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    >   | container_format | bare                                                                                                                                                                                  |
    >   | created_at       | 2023-07-21T01:29:03Z                                                                                                                                                                  |
    >   | disk_format      | qcow2                                                                                                                                                                                 |
    >   | file             | /v2/images/8c05a17d-9f49-4b6e-bf37-ba0382652210/file                                                                                                                                  |
    >   | id               | 8c05a17d-9f49-4b6e-bf37-ba0382652210                                                                                                                                                  |
    >   | min_disk         | 0                                                                                                                                                                                     |
    >   | min_ram          | 0                                                                                                                                                                                     |
    >   | name             | gaia-dmp-ubuntu-2004-kube-v1.25.4                                                                                                                                                     |
    >   | owner            | e918a13fed2648758175a15fac083569                                                                                                                                                      |
    >   | properties       | locations='[]', os_hidden='False', owner_specified.openstack.md5='', owner_specified.openstack.object='images/gaia-dmp-ubuntu-2004-kube-v1.25.4', owner_specified.openstack.sha256='' |
    >   | protected        | False                                                                                                                                                                                 |
    >   | schema           | /v2/schemas/image                                                                                                                                                                     |
    >   | status           | queued                                                                                                                                                                                |
    >   | tags             |                                                                                                                                                                                       |
    >   | updated_at       | 2023-07-21T01:51:42Z                                                                                                                                                                  |
    >   | visibility       | shared                                                                                                                                                                                |
    >   +------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

    #
    # ... and because Murphy, the Helm charts are trying to use the wrong one ?
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        image delete \
            '8c05a17d-9f49-4b6e-bf37-ba0382652210'


# -----------------------------------------------------
# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    /deployments/openstack/bin/delete-all.sh \
        "${cloudname:?}"

    #
    # Still has problems with undeletable shares.
    #


# -----------------------------------------------------
# Initialise our status file.
#[root@ansibler]

    /deployments/cluster-api/bootstrap/bin/init-status.sh \
        "${cloudname:?}"

    >   aglais:
    >     deployment:
    >       type: cluster-api
    >       name: iris-gaia-blue-20230721
    >       date: 20230721T035112
    >     openstack:
    >       cloud:
    >         name: iris-gaia-blue
    >       user:
    >         id: 5fa0c97a6dd14e01a3c7d91dad5c6b17
    >         name: dmorris_gaia
    >       project:
    >         id: e918a13fed2648758175a15fac083569
    >         name: iris-gaia-blue


# -----------------------------------------------------
# Deploy our bootstrap node.
#[root@ansibler]

    ansible-playbook \
        --inventory \
        '/deployments/cluster-api/bootstrap/ansible/config/inventory.yml' \
        '/deployments/cluster-api/bootstrap/ansible/00-create-all.yml'

    >   ....
    >   ....


# -----------------------------------------------------
# Login to our bootstrap node as root.
#[root@ansibler]

    ssh root@bootstrap


# -----------------------------------------------------
# Create the initial Kubernetes in Docker (KinD) cluster.
#[root@bootstrap]

    kindclustername=bootstrap
    kindclusterfull=${kindclustername:?}-$(date '+%Y%m%d')
    kindclusterpath=/opt/aglais/${kindclustername:?}
    kindclusterconf=${kindclusterpath:?}/${kindclusterfull:?}-kubeconfig.yml

    mkdir -p "${kindclusterpath}"

    kind create cluster \
        --name "${kindclusterfull:?}" \
        --kubeconfig "${kindclusterconf:?}"

    >   Creating cluster "bootstrap-20230721" ...
    >    ✓ Ensuring node image (kindest/node:v1.25.3) 🖼
    >    ✓ Preparing nodes 📦
    >    ✓ Writing configuration 📜
    >    ✓ Starting control-plane 🕹️
    >    ✓ Installing CNI 🔌
    >    ✓ Installing StorageClass 💾
    >   Set kubectl context to "kind-bootstrap-20230721"
    >   ....
    >   ....


# -----------------------------------------------------
# Install the Openstack ClusterAPI components.
#[root@bootstrap]

    clusterctl init \
        --kubeconfig "${kindclusterconf:?}" \
        --infrastructure openstack

    >   Fetching providers
    >   Installing cert-manager Version="v1.12.2"
    >   Waiting for cert-manager to be available...
    >   Installing Provider="cluster-api" Version="v1.4.4" TargetNamespace="capi-system"
    >   Installing Provider="bootstrap-kubeadm" Version="v1.4.4" TargetNamespace="capi-kubeadm-bootstrap-system"
    >   Installing Provider="control-plane-kubeadm" Version="v1.4.4" TargetNamespace="capi-kubeadm-control-plane-system"
    >   Installing Provider="infrastructure-openstack" Version="v0.7.3" TargetNamespace="capo-system"
    >   ....
    >   ....


# -----------------------------------------------------
# Install the StackHPC Helm charts.
#[root@bootstrap]

    helm repo add \
        capi \
        https://stackhpc.github.io/capi-helm-charts

    helm repo add \
        capi-addons \
        https://stackhpc.github.io/cluster-api-addon-provider

    helm upgrade \
        --kubeconfig "${kindclusterconf:?}" \
        cluster-api-addon-provider \
        capi-addons/cluster-api-addon-provider \
            --install \
            --version "0.1.0"

    >   Release "cluster-api-addon-provider" does not exist. Installing it now.
    >   NAME: cluster-api-addon-provider
    >   LAST DEPLOYED: Fri Jul 21 03:56:42 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None


# -----------------------------------------------------
# Wait for the StackHPC components to be ready.
#[root@bootstrap]

    api_resources_check()
        {
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            api-resources \
                --api-group addons.stackhpc.com \
        | awk '
            BEGIN {
                found = 0
                }
            {
            if ($1 == "helmreleases" || $1 == "manifests") {
                found++
                }
            }
            END {
                if (found == 2) {
                    print "match"
                    }
                else {
                    print "no match"
                    }
                }
            '
        }

    loop=120
    while [[ ${loop} > 0 ]]
    do
        result=$(
            api_resources_check
            )

        echo "Loop [${loop}] [${result}]"

        if [[ "${result}" == "match" ]]
        then
            break
        else
            sleep 1
            ((loop--))
        fi
    done

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        api-resources \
            --api-group addons.stackhpc.com

    >   NAME           SHORTNAMES   APIVERSION                     NAMESPACED   KIND
    >   helmreleases                addons.stackhpc.com/v1alpha1   true         HelmRelease
    >   manifests                   addons.stackhpc.com/v1alpha1   true         Manifests


# -----------------------------------------------------
# Deploy our target cluster.
#[root@bootstrap]

    workclustername=gaia-dmp-one
    workclusterfull=${workclustername:?}-$(date '+%Y%m%d')

    workclusterpath=/opt/aglais/${workclustername:?}
    workclustertext=${workclusterpath:?}/${workclusterfull:?}.txt
    workclusterconf=${workclusterpath:?}/${workclusterfull:?}-kubeconfig.yml

    helm upgrade \
        --kubeconfig "${kindclusterconf:?}" \
        "${workclusterfull:?}" \
        capi/openstack-cluster \
            --install \
            --version "0.1.0" \
            --values '/opt/aglais/clusterapi-config.yml' \
            --values '/opt/aglais/openstack-clouds.yaml'

    >   Release "gaia-dmp-one-20230721" does not exist. Installing it now.
    >   NAME: gaia-dmp-one-20230721
    >   LAST DEPLOYED: Fri Jul 21 03:57:21 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None


# -----------------------------------------------------
# Watch the events log.
#[root@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get events \
            --watch

    >   LAST SEEN   TYPE      REASON                          OBJECT                                                        MESSAGE
    >   2m12s       Normal    Starting                        node/bootstrap-20230721-control-plane                         Starting kubelet.
    >   ....
    >   ....
    >   0s          Normal    Successfulcreateserver          openstackmachine/gaia-dmp-one-20230721-control-plane-9096de31-zm4h7   Created server gaia-dmp-one-20230721-control-plane-9096de31-zm4h7 with id 42c31555-538d-437f-a5c5-7b98937d676b
    >   ....
    >   ....



# -----------------------------------------------------
# -----------------------------------------------------
# Watch the events log.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
            server list

    >   +--------------------------------------+----------------------------------------------------+--------+-----------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | ID                                   | Name                                               | Status | Networks                                                              | Image                             | Flavor               |
    >   +--------------------------------------+----------------------------------------------------+--------+-----------------------------------------------------------------------+-----------------------------------+----------------------+
    >   | 8e285723-d145-4baf-a91d-4a94f815fb23 | gaia-dmp-one-20230721-control-plane-9096de31-r6ps2 | ACTIVE | k8s-clusterapi-cluster-default-gaia-dmp-one-20230721=192.168.3.182    | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 42c31555-538d-437f-a5c5-7b98937d676b | gaia-dmp-one-20230721-control-plane-9096de31-zm4h7 | ACTIVE | k8s-clusterapi-cluster-default-gaia-dmp-one-20230721=192.168.3.175    | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 9311a0c8-b29a-45c3-992a-393a36702c2f | gaia-dmp-one-20230721-md-0-9096de31-25hzk          | ACTIVE | k8s-clusterapi-cluster-default-gaia-dmp-one-20230721=192.168.3.45     | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 3b811a13-3800-4174-b10f-373a05f2efe4 | gaia-dmp-one-20230721-md-0-9096de31-f75dr          | ACTIVE | k8s-clusterapi-cluster-default-gaia-dmp-one-20230721=192.168.3.100    | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 93fb1892-4429-4699-a261-a3a602ad30bf | gaia-dmp-one-20230721-md-0-9096de31-4m4nx          | ACTIVE | k8s-clusterapi-cluster-default-gaia-dmp-one-20230721=192.168.3.127    | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 01cbf736-37d4-40ba-ab84-2e2362e95803 | gaia-dmp-one-20230721-control-plane-9096de31-pvc7x | ACTIVE | k8s-clusterapi-cluster-default-gaia-dmp-one-20230721=192.168.3.170    | gaia-dmp-ubuntu-2004-kube-v1.25.4 | gaia.vm.cclake.4vcpu |
    >   | 4f6d82e6-0a6f-4aab-a9d2-7eb0cca85953 | iris-gaia-blue-20230721-bootstrap                  | ACTIVE | iris-gaia-blue-20230721-internal-network=10.10.2.245, 128.232.226.106 | Fedora-34.1.2                     | gaia.vm.cclake.2vcpu |
    >   +--------------------------------------+----------------------------------------------------+--------+-----------------------------------------------------------------------+-----------------------------------+----------------------+

    #
    # Yay - looks good.
    #
    # TODO - Diagnose what is wrong with the clouds.yaml created by Ansible.
    #

# -----------------------------------------------------
# Check the addon-provider logs.
#[root@bootstrap]

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        get pods

    >   NAME                                                READY   STATUS    RESTARTS        AGE
    >   cluster-api-addon-provider-5cb78d8945-9ljxj         1/1     Running   0               12m
    >   gaia-dmp-one-20230721-autoscaler-5c674b557d-48hcf   1/1     Running   4 (6m59s ago)   12m


    podname=$(
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            get pods \
                --output json \
        | jq -r '.items[].metadata.name | select(test("cluster-api-addon-provider")) '
        )

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        logs "${podname:?}" \
            --follow

    >   ....
    >   ....



