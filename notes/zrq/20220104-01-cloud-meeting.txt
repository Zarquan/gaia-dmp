#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

Replying to John's email with comments on the 7th Jan meeting agenda.

---- ----

Hi John,

Looking forward to the meeting. Some initial thoughts on the points you suggest we could discuss in the meeting :

* Self-service creation of a (generic) Spark Science Platform

We are about half way towards implementing an on-demand Spark platform like this. Unfortunately in 2021 we got diverted into diagnosing and solving a number of low level technical issues, IO bandwidth being the main one, which means we haven't been able to make as much progress on the Spark platform as we would have liked.

What we have learned in 2021 is that implementing a performant platform that meets the kind of requirements that the machine learning use cases our users are developing depends on all the rest of your points being in place. So yes to all of them.

* Using built in platform monitoring to understand bottlenecks

Yes please.
Worked examples of Prometheus and Grafana configurations, and direct access to the live results, would be a huge benefit to developers, enabling them to diagnose and solve issues quickly and easily. Emailing the system administrators to request a screen shot of yesterday's Grafana output is not a viable solution.

* Exposing NVMe storage and GPUs to the above platforms

Yes please.
Would this be part of an IRIS project?
Would the Gaia development team be able to have time on this platform to learn how migrate our system on to it.
Would we be able to reserve time on this platform using Blazar ?

* Hyperconverged shared storage, using the above NVMe storage

Yes please.
We believe this is absolutely vital to meeting the high IO bandwidth requirements resulting from training machine learning algorithms.

We have requested Hyperconverged direct attached storage as part of our 2022 IRIS resource allocation. This will likely take the form of 4Tbyte SSD drives installed on each of the physical hypervisor machines. Assuming the funding is agreed, would it be possible to implement this on the Arcus cloud ?

From what we have learned both the RAL and Sommerville Openstack systems provide Tbytes of direct attached storage on the hypervisor machines. If it is unlikely to be available on the Acrus cloud, what options do we have for moving part of our 2022 IRIS allocation to another cloud platform?

We were already thinking of asking for part of our 2022 allocation to be on one of the other cloud platforms enabling us to work on making our deployment portable across different platforms. Can you advise us on who to contact to make this happen ?

* Monitoring and benchmarks to help compare performance with a reference

Yes please.
We are aiming to make our Spark deployment as portable as possible, with performance metrics for example use cases
that could be used to compare performance on different platforms and configurations.

Any help with developing monitoring tools and examples that could help with this would be appreciated.

* Reserve future compute capacity, using Blazar

Yes please.
We believe this is absolutely vital to the development of a generic on-demand analysis platform. Being able to scale the number and size of clusters based on predicted daily or weekly demand is core to being able to offer a scalable platform that can cope with the expected peaks in demand.

Simple example is an analysis platform used as part of a teaching course. During a workshop 20 or 30 people will all want to run the same analysis package on the same data at the same time. Outside or that, there may only be a few people using the system. Without a reservation system like Balzar, a user facing service has to permanently reserve enough resources to meet the expected x30 peak demand. If they were confident of being able to reserve more resources when needed, a user facing service could start with a much lower level background allocation and only reserve additional resources when needed.

We are looking forward to working with you to integrate Blazar with a booking system for our analysis platform. We think developing this should be the priority for 2022.



