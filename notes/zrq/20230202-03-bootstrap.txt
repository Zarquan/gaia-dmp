#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Install K8s on O7k

        prerequisites:
          - bootstrap VM with public network access
          - Podman
            https://podman.io/
          - Kind
            https://kind.sigs.k8s.io/
            https://kubernetes.io/docs/tasks/tools/#kind
          - Kubectl
            https://kubernetes.io/docs/tasks/tools/#kubectl
          - Kubeadm
            https://kubernetes.io/docs/tasks/tools/#kubeadm

    Result:

        Work in progress ...

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    #
    # Live is red, selecting green for experimenting.
    #

    agcolour=green

    source "${HOME:?}/aglais.env"

    clientname=ansibler-${agcolour}
    cloudname=iris-gaia-${agcolour}

    podman run \
        --rm \
        --tty \
        --interactive \
        --name     "${clientname:?}" \
        --hostname "${clientname:?}" \
        --publish  "3000:3000" \
        --env "cloudname=${cloudname:?}" \
        --env "configname=${configname:?}" \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK:?}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        ghcr.io/wfau/atolmis/ansible-client:2022.07.25 \
        bash

    >   ....
    >   ....


# -----------------------------------------------------
# Add YAML editor role.
#[root@ansibler]

    ansible-galaxy install kwoodson.yedit

    >   Starting galaxy role install process
    >   - downloading role 'yedit', owned by kwoodson
    >   - downloading role from https://github.com/kwoodson/ansible-role-yedit/archive/master.tar.gz
    >   - extracting kwoodson.yedit to /root/.ansible/roles/kwoodson.yedit
    >   - kwoodson.yedit (master) was installed successfully


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

    >   real    2m3.014s
    >   user    0m59.643s
    >   sys     0m6.474s


# -----------------------------------------------------
# Settings ...
#[root@ansibler]

    deployname=${cloudname:?}-$(date '+%Y%m%d')
    deploydate=$(date '+%Y%m%dT%H%M%S')

    statusyml='/opt/aglais/aglais-status.yml'
    if [ ! -e "$(dirname ${statusyml})" ]
    then
        mkdir "$(dirname ${statusyml})"
    fi
    rm -f "${statusyml}"
    touch "${statusyml}"

    yq eval \
        --inplace \
        "
        .aglais.deployment.type = \"cluster-api\"   |
        .aglais.deployment.name = \"${deployname}\" |
        .aglais.deployment.date = \"${deploydate}\" |
        .aglais.openstack.cloud.name = \"${cloudname}\"
        " "${statusyml}"


# -----------------------------------------------------
# Create our bootstrap components.
#[root@ansibler]

    inventory=/deployments/cluster-api/bootstrap/ansible/config/inventory.yml

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/01-create-keypair.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/02-create-network.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/03-create-bootstrap.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/04-local-config.yml'

    >   ....
    >   ....


# -----------------------------------------------------
# Check our local config.
#[root@ansibler]

    cat /opt/aglais/aglais-status.yml

    >   aglais:
    >     deployment:
    >       date: 20230203T041031
    >       name: iris-gaia-green-20230203
    >       type: cluster-api
    >     openstack:
    >       cloud:
    >         name: iris-gaia-green
    >       keypairs:
    >         team:
    >           fingerprint: 2e:84:98:98:df:70:06:0e:4c:ed:bd:d4:d6:6b:eb:16
    >           id: iris-gaia-green-20230203-keypair
    >           name: iris-gaia-green-20230203-keypair
    >       networks:
    >         internal:
    >           network:
    >             id: c657212a-cc84-44c8-8859-14b102ab7aa6
    >             name: iris-gaia-green-20230203-internal-network
    >           router:
    >             id: 16b404a3-5d35-449d-b435-db99f60d5966
    >             name: iris-gaia-green-20230203-internal-router
    >           subnet:
    >             cidr: 10.10.0.0/16
    >             id: c0f10f13-2979-47d7-ab86-f0056ed16bbc
    >             name: iris-gaia-green-20230203-internal-subnet
    >       servers:
    >         bootstrap:
    >           float:
    >             external: 128.232.227.214
    >             id: 769b828a-e49c-4b32-b4c7-58eecb48aa18
    >             internal: 10.10.3.3
    >           server:
    >             address:
    >               ipv4: 10.10.3.3
    >             flavor:
    >               name: gaia.vm.cclake.2vcpu
    >             hostname: bootstrap
    >             id: 0436ff91-aa3c-407c-919b-de30c131eefc
    >             image:
    >               id: e5c23082-cc34-4213-ad31-ff4684657691
    >               name: Fedora-34.1.2
    >             name: iris-gaia-green-20230203-bootstrap


# -----------------------------------------------------
# SSH test.
#[root@ansibler]

    ssh bootstrap \
        '
        date
        hostname
        '

    >   Fri Feb  3 04:12:26 UTC 2023
    >   iris-gaia-green-20230203-bootstrap


# -----------------------------------------------------
# Install Podman.
#[root@ansibler]

    ssh bootstrap \
        '
        sudo dnf install -y podman
        '

    >   ....
    >   ....


# -----------------------------------------------------
# Install kubectl.
# https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management
#[root@ansibler]

    ssh bootstrap \
        '

cat > /tmp/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

        sudo chown 'root:root' '/tmp/kubernetes.repo'
        sudo mv '/tmp/kubernetes.repo' '/etc/yum.repos.d/kubernetes.repo'

        sudo dnf install -y 'kubectl'

        '


# -----------------------------------------------------
# Install kind.
# https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries
#[root@ansibler]

    ssh bootstrap \
        '
        kversion=0.17.0
        kfile=kind-${kversion}
        ktemp=/tmp/${kfile}

        curl --location --output "${ktemp}" "https://kind.sigs.k8s.io/dl/v${version}/kind-linux-amd64"
        pushd /usr/local/bin
            sudo mv "${ktemp}" .
            sudo chown 'root:root' "${kfile}"
            sudo chmod 'u=rwx,g=rx,o=rx' "${kfile}"
            sudo ln -s "${kfile}" "kind"
        popd

        sudo dnf install -y firewalld
        sudo systemctl start firewalld

        sudo vi /etc/firewalld/firewalld.conf
        sudo sed -i -e 's/^FirewallBackend=.*$/FirewallBackend=iptables/' '/etc/firewalld/firewalld.conf'

cat > '/tmp/kind-podman' << EOF
#!/bin/sh
export KIND_EXPERIMENTAL_PROVIDER=podman
EOF
        sudo chown 'root:root' '/tmp/kind-podman'
        sudo mv '/tmp/kind-podman' '/etc/profile.d/kind-podman'

        source '/etc/profile.d/kind-podman'
        '


# -----------------------------------------------------
# Create a cluster.
# https://kind.sigs.k8s.io/docs/user/quick-start/#creating-a-cluster
#[root@ansibler]

    ssh bootstrap \
        '
        kind create cluster
        '

    >   using podman due to KIND_EXPERIMENTAL_PROVIDER
    >   enabling experimental podman provider
    >   Cgroup controller detection is not implemented for Podman. If you see cgroup-related errors, you might need to set systemd property "Delegate=yes", see https://kind.sigs.k8s.io/docs/user/rootless/
    >   Creating cluster "kind" ...
    >    âœ“ Ensuring node image (kindest/node:v1.25.3) ðŸ–¼
    >    âœ— Preparing nodes ðŸ“¦
    >   ERROR: failed to create cluster: could not find a log line that matches "Reached target .*Multi-User System.*|detected cgroup v1"


# -----------------------------------------------------
# Update our system config.
# https://kind.sigs.k8s.io/docs/user/rootless/#host-requirements
#[root@ansibler]

    ssh bootstrap \
        '

cat > /tmp/delegate.conf << EOF
[Service]
Delegate=yes
EOF

        sudo mkdir '/etc/systemd/user@.service.d'

        sudo chown 'root:root' '/tmp/delegate.conf'
        sudo mv '/tmp/delegate.conf' '/etc/systemd/user@.service.d/delegate.conf'

        sudo systemctl daemon-reload
        sudo systemctl status

        '

# -----------------------------------------------------
# Create a cluster, with logs.
# https://github.com/kubernetes-sigs/kind/pull/2478#issuecomment-1214656908
#[root@ansibler]

    ssh bootstrap \
        '
        kind create cluster --retain
        kind export logs
        kind delete cluster
        '

    >   enabling experimental podman provider
    >   Exporting logs for cluster "kind" to:
    >   /tmp/2167801430
    >   ERROR: [
    >       command "
    >           podman exec --privileged kind-control-plane sh -c 'tar --hard-dereference -C /var/log/ -chf - . || (r=$?; [ $r -eq 1 ] || exit $r)'
    >           "
    >       failed with error: exit status 255,
    >           [
    >           command "podman exec --privileged kind-control-plane crictl images" failed with error: exit status 255,
    >           command "podman exec --privileged kind-control-plane cat /kind/version" failed with error: exit status 255,
    >           command "podman exec --privileged kind-control-plane journalctl --no-pager" failed with error: exit status 255,
    >           command "podman exec --privileged kind-control-plane journalctl --no-pager -u kubelet.service" failed with error: exit status 255,
    >           command "podman exec --privileged kind-control-plane journalctl --no-pager -u containerd.service" failed with error: exit status 255
    >           ]
    >       ]



