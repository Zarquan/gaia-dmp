#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: [{"name": "ChatGPT","contribution": {"value": 0,"units": "%"}}]
#


    Target:

        Success

    Result:

        Work in progress ...

# -----------------------------------------------------
# -----------------------------------------------------

    #
    # Delete and create everything up to 'Deploy our target cluster'.
    # notes/zrq/20230722-01-bootstrap.txt
    #

    >   ....
    >   ....


# -----------------------------------------------------
# Launch another terminal.
# -----------------------------------------------------
# Connect to the client container.
#[user@desktop]

    podman exec \
        --tty \
        --interactive \
        ansibler-blue \
            bash

    ssh root@bootstrap

    statusyml=/opt/aglais/aglais-status.yml
    touch "${statusyml:?}"

    kindclustername=$(
        yq '
           .aglais.kubernetes.kind.name
           ' "${statusyml:?}"
        )

    kindclusterconf=$(
        yq '
           .aglais.kubernetes.kind.conf
           ' "${statusyml:?}"
        )

# -----------------------------------------------------
# Follow the addon-provider logs.
#[root@bootstrap]

    podname=$(
        kubectl \
            --kubeconfig "${kindclusterconf:?}" \
            get pods \
                --output json \
        | jq -r '.items[].metadata.name | select(test("cluster-api-addon-provider")) '
        )

    kubectl \
        --kubeconfig "${kindclusterconf:?}" \
        logs "${podname:?}" \
            --follow

    >   ....
    >   ....


# -----------------------------------------------------
# Back to the main terminal.
# -----------------------------------------------------
# Enable NGINX ingress, monitoring and dashboard.
#[root@bootstrap]

    yq eval \
        --inplace \
        "
        .addons.ingress.enabled = true,
        .addons.monitoring.enabled = true,
        .addons.kubernetesDashboard.enabled = true
        " \
        /opt/aglais/clusterapi-config.yml


# -----------------------------------------------------
# Deploy our target cluster.
#[root@bootstrap]

    workclusterbase=gaia-dmp-one
    workclustername=${workclusterbase:?}-$(date '+%Y%m%d')

    workclusterpath=/opt/aglais/${workclusterbase:?}
    workclusterconf=${workclusterpath:?}/${workclustername:?}-kubeconfig.yml

    yq eval \
        --inplace \
        "
        .aglais.kubernetes.work.name = \"${workclustername}\",
        .aglais.kubernetes.work.conf = \"${workclusterconf}\"
        " \
        "${statusyml:?}"

    helm upgrade \
        --wait \
        --kubeconfig "${kindclusterconf:?}" \
        "${workclustername:?}" \
        capi/openstack-cluster \
            --install \
            --version "0.1.0" \
            --values '/opt/aglais/clusterapi-config.yml' \
            --values '/opt/aglais/openstack-clouds.yaml'

    >   ....
    >   ....


# -----------------------------------------------------
# Launch another terminal.
# -----------------------------------------------------
# Connect to the client container.
#[user@desktop]

    podman exec \
        --tty \
        --interactive \
        ansibler-blue \
            bash

    ssh root@bootstrap

    statusyml=/opt/aglais/aglais-status.yml
    touch "${statusyml:?}"

    kindclustername=$(
        yq '
           .aglais.kubernetes.kind.name
           ' "${statusyml:?}"
        )

    kindclusterconf=$(
        yq '
           .aglais.kubernetes.kind.conf
           ' "${statusyml:?}"
        )

    workclustername=$(
        yq '
           .aglais.kubernetes.work.name
           ' "${statusyml:?}"
        )

    workclusterconf=$(
        yq '
           .aglais.kubernetes.work.conf
           ' "${statusyml:?}"
        )

# -----------------------------------------------------
# Watch the cluster status.
#[root@bootstrap]

    watch clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        describe cluster \
            "${workclustername:?}"

    >   ....
    >   ....


    clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        describe cluster \
            "${workclustername:?}"

    >   NAME                                                                      READY  SEVERITY  REASON  SINCE  MESSAGE
    >   Cluster/gaia-dmp-one-20230725                                             True                     110s
    >   ├─ClusterInfrastructure - OpenStackCluster/gaia-dmp-one-20230725
    >   ├─ControlPlane - KubeadmControlPlane/gaia-dmp-one-20230725-control-plane  True                     110s
    >   │  └─3 Machines...                                                         True                     2m57s  See gaia-dmp-one-20230725-control-plane-sbtgq, gaia-dmp-one-20230725-control-plane-sjrxd, ...
    >   └─Workers
    >       └─MachineDeployment/gaia-dmp-one-20230725-md-0                          True                     3m2s
    >           └─3 Machines...                                                       True                     3m58s  See gaia-dmp-one-20230725-md-0-5b4bc45c44xhbb95-59vxz, gaia-dmp-one-20230725-md-0-5b4bc45c44xhbb95-c6879, ...


# -----------------------------------------------------
# Back to the main terminal.
# -----------------------------------------------------
# Fetch the work cluster config.
#[root@bootstrap]

    mkdir -p $(dirname "${workclusterconf}")

    clusterctl \
        --kubeconfig "${kindclusterconf:?}" \
        get \
            kubeconfig "${workclustername:?}" \
    | tee "${workclusterconf}" \
    | yq '.'

    >   ....
    >   ....

# -----------------------------------------------------

    #
    # Ask ChatGPT for some clues ...
    #
    # Ended up here:
    # https://github.com/kubernetes/dashboard/tree/master/docs/user/accessing-dashboard
    # https://github.com/kubernetes/dashboard/tree/master/docs/user/access-control
    #
    #   Use kubectl port-forward and access Dashboard with a simple URL.
    #   ... (if you) have nginx already installed in your cluster, follow below steps:
    #
    #   (1) Find nginx installation namespace.
    #   (2) Find main nginx-ingress service name.
    #
    # Once you have all the information simply run (make sure to replace placeholders with correct names):
    #
    #   kubectl -n <nginx-namespace> port-forward svc/<nginx-service-name> 8443:443
    #

# -----------------------------------------------------
# List the services.
#[root@bootstrap]

    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        get services \
            --all-namespaces

    >   NAMESPACE                NAME                                             TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)                        AGE
    >   calico-apiserver         calico-api                                       ClusterIP      172.28.246.14    <none>            443/TCP                        61m
    >   calico-system            calico-kube-controllers-metrics                  ClusterIP      None             <none>            9094/TCP                       62m
    >   calico-system            calico-typha                                     ClusterIP      172.26.85.83     <none>            5473/TCP                       62m
    >   default                  kubernetes                                       ClusterIP      172.24.0.1       <none>            443/TCP                        64m
    >   ingress-nginx            ingress-nginx-controller                         LoadBalancer   172.28.234.140   128.232.226.216   80:30596/TCP,443:30936/TCP     61m
    >   ingress-nginx            ingress-nginx-controller-admission               ClusterIP      172.27.124.99    <none>            443/TCP                        61m
    >   kube-system              kube-dns                                         ClusterIP      172.24.0.10      <none>            53/UDP,53/TCP,9153/TCP         63m
    >   kube-system              kube-prometheus-stack-coredns                    ClusterIP      None             <none>            9153/TCP                       61m
    >   kube-system              kube-prometheus-stack-kube-controller-manager    ClusterIP      None             <none>            10257/TCP                      61m
    >   kube-system              kube-prometheus-stack-kube-etcd                  ClusterIP      None             <none>            2381/TCP                       61m
    >   kube-system              kube-prometheus-stack-kube-proxy                 ClusterIP      None             <none>            10249/TCP                      61m
    >   kube-system              kube-prometheus-stack-kube-scheduler             ClusterIP      None             <none>            10259/TCP                      61m
    >   kube-system              kube-prometheus-stack-kubelet                    ClusterIP      None             <none>            10250/TCP,10255/TCP,4194/TCP   61m
    >   kube-system              metrics-server                                   ClusterIP      172.26.168.56    <none>            443/TCP                        63m
    >   kubernetes-dashboard     kubernetes-dashboard                             ClusterIP      172.27.166.79    <none>            443/TCP                        63m
    >   monitoring-system        alertmanager-operated                            ClusterIP      None             <none>            9093/TCP,9094/TCP,9094/UDP     61m
    >   monitoring-system        kube-prometheus-stack-alertmanager               ClusterIP      172.25.137.237   <none>            9093/TCP                       61m
    >   monitoring-system        kube-prometheus-stack-grafana                    ClusterIP      172.29.153.12    <none>            80/TCP                         61m
    >   monitoring-system        kube-prometheus-stack-kube-state-metrics         ClusterIP      172.28.78.217    <none>            8080/TCP                       61m
    >   monitoring-system        kube-prometheus-stack-operator                   ClusterIP      172.25.99.68     <none>            443/TCP                        61m
    >   monitoring-system        kube-prometheus-stack-prometheus                 ClusterIP      172.28.157.164   <none>            9090/TCP                       61m
    >   monitoring-system        kube-prometheus-stack-prometheus-node-exporter   ClusterIP      172.25.217.126   <none>            9100/TCP                       61m
    >   monitoring-system        loki-stack                                       ClusterIP      172.30.105.65    <none>            3100/TCP                       63m
    >   monitoring-system        loki-stack-headless                              ClusterIP      None             <none>            3100/TCP                       63m
    >   monitoring-system        loki-stack-memberlist                            ClusterIP      None             <none>            7946/TCP                       63m
    >   monitoring-system        prometheus-operated                              ClusterIP      None             <none>            9090/TCP                       61m
    >   node-feature-discovery   node-feature-discovery-master                    ClusterIP      172.26.140.31    <none>            8080/TCP                       63m

    >   ....
    >   ingress-nginx            ingress-nginx-controller                         LoadBalancer   172.28.234.140   128.232.226.216   80:30596/TCP,443:30936/TCP     61m
    >   ingress-nginx            ingress-nginx-controller-admission               ClusterIP      172.27.124.99    <none>            443/TCP                        61m
    >   ....
    >   kubernetes-dashboard     kubernetes-dashboard                             ClusterIP      172.27.166.79    <none>            443/TCP                        63m
    >   ....


    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        --namespace "ingress-nginx" \
        port-forward \
            svc/ingress-nginx-controller \
            8443:443

    >   Forwarding from 127.0.0.1:8443 -> 8443
    >   Forwarding from [::1]:8443 -> 8443
    >   Handling connection for 8443
    >   Handling connection for 8443
    >   Handling connection for 8443
    >   ....
    >   ....

    #
    # That didn't do what we wanted ..
    #

    curl --head https://localhost:8443/

    >   HTTP/1.1 400 Bad Request
    >   Date: Tue, 25 Jul 2023 04:27:30 GMT


    #
    # Alternative method.
    #

    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        proxy

    >   Starting to serve on 127.0.0.1:8001

    #
    # Separate terminal ...
    #

    curl --insecure \
        https://localhost:8001/

    >   curl: (7) Failed to connect to localhost port 8001: Connection refused


    curl --insecure \
        https://localhost:8001/

    >   curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number

    curl --insecure \
        http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

    >   {
    >     "kind": "Status",
    >     "apiVersion": "v1",
    >     "metadata": {},
    >     "status": "Failure",
    >     "message": "no endpoints available for service \"https:kubernetes-dashboard:\"",
    >     "reason": "ServiceUnavailable",
    >     "code": 503
    >   }


    curl --insecure \
        http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/kubernetes-dashboard:/proxy/

    >   {
    >     "kind": "Status",
    >     "apiVersion": "v1",
    >     "metadata": {},
    >     "status": "Failure",
    >     "message": "no endpoints available for service \"kubernetes-dashboard:\"",
    >     "reason": "ServiceUnavailable",
    >     "code": 503
    >   }


    curl --insecure \
        http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/kubernetes-dashboard/proxy/
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "no endpoints available for service \"kubernetes-dashboard\"",
  "reason": "ServiceUnavailable",
  "code": 503
}
--END--


    curl --insecure \
        http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/

    >   <!DOCTYPE html><html lang="en" dir="ltr"><head>
    >     <meta charset="utf-8">
    >     <title>Kubernetes Dashboard</title>
    >     <link rel="icon" type="image/png" href="assets/images/kubernetes-logo.png">
    >     <meta name="viewport" content="width=device-width">
    >     ....
    >     ....



    kubectl \
        --kubeconfig "${workclusterconf:?}" \
        proxy \
            --address 0.0.0.0

    >   Starting to serve on [::]:8001


    curl --insecure \
        http://128.232.226.185:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/

        #
        # Wrong IP address.
        # kubectl proxy is running on the bootstrap node, not the ingress controller.
        #

    curl --insecure \
        https://128.232.226.213:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/

        #
        # Blocked by firewall ?
        # Added a rile to allow port 8001.
        #

    curl --insecure \
        https://128.232.226.213:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/

    >   curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number

        #
        # HTTPS request from a HTTP endpoint ?
        # https://faun.pub/digitalocean-kubernetes-and-ssl-wrong-version-number-error-for-the-requests-from-inside-a-pod-bb3a0bc83a71
        #

    #
    # From desktop ..
    curl --insecure \
        http://128.232.226.213:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/

    >   Forbidden


    #
    # From bootstrap ..
    curl --insecure \
        http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/

    >   <!DOCTYPE html><html lang="en" dir="ltr"><head>
    >     <meta charset="utf-8">
    >     <title>Kubernetes Dashboard</title>
    >     <link rel="icon" type="image/png" href="assets/images/kubernetes-logo.png">
    >     <meta name="viewport" content="width=device-width">
    >     ....
    >     ....

    #
    # From bootstrap ..
    curl --insecure \
        http://128.232.226.213:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/

    >   Forbidden

    #
    # Looks like dashboard is detecting where the request is coming from and rejecting insecure clients.
    # OMG .. why is this so hard !?
    #

#
# Try running the proxy on our desktop ...
#

    ssh root@128.232.226.213 \
        '
        hostname
        date
        '

    >   iris-gaia-blue-20230724-bootstrap
    >   Tue 25 Jul 05:23:05 UTC 2023


    scp root@128.232.226.213:/opt/aglais/gaia-dmp-one/gaia-dmp-one-20230725-kubeconfig.yml .

    >   gaia-dmp-one-20230725-kubeconfig.yml        100% 5660   309.6KB/s   00:00

    kubectl --kubeconfig gaia-dmp-one-20230725-kubeconfig.yml proxy

    >   bash: kubectl: command not found

    sudo -s

    cat > /etc/yum.repos.d/kubernetes.repo << 'EOF'
[kubernetes]
name=Kubernetes - $basearch
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-$basearch
enabled=1
gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

    dnf install kubectl

    Ctrl^D

    kubectl --kubeconfig gaia-dmp-one-20230725-kubeconfig.yml proxy

    >   Starting to serve on 127.0.0.1:8001
    >   ....


    curl --insecure \
        http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/

    >   <!DOCTYPE html><html lang="en" dir="ltr"><head>
    >     <meta charset="utf-8">
    >     <title>Kubernetes Dashboard</title>
    >     <link rel="icon" type="image/png" href="assets/images/kubernetes-logo.png">
    >     <meta name="viewport" content="width=device-width">
    >     ....
    >     ....

    firefox \
        http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:https/proxy/#/login

        #
        # Works, but it is asking for the token ..
        #

        #
        # Need a break from £$%^&*
        #


