#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#

    #
    # Emails between Roger and Nigel

    From: Roger Mor Crespo <roger.mor@pervasive-tech.com<mailto:roger.mor@pervasive-tech.com>>
    Subject: Re: Non-distributed processing launched from Zeppelin or Jupyter notebook in a Spark context
    Date: 7 June 2021 at 08:36:29 BST
    To: <nch@roe.ac.uk<mailto:nch@roe.ac.uk>>
    Cc: <eutrilla@sciops.esa.int<mailto:eutrilla@sciops.esa.int>>

    Dear Nigel (and Enrique),

    Sorry for my late answer, I moved to industry 6 month ago and I do not regularly check my old email.
    My new email is : roger.mor@pervasive-tech.com<mailto:roger.mor@pervasive-tech.com>

    Nowadays I'm working in Pervasive Technologies <https://pervasive-tech.com/>  small company working in
    Artificial Intelligence and Cloud Computing. I'm still collaborating with the team of Xavier Luri at ICCUB,
    in fact we (ICCUB and Pervaisve Technologies) are developing a project together inside the framework of
    OCRE (www.ocre-project.eu<http://www.ocre-project.eu/> ; https://www.linkedin.com/company/ocre-project/).

    It is about Gaia exploitation in Cloud Computing. The project is called Galactic RainCloudS and we got
    the best mark in the 1st open call for Cloud funding of OCRE, we will start the project within the
    following 2 months.

    Let me answer your question between lines:

    On Mon, 7 Jun 2021 at 08:31, Roger Mor <rmor@fqa.ub.edu<mailto:rmor@fqa.ub.edu>> wrote:


    ---------- Forwarded message ---------
    From: Nigel C. Hambly <nch@roe.ac.uk<mailto:nch@roe.ac.uk>>
    Date: Wed, 2 Jun 2021 at 10:50
    Subject: Non-distributed processing launched from Zeppelin or Jupyter notebook in a Spark context
    To: Roger Mor <rmor@fqa.ub.edu<mailto:rmor@fqa.ub.edu>>, Enrique Utrilla <eutrilla@sciops.esa.int<mailto:eutrilla@sciops.esa.int>>

    Hi Guys

    I have a general question that you may have come up against regarding these big data platforms that
    we’re all playing with. My question is borne out of ignorance and inexperience and it could be that
    we’ve missed something fundamental in our deployment. But here goes anyway.

    Every small thing is very complex in spark context :) I will try to answer the question.

    On our deployment of the GDAF-like environment with an Apache Zeppelin front-end talking through
    Yarn to a Spark cluster back-end, if I do something heavy outside of the Spark data frame API in
    python, it executes on the Zeppelin node and if particularly big will choke it up. In my specific
    workflow I’m creating a Pandas data object from a PySpark data frame (which I guess does a collect
    from the underlying RDD)

    Yes it makes a "full" collect, if you do something like toPandas, id there isn't enough memory it
    probably crashes. The action collects the data to the driver node and converts it to a local data
    structure, in this case a pandas dataframe. And this is the key point. Who is the driver when you
    execute from Zeppelin notebook?

    , then throwing it into another third-party package HDBSCAN. But I believe the question is generic
    in that I could be doing any non-distributed heavy lifting in scikit-learn or something.

    Is there some configuration (or some other way) of setting up the system such that such a process
    is targetted on one node of the cluster (e.g. the master perhaps) to avoid soaking the web notebook
    front-end node? Or maybe a way of coding in such a situation to achieve the same thing?

    By default the driver of a Zeppelin execution is the local node, spark.master=local[*]. You should
    be able to configure in the Zepeline configuration the "spark.master" as your desiresd node
    (eg. whatever://master:xxxx). If you choose the spark.master to be whatever://master:xxxx, this will
    be the driver but I think the jobs will not be distributed, and all computation will run in the
    specified node. There are more options if you use YARN (smaller than 3.x) then you can set up the
    spark.master as "yarn-client" this means the local will be the driver or you can set "yarn-cluster",
    with yarn-cluster the "driver program" is sended remotely to one of the nodes of the cluster, then a
    "random" node is working as driver (this option never worked correctly in GDAF) but maybe you are lucky :).
    If you use YARN 3.x then you can set "spark.master" as yarn and "spark.submit.deployMode" as cluster,
    this is equivalent to yarn-cluster and one of the nodes of the cluster should take the role of the
    driver instead of the local node.

    I hope it works for you! And if not, I'm afraid I cannot further help with this matter :(

    Tip: be sure that you are not using an inline generic configuration  of SPARK_HOME in Zeppelin
    that could supperside the configuration above (or viceversa)

    Last: If you are using mesos instead of yarn forget everything I said because then you have to set
    the spark.master as mesos://host:5050. That's all I know about Mesos.

    Well, I hope it helps.

    With best regards,

    Roger Mor Crespo -  Data Scientist

    Pervasive Technologies
    www.pervasive-tech.com





