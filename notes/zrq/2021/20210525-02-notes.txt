#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#






John Garbutt  4:57 PM
As a heads up, been chatting with some people about getting some NVMe based (well faster) storage, for your needs.
4:58
Its part of an exascale project, but getting a real user that is IO bound some real help, seems like a great way to validate some of the thinking on how best to integrate these for AI/ML workloads.
4:58
Does that sound useful? (And I know you probably just want some better flavors too, at this point)
4:59
We should have a meeting soon on how we can help you more with the upcoming digital assets (which will be used at both Cambridge and Edingburgh)

Dave Morris  12:16 AM
NVMe is nice and fast, but very expensive

Dave Morris  12:23 AM
+1 to meeting to discuss the options for future direction

Dave Morris  5:57 PM
@John Garbutt We should be specific about what problem the faster storage would be addressing.
:+1:
1


John Garbutt  6:38 PM
My idea is using Cinder volumes backed by NVMe over fabrics, such that you get to choose where to deploy your most expensive storage, as and when it is required, overcoming the current fixed ratio of fast local storage to CPU to memory, etc.
6:39
It may not work in context, but it the idea is you could create a dedicated, and performance parallel filesystem on NVMe over fabric, on demand, using something like BeeGFS or similar.

Dave Morris  6:46 PM
What problem is this solving ?

John Garbutt  9:18 AM
Well the hope is it can be used to remove IO bottlenecks for some applications, without the massive expense of putting an NVMe in every compute node, which is likely a complete waste with the current mix of applications.
9:19
One target will be exploring the AI/ML benchmarks from STFC folks, some of those are expected to be somewhat limited by I/O throughput with the current IRIS resources

Dave Morris  10:38 AM
Is NVMe worth the extra cost compared to SSD ?

John Garbutt  11:28 AM
Depends on the workload, it can be 300MB/s vs 3000MB/s from a single drive, so for some use cases NVMe is a lot cheaper way to get the required performance. There is cheaper NVMe vs crazy expensive NVMe (Optane, etc). I am thinking about the cheaper NVMe here. The main difference is Optane gives you very low latency, which in many cases isn't required. (Also Cambridge have them funded by another route, and they are sitting not being used for anything other than occasional random benchmarks, as I understand it: https://io500.org/list/sc18/io500)

John Garbutt  11:35 AM
In some ways, this is the completion vs the Dell a100 boxes Cambridge are currently deploying for AI/ML: https://www.nvidia.com/en-gb/data-center/dgx-a100/
NVIDIANVIDIA
NVIDIA DGX A100 : The Universal System for AI Infrastructure
Essential Building Block of the AI Data Center & Offering Best Performance. (193 kB)
https://www.nvidia.com/en-gb/data-center/dgx-a100/

11:38
Part of my thinking here, is if we can over provision I/O for a test run, it will help identify where the next bottleneck is, and how much performance you need with the current CPU based solution. Its possible it make no difference, and something else is going on.



NVIDIA DGX A100
The Universal System for AI Infrastructure
https://www.nvidia.com/en-gb/data-center/dgx-a100/

NVIDIA Accelerates Apache Spark, Worldâ€™s Leading Data Analytics Platform
https://nvidianews.nvidia.com/news/nvidia-accelerates-apache-spark-worlds-leading-data-analytics-platform

