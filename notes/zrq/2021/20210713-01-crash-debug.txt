#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Diagnose the cause of ConnectException reported by Dennis via Slack :
        13th July 2021 10:30
        
            "new issues with the system! java.net.ConnectException: Connection refused (Connection refused) is the latest issue,
             It means I cannot run anything after that happens meaning nothing can be reliably run,"

        Initial checks revealed worker02 was missing in action.
        Not responding to anything, including ssh login.

    Result:
    
        Lots of clues, nothing conclusive.
        Lotsof different errors in the logs, most of then non-fatal.
        Spark is working to mitigate failures making it hard ot find the underlying cause.

        Need to look at the notebook that Dennis is running.              


# -----------------------------------------------------
# Create a new branch.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        newbranch=$(date '+%Y%m%d')-zrq-crash-debug

        git checkout master
        git checkout -b "${newbranch:?}"

        ....
        ....

        git push --set-upstream origin "${newbranch:?}"

    popd


# -----------------------------------------------------
# Try login to Zeppelin as normal.
#[user@desktop]

    firefox --new-window "http://zeppelin.gaia-prod.aglais.uk:8080/" &


        Initial front page looks OK.

        Try running a notebook and we immediately get a [Connection refused] error.

    >   java.net.ConnectException: Connection refused (Connection refused)
    >   	at java.net.PlainSocketImpl.socketConnect(Native Method)
    >   	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
    >   	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
    >   	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
    >   	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    >   	at java.net.Socket.connect(Socket.java:607)
    >   	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
    >   	at org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:51)
    >   	at org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:37)
    >   	at org.apache.commons.pool2.BasePooledObjectFactory.makeObject(BasePooledObjectFactory.java:60)
    >   	at org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:861)
    >   	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:435)
    >   	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.getClient(RemoteInterpreterProcess.java:62)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:133)
    >   	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:228)
    >   	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:449)
    >   	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
    >   	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:315)
    >   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    >   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    >   	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    >   	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:748)
            

# -----------------------------------------------------
# Try login via ssh.
#[user@desktop]

    ssh zeppelin
    
        date
        hostname

    >   Tue 13 Jul 11:00:25 UTC 2021
    >   gaia-prod-20210623-zeppelin.novalocal

    
# -----------------------------------------------------
# Try the other machines.
# First time it locked up on worker02, so added a connection timeout.
# https://stackoverflow.com/questions/4936807/how-to-set-ssh-timeout
# https://stackoverflow.com/a/5255550
#[user@zeppelin]

    vmnames=(
        monitor
        master01
        worker01
        worker02
        worker03
        worker04
        )

    for vmname in ${vmnames[@]}
    do
        echo ""
        echo "Node [${vmname}]"
    
        ssh "${vmname}" \
            -o ConnectTimeout=30 \
            '
            date
            hostname
            '
    done

    >   Node [monitor]
    >   Tue 13 Jul 11:05:25 UTC 2021
    >   gaia-prod-20210623-monitor.novalocal

    >   Node [master01]
    >   Tue 13 Jul 11:05:25 UTC 2021
    >   gaia-prod-20210623-master01.novalocal

    >   Node [worker01]
    >   Tue 13 Jul 11:05:25 UTC 2021
    >   gaia-prod-20210623-worker01.novalocal

    >   Node [worker02]
    >   ssh: connect to host worker02 port 22: Connection timed out

    >   Node [worker03]
    >   Tue 13 Jul 11:05:56 UTC 2021
    >   gaia-prod-20210623-worker03.novalocal

    >   Node [worker04]
    >   Tue 13 Jul 11:05:56 UTC 2021
    >   gaia-prod-20210623-worker04.novalocal


# -----------------------------------------------------
# Check the Zeppelin logs.
#[user@zeppelin]
        
    pushd /home/fedora/zeppelin/logs/

        ls -al
        
    >   ....
    >   ....
    >   -rw-rw-r--.  1 fedora fedora    373875 Jul 13 11:03 zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.log
    >   -rw-rw-r--.  1 fedora fedora    997802 Jul  9 23:59 zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-09
    >   -rw-rw-r--.  1 fedora fedora     20717 Jul 10 23:35 zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-10
    >   -rw-rw-r--.  1 fedora fedora    210306 Jul 11 23:35 zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-11
    >   -rw-rw-r--.  1 fedora fedora     75364 Jul 12 23:35 zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-12
    >   -rw-rw-r--.  1 fedora fedora     46459 Jul 13 10:57 zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.out
    >   -rw-rw-r--.  1 fedora fedora       384 Jul 13 09:19 zeppelin-interpreter-md-fedora-gaia-prod-20210623-zeppelin.novalocal.log
    >   -rw-rw-r--.  1 fedora fedora      8351 Jul  9 19:45 zeppelin-interpreter-md-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-09
    >   -rw-rw-r--.  1 fedora fedora       384 Jul 12 16:49 zeppelin-interpreter-md-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-12
    >   -rw-rw-r--.  1 fedora fedora      5860 Jul  9 15:23 zeppelin-interpreter-python-fedora-gaia-prod-20210623-zeppelin.novalocal.log
    >   -rw-rw-r--.  1 fedora fedora     10815 Jul 13 09:12 zeppelin-interpreter-sh-fedora-gaia-prod-20210623-zeppelin.novalocal.log
    >   -rw-rw-r--.  1 fedora fedora     39828 Jul  9 15:23 zeppelin-interpreter-sh-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-09
    >   -rw-rw-r--.  1 fedora fedora  34983608 Jul 13 10:22 zeppelin-interpreter-spark-fedora-gaia-prod-20210623-zeppelin.novalocal.log
    >   -rw-rw-r--.  1 fedora fedora 764282461 Jul  9 20:03 zeppelin-interpreter-spark-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-09
    >   -rw-rw-r--.  1 fedora fedora  14532576 Jul 11 16:18 zeppelin-interpreter-spark-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-11
    >   -rw-rw-r--.  1 fedora fedora   3794069 Jul 12 17:05 zeppelin-interpreter-spark-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-12


        for logfile in $(ls -1)
        do
            echo ""
            echo "--START--"
            echo "Log file [${logfile:?}]"            
            tail "${logfile:?}"
            echo "--END--"
        done

    #
    # Move older files into an attic.
    #

    mkdir attic
    mv zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-09 attic
    mv zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-10 attic
    mv zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-11 attic
    mv zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-12 attic
    mv zeppelin-interpreter-md-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-09 attic
    mv zeppelin-interpreter-md-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-12 attic
    mv zeppelin-interpreter-python-fedora-gaia-prod-20210623-zeppelin.novalocal.log attic
    mv zeppelin-interpreter-sh-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-09 attic
    mv zeppelin-interpreter-spark-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-09 attic
    mv zeppelin-interpreter-spark-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-11 attic
    mv zeppelin-interpreter-spark-fedora-gaia-prod-20210623-zeppelin.novalocal.log.2021-07-12 attic
    

# -----------------------------------------------------
# Identify the normal and not-normal parts of the logs.
#[user@zeppelin]


    Log file [zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.log]
    This is normal

    >    WARN [2021-07-13 10:57:44,521] ({qtp1580893732-1091} InterpreterSettingManager.java[compare]:886) - InterpreterGroup neo4j is not specified in zeppelin.interpreter.group.order
    >    WARN [2021-07-13 10:57:44,521] ({qtp1580893732-1091} InterpreterSettingManager.java[compare]:892) - InterpreterGroup neo4j is not specified in zeppelin.interpreter.group.order
    >    WARN [2021-07-13 10:57:44,521] ({qtp1580893732-1091} InterpreterSettingManager.java[compare]:892) - InterpreterGroup neo4j is not specified in zeppelin.interpreter.group.order
    >    INFO [2021-07-13 10:57:49,223] ({qtp1580893732-1091} InterpreterRestApi.java[restartSetting]:180) - Restart interpreterSetting spark, msg={"noteId":"2G748GZSW"}
    >    INFO [2021-07-13 10:57:49,224] ({qtp1580893732-1091} ManagedInterpreterGroup.java[close]:100) - Close Session: shared_session for interpreter setting: spark
    >    WARN [2021-07-13 10:57:49,225] ({qtp1580893732-1091} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkInterpreter
    >    WARN [2021-07-13 10:57:49,225] ({qtp1580893732-1091} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.SparkSqlInterpreter
    >    WARN [2021-07-13 10:57:49,225] ({qtp1580893732-1091} RemoteInterpreter.java[close]:199) - close is called when RemoterInterpreter is not opened for org.apache.zeppelin.spark.DepInterpreter


    Log file [zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.log]
    This is not-normal

    >    WARN [2021-07-13 10:57:49,226] ({qtp1580893732-1091} HttpChannel.java[handleException]:584) - /api/interpreter/setting/restart/spark java.net.ConnectException: Connection refused (Connection refused)
    >    INFO [2021-07-13 11:03:50,951] ({Scheduler-120694604} NotebookServer.java[onClose]:372) - Closed connection to 5.71.196.169 : 52555. (1001) Idle Timeout


    Log file [zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.out]
    This is normal

    >   Jul 13, 2021 2:40:09 AM org.glassfish.jersey.servlet.WebComponent filterFormParameters
    >   WARNING: A servlet request to the URI http://zeppelin.aglais.uk:8080/api/login contains form parameters in the request body but the request body has been consumed by the servlet or a servlet filter accessing the request parameters. Only 
    >   resource methods using @FormParam will work as expected. Resource methods consuming the request body by other means will not work as expected.
    >   Jul 13, 2021 9:01:52 AM org.glassfish.jersey.servlet.WebComponent filterFormParameters
    >   WARNING: A servlet request to the URI http://zeppelin.aglais.uk:8080/api/login contains form parameters in the request body but the request body has been consumed by the servlet or a servlet filter accessing the request parameters. Only 
    >   resource methods using @FormParam will work as expected. Resource methods consuming the request body by other means will not work as expected.


    Log file [zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.out]
    This is not-normal

    >   Exception in thread "Thread-1088" Exception in thread "JobStatusPoller-20210706-145744_1307665902" java.lang.RuntimeException: org.apache.thrift.transport.TTransportException
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:139)
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreterEventPoller.run(RemoteInterpreterEventPoller.java:101)
    >   Caused by: org.apache.thrift.transport.TTransportException
    >           ....
    >           ....
    >   java.lang.RuntimeException: org.apache.thrift.transport.TTransportException
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:139)
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getStatus(RemoteInterpreter.java:380)
    >           ....
    >   Caused by: org.apache.thrift.transport.TTransportException
    >           at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
    >           at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
    >           ....
    

    Log file [zeppelin-interpreter-md-fedora-gaia-prod-20210623-zeppelin.novalocal.log]
    This is normal
    
    >    ....
    >    INFO [2021-07-13 09:19:32,500] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:114) - Job 20210713-091823_1640482153 started by scheduler org.apache.zeppelin.markdown.Markdown449783330
    >    INFO [2021-07-13 09:19:32,511] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:120) - Job 20210713-091823_1640482153 finished by scheduler org.apache.zeppelin.markdown.Markdown449783330


    Log file [zeppelin-interpreter-sh-fedora-gaia-prod-20210623-zeppelin.novalocal.log]
    This is normal

    >    ....
    >    INFO [2021-07-13 09:12:11,032] ({pool-2-thread-5} ShellInterpreter.java[interpret]:115) - Paragraph 20210713-090834_684236471 return with exit value: 0
    >    INFO [2021-07-13 09:12:11,033] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:120) - Job 20210713-090834_684236471 finished by scheduler org.apache.zeppelin.shell.ShellInterpreter828517801


    Log file [zeppelin-interpreter-spark-fedora-gaia-prod-20210623-zeppelin.novalocal.log]
    This is normal

    >    ....
    >    INFO [2021-07-13 10:22:22,296] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 243
    >    INFO [2021-07-13 10:22:22,296] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 245


# -----------------------------------------------------
# Looking at each of them in more detail.
#[user@zeppelin]
    
    Log file [zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.log]
    This is not-normal

    >    WARN [2021-07-13 10:57:49,226] ({qtp1580893732-1091} HttpChannel.java[handleException]:584) - /api/interpreter/setting/restart/spark java.net.ConnectException: Connection refused (Connection refused)
    >    INFO [2021-07-13 11:03:50,951] ({Scheduler-120694604} NotebookServer.java[onClose]:372) - Closed connection to 5.71.196.169 : 52555. (1001) Idle Timeout
    
    
    Log file [zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.out]
    This is not-normal

    >   Exception in thread "Thread-1088" Exception in thread "JobStatusPoller-20210706-145744_1307665902" java.lang.RuntimeException: org.apache.thrift.transport.TTransportException
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:139)
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreterEventPoller.run(RemoteInterpreterEventPoller.java:101)
    >   Caused by: org.apache.thrift.transport.TTransportException
    >           ....
    >           ....
    >   java.lang.RuntimeException: org.apache.thrift.transport.TTransportException
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:139)
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getStatus(RemoteInterpreter.java:380)
    >           ....
    >   Caused by: org.apache.thrift.transport.TTransportException
    >           at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
    >           at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
    >           ....
    
    
    

    Log file [zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.out]
    This is normal - looks like a conenction from Dennis's home ?

    >    INFO [2021-07-13 09:01:47,860] ({qtp1580893732-1349} NotebookServer.java[onOpen]:151) - New connection from 5.71.196.169 : 51294
    >    WARN [2021-07-13 09:01:52,684] ({qtp1580893732-1205} LoginRestApi.java[postLogin]:206) - {"status":"OK","message":"","body":{"principal":"dcr","ticket":"14c36b65-51cb-48a4-9806-4cdc17307486","roles":"[\"user\"]"}}
    >    INFO [2021-07-13 09:01:57,431] ({qtp1580893732-1205} NotebookServer.java[sendNote]:828) - New operation from 5.71.196.169 : 51294 : dcr : GET_NOTE : 2G9BXYCKP


    whois 5.71.196.169

    >   inetnum:        5.68.0.0 - 5.71.255.255
    >   netname:        BSKYB-BROADBAND
    >   descr:          Sky UK Limited
    >   country:        GB
    >   ....
    >   ....


    Log file [zeppelin-fedora-gaia-prod-20210623-zeppelin.novalocal.out]
    This looks like normal activity just before the problem occurs.

    >    ....
    >    INFO [2021-07-13 09:19:32,481] ({qtp1580893732-1350} VFSNotebookRepo.java[save]:196) - Saving note:2G9BXYCKP
    >    INFO [2021-07-13 09:19:32,493] ({pool-2-thread-26} SchedulerFactory.java[jobStarted]:114) - Job 20210713-091823_1640482153 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-md:shared_process-shared_session
    >    INFO [2021-07-13 09:19:32,495] ({pool-2-thread-26} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20210713-091823_1640482153, interpreter: md, note_id: 2G9BXYCKP, user: dcr]
    >    INFO [2021-07-13 09:19:32,520] ({pool-2-thread-26} NotebookServer.java[afterStatusChange]:2314) - Job 20210713-091823_1640482153 is finished successfully, status: FINISHED
    >    INFO [2021-07-13 09:19:32,580] ({qtp1580893732-1350} VFSNotebookRepo.java[save]:196) - Saving note:2G9BXYCKP
    >    INFO [2021-07-13 09:19:32,603] ({pool-2-thread-26} VFSNotebookRepo.java[save]:196) - Saving note:2G9BXYCKP
    >    INFO [2021-07-13 09:19:32,607] ({pool-2-thread-26} SchedulerFactory.java[jobFinished]:120) - Job 20210713-091823_1640482153 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-md:shared_process-shared_session
    >    INFO [2021-07-13 09:19:46,073] ({qtp1580893732-1205} VFSNotebookRepo.java[save]:196) - Saving note:2G9BXYCKP
    >    INFO [2021-07-13 09:20:43,084] ({pool-2-thread-46} NotebookServer.java[afterStatusChange]:2314) - Job 20210706-142657_7677965 is finished successfully, status: FINISHED
    >    INFO [2021-07-13 09:20:43,162] ({pool-2-thread-46} VFSNotebookRepo.java[save]:196) - Saving note:2G9BXYCKP
    >    INFO [2021-07-13 09:20:43,167] ({pool-2-thread-46} SchedulerFactory.java[jobFinished]:120) - Job 20210706-142657_7677965 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
    >    INFO [2021-07-13 09:20:43,305] ({pool-2-thread-24} NotebookServer.java[afterStatusChange]:2314) - Job 20210706-141918_2051034386 is finished successfully, status: FINISHED
    >    INFO [2021-07-13 09:20:43,344] ({pool-2-thread-24} VFSNotebookRepo.java[save]:196) - Saving note:2G9BXYCKP
    >    ....
    >    INFO [2021-07-13 09:20:43,347] ({pool-2-thread-24} SchedulerFactory.java[jobFinished]:120) - Job 20210706-141918_2051034386 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
    >    INFO [2021-07-13 09:20:43,347] ({pool-2-thread-99} SchedulerFactory.java[jobStarted]:114) - Job 20210706-151223_1219291343 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
    >    INFO [2021-07-13 09:20:43,348] ({pool-2-thread-99} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20210706-151223_1219291343, interpreter: spark.pyspark, note_id: 2G9BXYCKP, user: dcr]
    >    INFO [2021-07-13 09:20:43,848] ({pool-2-thread-51} SchedulerFactory.java[jobStarted]:114) - Job 20210706-145744_1307665902 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
    >    INFO [2021-07-13 09:20:43,861] ({pool-2-thread-51} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20210706-145744_1307665902, interpreter: spark.pyspark, note_id: 2G9BXYCKP, user: dcr]


    .. and then the wheels fall off

    >   ERROR [2021-07-13 09:21:57,563] ({pool-2-thread-51} Job.java[run]:190) - Job failed
    >   java.lang.RuntimeException: org.apache.thrift.transport.TTransportException
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:139)
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:228)
    >   ....
    >   ERROR [2021-07-13 09:21:57,564] ({pool-2-thread-99} Job.java[run]:190) - Job failed
    >   java.lang.RuntimeException: org.apache.thrift.transport.TTransportException
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:139)
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:228)
    >   ....
    >   ERROR [2021-07-13 09:21:57,571] ({JobProgressPoller, jobId=20210706-151223_1219291343} JobProgressPoller.java[run]:58) - Can not get or update progress
    >   java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: java.net.SocketException: Connection reset
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:139)
    >   ....
    >   Caused by: java.net.SocketException: Connection reset
    >           at java.net.SocketInputStream.read(SocketInputStream.java:210)


    After that, any attempt to run the job fails

    >   ....
    >    INFO [2021-07-13 09:21:57,810] ({pool-2-thread-27} SchedulerFactory.java[jobStarted]:114) - Job 20210709-140832_772076010 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
    >    INFO [2021-07-13 09:21:57,811] ({pool-2-thread-27} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20210709-140832_772076010, interpreter: spark.pyspark, note_id: 2G9BXYCKP, user: dcr]
    >   ERROR [2021-07-13 09:21:57,811] ({pool-2-thread-27} Job.java[run]:190) - Job failed
    >   java.lang.RuntimeException: org.apache.zeppelin.interpreter.InterpreterException: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:141)
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:228)
    >   ....


    Trying to load the notebook again ... fails

    >   ....
    >    INFO [2021-07-13 09:27:34,346] ({qtp1580893732-1350} NotebookServer.java[sendNote]:828) - New operation from 5.71.196.169 : 51294 : dcr : GET_NOTE : 2G9BXYCKP
    >    WARN [2021-07-13 09:27:34,401] ({qtp1580893732-1363} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
    >    WARN [2021-07-13 09:27:34,401] ({qtp1580893732-1363} InterpreterSettingManager.java[compare]:886) - InterpreterGroup sap is not specified in zeppelin.interpreter.group.order
    >    ....
    >    ....
    >    WARN [2021-07-13 09:27:34,402] ({qtp1580893732-1363} InterpreterSettingManager.java[compare]:892) - InterpreterGroup neo4j is not specified in zeppelin.interpreter.group.order
    >    WARN [2021-07-13 09:27:34,402] ({qtp1580893732-1363} InterpreterSettingManager.java[compare]:892) - InterpreterGroup neo4j is not specified in zeppelin.interpreter.group.order
    >    INFO [2021-07-13 09:27:39,166] ({qtp1580893732-1363} VFSNotebookRepo.java[save]:196) - Saving note:2G9BXYCKP
    >    INFO [2021-07-13 09:27:40,028] ({qtp1580893732-1091} VFSNotebookRepo.java[save]:196) - Saving note:2G9BXYCKP
    >    INFO [2021-07-13 09:27:40,032] ({pool-2-thread-29} SchedulerFactory.java[jobStarted]:114) - Job 20210706-141918_2051034386 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpreter-spark:shared_process-shared_session
    >    INFO [2021-07-13 09:27:40,032] ({pool-2-thread-29} Paragraph.java[jobRun]:381) - Run paragraph [paragraph_id: 20210706-141918_2051034386, interpreter: spark.pyspark, note_id: 2G9BXYCKP, user: dcr]
    >   ERROR [2021-07-13 09:27:40,033] ({pool-2-thread-29} Job.java[run]:190) - Job failed
    >   java.lang.RuntimeException: org.apache.zeppelin.interpreter.InterpreterException: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:141)
    >           at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:228)
    >           at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:449)
    >   ....







    Log file [zeppelin-interpreter-spark-fedora-gaia-prod-20210623-zeppelin.novalocal.log]
    This looks not-normal

    >   ....
    >   2021-07-13 10:18:46,871 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32698
    >   2021-07-13 10:18:46,872 INFO executor.Executor: Running task 1766.2 in stage 4.0 (TID 32698)
    >   2021-07-13 10:18:46,875 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:46,876 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-02363-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49979748, partition values: [empty row]
    >   2021-07-13 10:18:46,932 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:46,943 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:46,951 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:47,178 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-01281-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49643213, partition values: [empty row]
    >   2021-07-13 10:18:47,231 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:47,239 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:47,246 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:47,392 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-04437-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49827810, partition values: [empty row]
    >   2021-07-13 10:18:47,474 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:47,500 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000604980000, 1018691584, 0) failed; error='Cannot allocate memory' (errno=12)
    >   
    >   
    >   [2021-07-13 10:18:49.687]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
    >   Last 4096 bytes of prelaunch.err :
    >   Last 4096 bytes of stderr :
    >   l)
    >   2021-07-13 10:18:46,635 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:46,730 INFO memory.MemoryStore: Block rdd_12_974 stored as values in memory (estimated size 4.7 MB, free 932.2 MB)
    >   2021-07-13 10:18:46,732 INFO executor.Executor: Finished task 974.3 in stage 4.0 (TID 32684). 2299 bytes result sent to driver
    >   ....


    ... but then it seems to continue as normal

    >   ....
    >   2021-07-13 10:18:46,734 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32695
    >   2021-07-13 10:18:46,734 INFO executor.Executor: Running task 2576.2 in stage 4.0 (TID 32695)
    >   2021-07-13 10:18:46,738 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-06948-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49827888, partition values: [empty row]
    >   2021-07-13 10:18:46,787 INFO memory.MemoryStore: Block rdd_12_3672 stored as values in memory (estimated size 6.7 MB, free 925.5 MB)
    >   2021-07-13 10:18:46,789 INFO executor.Executor: Finished task 3672.3 in stage 4.0 (TID 32681). 2299 bytes result sent to driver
    >   ....


    ... until the wheels fall off

    >   ....
    >   2021-07-13 10:18:46,872 INFO executor.Executor: Running task 1766.2 in stage 4.0 (TID 32698)
    >   2021-07-13 10:18:46,875 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:46,876 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-02363-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49979748, partition values: [empty row]
    >   2021-07-13 10:18:46,932 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:46,943 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:46,951 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:47,178 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-01281-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49643213, partition values: [empty row]
    >   2021-07-13 10:18:47,231 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:47,239 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:47,246 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:47,392 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-04437-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49827810, partition values: [empty row]
    >   2021-07-13 10:18:47,474 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:47,500 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000604980000, 1018691584, 0) failed; error='Cannot allocate memory' (errno=12)
    >   
    >    INFO [2021-07-13 10:18:49,887] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 1765), so marking it as still running.
    >    INFO [2021-07-13 10:18:49,890] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 233), so marking it as still running.
    >    INFO [2021-07-13 10:18:49,890] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 563), so marking it as still running.
    >    INFO [2021-07-13 10:18:49,890] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 296), so marking it as still running.
    >    ....
    >    ....
    >    INFO [2021-07-13 10:18:49,901] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 128), so marking it as still running.
    >    INFO [2021-07-13 10:18:49,901] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 5501), so marking it as still running.
    >    INFO [2021-07-13 10:18:49,901] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 239), so marking it as still running.
    >    WARN [2021-07-13 10:18:49,901] ({dispatcher-event-loop-3} Logging.scala[logWarning]:66) - Lost task 227.2 in stage 4.0 (TID 32696, worker01, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container from a bad node: container_1624450963932_0020_01_000013 on host: worker01. Exit status: 1. Diagnostics: [2021-07-13 10:18:49.683]Exception from container-launch.
    >   Container id: container_1624450963932_0020_01_000013
    >   Exit code: 1
    >   
    >   [2021-07-13 10:18:49.686]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
    >   Last 4096 bytes of prelaunch.err :
    >   Last 4096 bytes of stderr :
    >   l)
    >   2021-07-13 10:18:46,635 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 10:18:46,730 INFO memory.MemoryStore: Block rdd_12_974 stored as values in memory (estimated size 4.7 MB, free 932.2 MB)
    >   2021-07-13 10:18:46,732 INFO executor.Executor: Finished task 974.3 in stage 4.0 (TID 32684). 2299 bytes result sent to driver
    >   ....


    #
    # Lightbulb !!
    # I'm reading the logs the wrong way.
    #


#
# This part is from the Zeppelin node:

    >   ....
    >    INFO [2021-07-13 10:18:49,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 2564), so marking it as still running.
    >    INFO [2021-07-13 10:18:49,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 4707), so marking it as still running.
    >    INFO [2021-07-13 10:18:49,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 2745), so marking it as still running.
    >    INFO [2021-07-13 10:18:49,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 5641), so marking it as still running.
    >    INFO [2021-07-13 10:18:49,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 5280), so marking it as still running.
    >    WARN [2021-07-13 10:18:49,904] ({dispatcher-event-loop-3} Logging.scala[logWarning]:66) - Lost task 3456.2 in stage 4.0 (TID 32694, worker01, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container from a bad node: container_1624450963932_0020_01_000013 on host: worker01. Exit status: 1. Diagnostics: [2021-07-13 10:18:49.683]Exception from container-launch.
    >   Container id: container_1624450963932_0020_01_000013
    >   Exit code: 1
    >   
    >   [2021-07-13 10:18:49.686]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
    >   Last 4096 bytes of prelaunch.err :
    >   Last 4096 bytes of stderr :
    >   
    >       #
    >       # This part is the "last 4096 bytes of stderr" from the failed container.
    >   
    >       l)
    >       2021-07-13 10:18:46,635 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,730 INFO memory.MemoryStore: Block rdd_12_974 stored as values in memory (estimated size 4.7 MB, free 932.2 MB)
    >       2021-07-13 10:18:46,732 INFO executor.Executor: Finished task 974.3 in stage 4.0 (TID 32684). 2299 bytes result sent to driver
    >       2021-07-13 10:18:46,734 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32695
    >       2021-07-13 10:18:46,734 INFO executor.Executor: Running task 2576.2 in stage 4.0 (TID 32695)
    >       2021-07-13 10:18:46,738 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-06948-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49827888, partition values: [empty row]
    >       2021-07-13 10:18:46,787 INFO memory.MemoryStore: Block rdd_12_3672 stored as values in memory (estimated size 6.7 MB, free 925.5 MB)
    >       2021-07-13 10:18:46,789 INFO executor.Executor: Finished task 3672.3 in stage 4.0 (TID 32681). 2299 bytes result sent to driver
    >       2021-07-13 10:18:46,790 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32696
    >       2021-07-13 10:18:46,790 INFO executor.Executor: Running task 227.2 in stage 4.0 (TID 32696)
    >       2021-07-13 10:18:46,795 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-05045-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50656698, partition values: [empty row]
    >       2021-07-13 10:18:46,824 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,835 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,841 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,847 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,861 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,866 INFO memory.MemoryStore: Block rdd_12_4202 stored as values in memory (estimated size 5.7 MB, free 919.9 MB)
    >       2021-07-13 10:18:46,869 INFO executor.Executor: Finished task 4202.2 in stage 4.0 (TID 32683). 2299 bytes result sent to driver
    >       2021-07-13 10:18:46,871 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32698
    >       2021-07-13 10:18:46,872 INFO executor.Executor: Running task 1766.2 in stage 4.0 (TID 32698)
    >       2021-07-13 10:18:46,875 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,876 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-02363-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49979748, partition values: [empty row]
    >       2021-07-13 10:18:46,932 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,943 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,951 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:47,178 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-01281-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49643213, partition values: [empty row]
    >       2021-07-13 10:18:47,231 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:47,239 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:47,246 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:47,392 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-04437-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49827810, partition values: [empty row]
    >       2021-07-13 10:18:47,474 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:47,500 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000604980000, 1018691584, 0) failed; error='Cannot allocate memory' (errno=12)


#
# This looks like a duplicate ?

    >   ....
    >   [2021-07-13 10:18:49.687]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
    >   Last 4096 bytes of prelaunch.err :
    >   Last 4096 bytes of stderr :
    >   
    >       #
    >       # This part is the "last 4096 bytes of stderr" from the failed container.
    >   
    >       l)
    >       2021-07-13 10:18:46,635 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,730 INFO memory.MemoryStore: Block rdd_12_974 stored as values in memory (estimated size 4.7 MB, free 932.2 MB)
    >       2021-07-13 10:18:46,732 INFO executor.Executor: Finished task 974.3 in stage 4.0 (TID 32684). 2299 bytes result sent to driver
    >       2021-07-13 10:18:46,734 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32695
    >       2021-07-13 10:18:46,734 INFO executor.Executor: Running task 2576.2 in stage 4.0 (TID 32695)
    >       2021-07-13 10:18:46,738 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-06948-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49827888, partition values: [empty row]
    >       2021-07-13 10:18:46,787 INFO memory.MemoryStore: Block rdd_12_3672 stored as values in memory (estimated size 6.7 MB, free 925.5 MB)
    >       2021-07-13 10:18:46,789 INFO executor.Executor: Finished task 3672.3 in stage 4.0 (TID 32681). 2299 bytes result sent to driver
    >       2021-07-13 10:18:46,790 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32696
    >       2021-07-13 10:18:46,790 INFO executor.Executor: Running task 227.2 in stage 4.0 (TID 32696)
    >       2021-07-13 10:18:46,795 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-05045-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50656698, partition values: [empty row]
    >       2021-07-13 10:18:46,824 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,835 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,841 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,847 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,861 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,866 INFO memory.MemoryStore: Block rdd_12_4202 stored as values in memory (estimated size 5.7 MB, free 919.9 MB)
    >       2021-07-13 10:18:46,869 INFO executor.Executor: Finished task 4202.2 in stage 4.0 (TID 32683). 2299 bytes result sent to driver
    >       2021-07-13 10:18:46,871 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32698
    >       2021-07-13 10:18:46,872 INFO executor.Executor: Running task 1766.2 in stage 4.0 (TID 32698)
    >       2021-07-13 10:18:46,875 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,876 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-02363-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49979748, partition values: [empty row]
    >       2021-07-13 10:18:46,932 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,943 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:46,951 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:47,178 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-01281-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49643213, partition values: [empty row]
    >       2021-07-13 10:18:47,231 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:47,239 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:47,246 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:47,392 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-04437-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49827810, partition values: [empty row]
    >       2021-07-13 10:18:47,474 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       2021-07-13 10:18:47,500 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >       OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000604980000, 1018691584, 0) failed; error='Cannot allocate memory' (errno=12)

#
# And then back to the Zeppelin node:

    >   ....
    >    INFO [2021-07-13 10:18:49,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 253), so marking it as still running.
    >    INFO [2021-07-13 10:18:49,911] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 5450), so marking it as still running.
    >    INFO [2021-07-13 10:18:49,911] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 1815), so marking it as still running.
    >    INFO [2021-07-13 10:18:49,911] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 3458), so marking it as still running.
    >   ....


This looks normal (ish) caching in memory and on disc, but only on worker01 and worker04.
No mention of worker02 and worker03?

    >   ....
    >    INFO [2021-07-13 10:20:03,267] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Added rdd_12_5156 in memory on worker01:41985 (size: 4.6 MB, free: 5.2 GB)
    >    INFO [2021-07-13 10:20:03,270] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2639.2 in stage 4.0 (TID 33472, worker01, executor 23, partition 2639, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-07-13 10:20:03,270] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 5156.2 in stage 4.0 (TID 33461) in 1066 ms on worker01 (executor 23) (4936/5720)
    >    INFO [2021-07-13 10:20:03,330] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added rdd_12_4303 in memory on worker01:41985 (size: 4.7 MB, free: 5.2 GB)
    >    INFO [2021-07-13 10:20:03,334] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 626.2 in stage 4.0 (TID 33473, worker01, executor 23, partition 626, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-07-13 10:20:03,334] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 4303.3 in stage 4.0 (TID 33459) in 1287 ms on worker01 (executor 23) (4937/5720)
    >    INFO [2021-07-13 10:20:03,345] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added rdd_12_5295 on disk on worker01:35105 (size: 8.3 MB)
    >    INFO [2021-07-13 10:20:03,356] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Starting task 2976.4 in stage 4.0 (TID 33474, worker01, executor 5, partition 2976, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-07-13 10:20:03,356] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 5295.1 in stage 4.0 (TID 33446) in 2207 ms on worker01 (executor 5) (4938/5720)
    >    INFO [2021-07-13 10:20:03,520] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Added rdd_12_5059 on disk on worker04:43703 (size: 4.9 MB)
    >    INFO [2021-07-13 10:20:03,529] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2342.5 in stage 4.0 (TID 33475, worker04, executor 11, partition 2342, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-07-13 10:20:03,530] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 5059.1 in stage 4.0 (TID 33458) in 1486 ms on worker04 (executor 11) (4939/5720)
    >    INFO [2021-07-13 10:20:03,785] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added rdd_12_248 in memory on worker04:42335 (size: 4.4 MB, free: 4.1 GB)
    >    INFO [2021-07-13 10:20:03,788] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 5355.1 in stage 4.0 (TID 33476, worker04, executor 22, partition 5355, PROCESS_LOCAL, 8582 bytes)
    >    INFO [2021-07-13 10:20:03,789] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 248.5 in stage 4.0 (TID 33462) in 1208 ms on worker04 (executor 22) (4940/5720)
    >    INFO [2021-07-13 10:20:03,902] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added rdd_12_652 on disk on worker04:43703 (size: 5.3 MB)
    >    INFO [2021-07-13 10:20:03,912] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Starting task 5696.1 in stage 4.0 (TID 33477, worker04, executor 11, partition 5696, PROCESS_LOCAL, 8714 bytes)
    >    INFO [2021-07-13 10:20:03,912] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 652.3 in stage 4.0 (TID 33463) in 1207 ms on worker04 (executor 11) (4941/5720)
    >    INFO [2021-07-13 10:20:03,916] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added rdd_12_2327 in memory on worker01:41985 (size: 6.8 MB, free: 5.2 GB)
    >    INFO [2021-07-13 10:20:03,919] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2661.3 in stage 4.0 (TID 33478, worker01, executor 23, partition 2661, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-07-13 10:20:03,920] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 2327.3 in stage 4.0 (TID 33457) in 1973 ms on worker01 (executor 23) (4942/5720)
    >    INFO [2021-07-13 10:20:04,000] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added rdd_12_3304 on disk on worker04:43703 (size: 7.3 MB)
    >    INFO [2021-07-13 10:20:04,012] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 2149.3 in stage 4.0 (TID 33479, worker04, executor 11, partition 2149, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-07-13 10:20:04,013] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3304.5 in stage 4.0 (TID 33460) in 1852 ms on worker04 (executor 11) (4943/5720)
    >    INFO [2021-07-13 10:20:04,117] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Added rdd_12_438 in memory on worker01:41985 (size: 6.2 MB, free: 5.2 GB)
    >    INFO [2021-07-13 10:20:04,120] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 5615.1 in stage 4.0 (TID 33480, worker01, executor 23, partition 5615, PROCESS_LOCAL, 8582 bytes)
    >    INFO [2021-07-13 10:20:04,121] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 438.6 in stage 4.0 (TID 33467) in 1129 ms on worker01 (executor 23) (4944/5720)
    >    INFO [2021-07-13 10:20:04,134] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added rdd_12_708 on disk on worker01:35105 (size: 5.8 MB)
    >    INFO [2021-07-13 10:20:04,143] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 3380.5 in stage 4.0 (TID 33481, worker01, executor 5, partition 3380, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-07-13 10:20:04,143] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 708.4 in stage 4.0 (TID 33464) in 1336 ms on worker01 (executor 5) (4945/5720)
    >   ....



    Here Spark is loosing executors and trying to compensate.


    >   ....
    >    INFO [2021-07-13 10:09:26,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 875), so marking it as still running.
    >    INFO [2021-07-13 10:09:26,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 570), so marking it as still running.
    >    INFO [2021-07-13 10:09:26,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 3164), so marking it as still running.
    >    INFO [2021-07-13 10:09:26,045] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(4, 3166), so marking it as still running.
    >    INFO [2021-07-13 10:09:26,045] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 1 (epoch 15)
    >    INFO [2021-07-13 10:09:26,045] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 1 from BlockManagerMaster.
    >    WARN [2021-07-13 10:09:26,045] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_1954 !
    >    WARN [2021-07-13 10:09:26,045] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_375 !
    >    WARN [2021-07-13 10:09:26,045] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_1825 !
    >    WARN [2021-07-13 10:09:26,045] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_3171 !
    >    ....
    >    ....
    >    WARN [2021-07-13 10:09:26,079] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_3603 !
    >    WARN [2021-07-13 10:09:26,079] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_4239 !
    >    WARN [2021-07-13 10:09:26,079] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_4438 !
    >    WARN [2021-07-13 10:09:26,079] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_2619 !
    >    INFO [2021-07-13 10:09:26,079] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(1, worker02, 46177, None)
    >    INFO [2021-07-13 10:09:26,079] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 1 successfully in removeExecutor
    >    INFO [2021-07-13 10:09:26,079] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 1 (epoch 15)
    >    INFO [2021-07-13 10:09:26,370] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on worker04:42263 (size: 26.6 KB, free: 6.2 GB)
    >    INFO [2021-07-13 10:09:27,233] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added rdd_12_4147 on disk on worker01:35105 (size: 5.6 MB)
    >    INFO [2021-07-13 10:09:27,247] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 355.1 in stage 4.0 (TID 28824, worker01, executor 5, partition 355, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-07-13 10:09:27,247] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 4147.2 in stage 4.0 (TID 28806) in 1313 ms on worker01 (executor 5) (3970/5720)
    >    ....
    >    ....
    >    INFO [2021-07-13 10:09:31,759] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 328.1 in stage 4.0 (TID 28855, worker04, executor 11, partition 328, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-07-13 10:09:31,760] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 884.1 in stage 4.0 (TID 28848) in 1656 ms on worker04 (executor 11) (4001/5720)
    >    INFO [2021-07-13 10:09:31,886] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added rdd_12_3514 in memory on worker04:42263 (size: 7.9 MB, free: 6.2 GB)
    >    INFO [2021-07-13 10:09:31,891] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Starting task 3083.1 in stage 4.0 (TID 28856, worker04, executor 20, partition 3083, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-07-13 10:09:31,892] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 3514.1 in stage 4.0 (TID 28845) in 1876 ms on worker04 (executor 20) (4002/5720)
    >    INFO [2021-07-13 10:09:31,936] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Disabling executor 3.
    >    INFO [2021-07-13 10:09:31,936] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 3 (epoch 16)
    >    INFO [2021-07-13 10:09:31,936] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Trying to remove executor 3 from BlockManagerMaster.
    >    WARN [2021-07-13 10:09:31,937] ({dispatcher-event-loop-4} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_1970 !
    >    WARN [2021-07-13 10:09:31,937] ({dispatcher-event-loop-4} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_2298 !
    >    WARN [2021-07-13 10:09:31,937] ({dispatcher-event-loop-4} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_1581 !
    >    WARN [2021-07-13 10:09:31,937] ({dispatcher-event-loop-4} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_1038 !
    >    ....
    >    ....
    >    WARN [2021-07-13 10:09:31,981] ({dispatcher-event-loop-4} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_938 !
    >    WARN [2021-07-13 10:09:31,981] ({dispatcher-event-loop-4} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_3085 !
    >    WARN [2021-07-13 10:09:31,981] ({dispatcher-event-loop-4} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_4548 !
    >    WARN [2021-07-13 10:09:31,981] ({dispatcher-event-loop-4} Logging.scala[logWarning]:66) - No more replicas available for rdd_12_1089 !
    >    INFO [2021-07-13 10:09:31,981] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(3, worker04, 42821, None)
    >    INFO [2021-07-13 10:09:31,982] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 3 successfully in removeExecutor
    >    INFO [2021-07-13 10:09:31,982] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 3 (epoch 16)
    >    INFO [2021-07-13 10:09:32,009] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added rdd_12_2509 in memory on worker04:43703 (size: 7.5 MB, free: 2.5 GB)
    >    INFO [2021-07-13 10:09:32,012] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Starting task 3872.1 in stage 4.0 (TID 28857, worker04, executor 11, partition 3872, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-07-13 10:09:32,012] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 2509.1 in stage 4.0 (TID 28842) in 2192 ms on worker04 (executor 11) (4003/5720)
    >    ....
    >    ....
    >    INFO [2021-07-13 10:09:32,088] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Starting task 251.2 in stage 4.0 (TID 28860, worker04, executor 11, partition 251, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-07-13 10:09:32,088] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 3664.2 in stage 4.0 (TID 28847) in 2039 ms on worker04 (executor 11) (4006/5720)
    >   ERROR [2021-07-13 10:09:32,298] ({dispatcher-event-loop-4} Logging.scala[logError]:70) - Lost executor 3 on worker04: Container from a bad node: container_1624450963932_0020_01_000004 on host: worker04. Exit status: 137. Diagnostics: [2021-07-13 10:09:31.996]Container killed on request. Exit code is 137
    >   [2021-07-13 10:09:31.997]Container exited with a non-zero exit code 137.
    >   [2021-07-13 10:09:31.998]Killed by external signal
    >   .
    >    WARN [2021-07-13 10:09:32,298] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - Requesting driver to remove executor 3 for reason Container from a bad node: container_1624450963932_0020_01_000004 on host: worker04. Exit status: 137. Diagnostics: [2021-07-13 10:09:31.996]Container killed on request. Exit code is 137
    >   [2021-07-13 10:09:31.997]Container exited with a non-zero exit code 137.
    >   ....


# -----------------------------------------------------
# Look at details on the worker node.
#[user@zeppelin]

    ssh worker 01

    pushd /var/hadoop/logs

        ls -al
        
    >   ....
    >   -rw-rw-r--. 1 fedora fedora  235248 Jul 13 12:31 hadoop-fedora-datanode-gaia-prod-20210623-worker01.novalocal.log
    >   -rw-rw-r--. 1 fedora fedora     700 Jun 23 12:22 hadoop-fedora-datanode-gaia-prod-20210623-worker01.novalocal.out
    >   -rw-rw-r--. 1 fedora fedora 1301571 Jul 13 13:32 hadoop-fedora-nodemanager-gaia-prod-20210623-worker01.novalocal.log
    >   -rw-rw-r--. 1 fedora fedora    2216 Jun 23 12:22 hadoop-fedora-nodemanager-gaia-prod-20210623-worker01.novalocal.out

    
        cat hadoop-fedora-datanode-gaia-prod-20210623-worker01.novalocal.log

    >   021-06-23 12:22:33,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG:
    >   /..
    >   STARTUP_MSG: Starting DataNode
    >   STARTUP_MSG:   host = worker01/10.10.2.202
    >   STARTUP_MSG:   args = []
    >   STARTUP_MSG:   version = 3.1.3


    cat hadoop-fedora-datanode-gaia-prod-20210623-worker01.novalocal.out

    >   core file size          (blocks, -c) unlimited
    >   data seg size           (kbytes, -d) unlimited
    >   scheduling priority             (-e) 0
    >   file size               (blocks, -f) unlimited
    >   pending signals                 (-i) 90238
    >   max locked memory       (kbytes, -l) 64
    >   max memory size         (kbytes, -m) unlimited
    >   open files                      (-n) 1024
    >   pipe size            (512 bytes, -p) 8
    >   POSIX message queues     (bytes, -q) 819200
    >   real-time priority              (-r) 0
    >   stack size              (kbytes, -s) 8192
    >   cpu time               (seconds, -t) unlimited
    >   max user processes              (-u) 90238
    >   virtual memory          (kbytes, -v) unlimited
    >   file locks                      (-x) unlimited


    cat hadoop-fedora-nodemanager-gaia-prod-20210623-worker01.novalocal.log

    >   2021-06-23 12:22:44,171 INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: STARTUP_MSG:
    >   /..
    >   STARTUP_MSG: Starting NodeManager
    >   STARTUP_MSG:   host = worker01/10.10.2.202
    >   STARTUP_MSG:   args = []
    >   STARTUP_MSG:   version = 3.1.3
    
    
    cat hadoop-fedora-nodemanager-gaia-prod-20210623-worker01.novalocal.out

    >   Jun 23, 2021 12:22:45 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
    >   INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
    >   Jun 23, 2021 12:22:45 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
    >   INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
    >   ....
    >   ....
    >   Jun 23, 2021 12:22:45 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
    >   INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
    >   Jun 23, 2021 12:22:46 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
    >   INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"
    
    
    ls userlogs/
    
    >   -
    
    Nothing ?
    No aplications, no containers.
    Is this because there has been a tidy up of completed tasks ?


# -----------------------------------------------------
# Look at details on the worker node.
#[user@zeppelin]

    ssh worker 02

    pushd /var/hadoop/logs

        ls -al

    >   -rw-rw-r--. 1 fedora fedora  257357 Jul 13 12:04 hadoop-fedora-datanode-gaia-prod-20210623-worker02.novalocal.log
    >   -rw-rw-r--. 1 fedora fedora     700 Jun 23 12:22 hadoop-fedora-datanode-gaia-prod-20210623-worker02.novalocal.out
    >   -rw-rw-r--. 1 fedora fedora 1355028 Jul 13 13:36 hadoop-fedora-nodemanager-gaia-prod-20210623-worker02.novalocal.log
    >   -rw-rw-r--. 1 fedora fedora    2216 Jun 23 12:22 hadoop-fedora-nodemanager-gaia-prod-20210623-worker02.novalocal.out


        tail hadoop-fedora-datanode-gaia-prod-20210623-worker02.novalocal.log

    >   ....
    >   No GCs detected
    >   2021-07-13 10:56:52,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1652469206-10.10.1.60-1624450949061 (Datanode Uuid 501d114e-8c38-4b20-a22a-bc3e8b60bdc6) service to master01/10.10.1.60:9000 beginning handshake with NN
    >   2021-07-13 10:57:53,465 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 53498ms
    >   No GCs detected
    >   2021-07-13 10:58:07,084 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 11164ms
    >   No GCs detected
    >   2021-07-13 11:06:53,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1652469206-10.10.1.60-1624450949061 (Datanode Uuid 501d114e-8c38-4b20-a22a-bc3e8b60bdc6) service to master01/10.10.1.60:9000 successfully registered with NN
    >   2021-07-13 11:06:54,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x583fc0977027f4e0,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 9 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
    >   2021-07-13 11:06:54,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1652469206-10.10.1.60-1624450949061
    >   2021-07-13 12:04:04,884 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1652469206-10.10.1.60-1624450949061 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0


        tail hadoop-fedora-datanode-gaia-prod-20210623-worker02.novalocal.out

    >   max memory size         (kbytes, -m) unlimited
    >   open files                      (-n) 1024
    >   pipe size            (512 bytes, -p) 8
    >   POSIX message queues     (bytes, -q) 819200
    >   real-time priority              (-r) 0
    >   stack size              (kbytes, -s) 8192
    >   cpu time               (seconds, -t) unlimited
    >   max user processes              (-u) 90238
    >   virtual memory          (kbytes, -v) unlimited
    >   file locks                      (-x) unlimited



        tail hadoop-fedora-nodemanager-gaia-prod-20210623-worker02.novalocal.log

    >   ....
    >   2021-07-13 12:22:44,062 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 401769658
    >   2021-07-13 12:26:24,630 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 4896281464, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-07-13 12:33:18,647 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting path : /var/hadoop/logs/userlogs/application_1624450963932_0019
    >   2021-07-13 12:36:24,630 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 4896281464, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-07-13 12:46:24,631 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 4896281464, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-07-13 12:56:24,632 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 4896281464, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-07-13 13:06:24,631 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 4896281464, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-07-13 13:16:24,632 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 4896281464, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-07-13 13:26:24,633 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 4896281464, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-07-13 13:36:24,633 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 4896281464, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0



        tail hadoop-fedora-nodemanager-gaia-prod-20210623-worker02.novalocal.out

    >   ....
    >   real-time priority              (-r) 0
    >   stack size              (kbytes, -s) 8192
    >   cpu time               (seconds, -t) unlimited
    >   max user processes              (-u) 90238
    >   virtual memory          (kbytes, -v) unlimited
    >   file locks                      (-x) unlimited
    >   Jun 23, 2021 12:22:45 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
    >   INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope "Singleton"
    >   Jun 23, 2021 12:22:46 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
    >   INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope "Singleton"


        ls -al userlogs

    >   drwx--x---. 15 fedora fedora 4096 Jul 13 09:46 application_1624450963932_0020


        ls -al userlogs/application_1624450963932_0020/

    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:33 container_1624450963932_0020_01_000002
    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:33 container_1624450963932_0020_01_000005
    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:33 container_1624450963932_0020_01_000008
    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:38 container_1624450963932_0020_01_000009
    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:41 container_1624450963932_0020_01_000010
    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:43 container_1624450963932_0020_01_000011
    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:45 container_1624450963932_0020_01_000014
    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:45 container_1624450963932_0020_01_000015
    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:46 container_1624450963932_0020_01_000016
    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:46 container_1624450963932_0020_01_000017
    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:46 container_1624450963932_0020_01_000018
    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:46 container_1624450963932_0020_01_000019
    >   drwx--x---.  2 fedora fedora 4096 Jul 13 09:46 container_1624450963932_0020_01_000020


        ls -al userlogs/application_1624450963932_0020/container_1624450963932_0020_01_000020

    >   -rw-rw-r--.  1 fedora fedora 36170 Jul 13 09:46 directory.info
    >   -rw-r-----.  1 fedora fedora  5487 Jul 13 09:46 launch_container.sh
    >   -rw-rw-r--.  1 fedora fedora     0 Jul 13 09:46 prelaunch.err
    >   -rw-rw-r--.  1 fedora fedora   100 Jul 13 09:46 prelaunch.out
    >   -rw-rw-r--.  1 fedora fedora 67161 Jul 13 11:06 stderr
    >   -rw-rw-r--.  1 fedora fedora     0 Jul 13 09:46 stdout


        less userlogs/application_1624450963932_0020/container_1624450963932_0020_01_000020/stderr


    >   ....
    >   2021-07-13 09:46:37,284 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 09:46:37,295 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 09:46:37,297 INFO compat.FilterCompat: Filtering using predicate: noteq(parallax, null)
    >   2021-07-13 09:46:38,246 INFO memory.MemoryStore: Block rdd_12_2280 stored as values in memory (estimated size 6.9 MB, free 6.2 GB)
    >   2021-07-13 09:48:45,671 INFO memory.MemoryStore: Block rdd_12_235 stored as values in memory (estimated size 4.1 MB, free 6.2 GB)
    >   2021-07-13 09:56:35,767 ERROR server.TransportChannelHandler: Connection to zeppelin/10.10.3.207:37007 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.
    >   2021-07-13 10:03:47,668 ERROR client.TransportClient: Failed to send RPC RPC 6982204349985887881 to zeppelin/10.10.3.207:37007: java.nio.channels.ClosedChannelException
    >   java.nio.channels.ClosedChannelException
    >           at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >           at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   ....


    >   ....
    >   2021-07-13 10:04:53,227 WARN storage.BlockManager: Putting block rdd_12_235 failed due to exception org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout.
    >   2021-07-13 10:28:06,461 WARN netty.NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC RPC 6982204349985887881 to zeppelin/10.10.3.207:37007: java.nio.channels.ClosedChannelException
    >   2021-07-13 10:52:20,330 WARN client.TransportClientFactory: DNS resolution for zeppelin/10.10.3.207:37007 took 1201862 ms
    >   2021-07-13 10:52:26,648 INFO client.TransportClientFactory: Found inactive connection to zeppelin/10.10.3.207:37007, creating a new one.
    >   2021-07-13 10:53:34,965 ERROR executor.CoarseGrainedExecutorBackend: Executor self-exiting due to : Driver zeppelin:37007 disassociated! Shutting down.
    >   2021-07-13 10:54:55,247 ERROR netty.RpcOutboxMessage: Ask timeout before connecting successfully
    >   2021-07-13 11:03:53,944 WARN netty.NettyRpcEnv: Ignored failure: java.io.IOException: Connecting to zeppelin/10.10.3.207:37007 timed out (120000 ms)
    >   2021-07-13 11:06:53,971 INFO storage.DiskBlockManager: Shutdown hook called
    >   2021-07-13 11:06:54,021 ERROR executor.Executor: Exception in task 235.1 in stage 4.0 (TID 27418)
    >   org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
    >           at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
    >           at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
    >           at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
    >   ....

    #
    # Connecting back from worker to Zeppelin timesout.
    #
    
    >   ....
    >   2021-07-13 11:03:53,944 WARN netty.NettyRpcEnv: Ignored failure: java.io.IOException: Connecting to zeppelin/10.10.3.207:37007 timed out (120000 ms)
    >   ....



# -----------------------------------------------------
# Spent several hours trawling through the log files.
# Seems that the failed workers have recovered ?
#[user@zeppelin]

    vmnames=(
        monitor
        master01
        worker01
        worker02
        worker03
        worker04
        )

    for vmname in ${vmnames[@]}
    do
        echo ""
        echo "Node [${vmname}]"
    
        ssh "${vmname}" \
            -o ConnectTimeout=30 \
            '
            date
            hostname
            '
    done

    >   Node [monitor]
    >   Tue 13 Jul 14:05:48 UTC 2021
    >   gaia-prod-20210623-monitor.novalocal
    >   
    >   Node [master01]
    >   Tue 13 Jul 14:05:49 UTC 2021
    >   gaia-prod-20210623-master01.novalocal
    >   
    >   Node [worker01]
    >   Tue 13 Jul 14:05:49 UTC 2021
    >   gaia-prod-20210623-worker01.novalocal
    >   
    >   Node [worker02]
    >   Tue 13 Jul 14:05:49 UTC 2021
    >   gaia-prod-20210623-worker02.novalocal
    >   
    >   Node [worker03]
    >   Tue 13 Jul 14:05:49 UTC 2021
    >   gaia-prod-20210623-worker03.novalocal
    >   
    >   Node [worker04]
    >   Tue 13 Jul 14:05:50 UTC 2021
    >   gaia-prod-20210623-worker04.novalocal

    














