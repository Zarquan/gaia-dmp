#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-indent
#zrq-notes-ansible
#zrq-notes-osformat
#



Compute resources

The Gaia Science Platform used the full allocation of cpu resources available during 2021.

During the period where only the 80% allocation was available
the project had to schedule work of deveopers and project scientists to spread the load between users.

Once the full 2021 allocation was available developers and scientists were able to work concurrently,
and we have been able to perform data analysis workflows that were not possible to run with the 80%
allocation.

----

IO performance

IO bandwidth of the Ceph storage system associated with the Cumulus cloud
has had a significant impact on overall performance.

The nature of many of the machine learning algorithms used in the data analysis
require sustained concurrent random access to the full data set which the
current Ceph storage system has been unable to provide.

We have regularly see diagnostic cpu metrics showing IO wait as high
as 50% to 60% for sustained periods of time while running the ML algorithms.

We have been working with StackHPC and the system administratos at Cambridge to
to find the best way to solve the IO bandwidth problems.

As a result of our work we reccomend two steps.
Firstly migrating the Gaia analysis platform from the Cumulus Openstack system to the
Arcus Openstack system will help to improve performance.
Secondly, the design of the Gaia analysis platform should be
modifed to use direct attached storage (DAS) for main science dataset.

----
Storage allocation

The Gaia Science Platform has used < 50% of the storage allocation requested for 2021.

This is partly due to the efficiency of the Parquet data format which is significantly more compact
than expected at storing the tabular datasets.

However, this is also because plans to improve IO bandwidth by replicating
the data were put on hold.
In theory replication can be used to improve IO performance for concurrent
access, but in practice as all the IO requests were being sent to the same
Ceph storage system, replicating the data did not help to improve performance.

----
Static/dynamic allocation

The Gaia Science Platform started using a dymanic cloud-compute allocation
in 2019, but changed to a static allocation in 2020 in response to issues
with resource contention between projects.

The Cumuls system has been operating at close to 100% capacity,
with little or no spare capacity to meet on demand resource requests.
This resulted in projects using defensive strategies
like resource hoarding to guarantee the resources
would be there when they needed them.

As part of our DevOps approach we would regularly delete and re-create
a multi-node Kubernetes cluster.
However, the demand for resources was such that it was possible for some
resources to be allocated to another project in the gap between deleting
the old cluster and creating a new one, causing the create step to fail.

To prevent this happening the Gaia Science Platform was moved to
a static allocation pinned to specific hardware in 2020.

This solved the problem of resource contention, but it was in effect
equivalent to a bare-metal allocation with the added overheads and
restriction of virtual machine deployments.

In 2022 we are planning to develop a resource booking system that
will work in conjunction with the Openstack Blazar interface
that StackHPC are planning to implement.

The hope is that these systems working together will enable the
Gaia Science Platform to use dynamic resource allocation
for at least part of the resources.

DM#2
Section 4.
Anticipated use

Bullet point 4, change
Batch scheduling via a bespoke resource reservation system (Apache Hadoop YARN may be employed pending the outcome of current experiments)Â 
Batch scheduling via a bespoke resource reservation system will be developed to work with the Openstack Blazar interface being deployed by StackHPC

Bullet point 5, delete all
Distributed computing environment employs components from Cloudera and Apache Spark
-

DM#3

Storage
    DR2     3Tb
    eDR3    3Tb
    DR3    30Tb (2022)
    DR4   300Tb (2025)

For 2022 that means 3+3+30 = 33Tbyte of science data
HDFS data replication x3, plus space to import and process the data
4 * (3+3+30) = 144Tbytes of space

This is the same headline figure as requested for 2021.
However based on what we discovered during 2021 we would
like to add some specific requirements.

Enough DirectAttchedStorage (DAS) available on the compute
nodes to store two full copies of the 30Tbyte dataset.

The reasoning behind this is that for most of the time roughly
half of the compute resources will be configured to run the live science
platform for researchers to use.
Which will need to have enough DAS to handle a full copy of
the science dataset.
The remaining resources will be configured to provide
test and development clusters to work on developing the
next iteration of the platform.

To meet periods of higher demand, resulting from more users
using the system at the same time or running more complex
analysis workloads, all of the compute resources can be configured
to run the live service, providing additional compute and
additional replication of the data set to support
higher level of cincurrent access.

This results in two resource requests for storage.
72Tbytes of space on the Ceph storage system
72Tbytes of solid state storage directly attached to the compute nodes

Compute

We are using three different analysis workflows developed
by Hambly and Crake during 2021 as benchmarks to predict
the kinds of workloads we will expect to see in 2022.

The simplest workflow is the proper motions analysis
which applies an aggregate calculation to the dataset,
grouping the data by regions on the sky.
Each worker node in the cluster performing a sequential
scan of their part of the dataset.

The next level of difficulty is the machine learning example
using a random forest classifier to ideitify good and bad
astrometric solutions.
The training stage of this workflow results in a large number
of concurrent random reads to the dataset.

The most diffcicult workflow is the Spark implementation
of the Kounkel & Covey (2019) study. This workflow has several
a number of stages which require different types of data access,
culminating in a large single instance of the HDBSCAN clustering
algorithm which requires a large memory allocation.

The current allocation of 6 CascadeLake machines () is working well
for the current workload. Providing sufficient resources for the
main science platform and additional space for the test and development projects.

In theory the expected 10x increase in data size for the full DR3
will mean that to maintain the same level of performance will
require an equivalent *10 increase in compute resources.

Unlike the storage requirements we do not expect
to need to simply scale the compute resources by a factor
of 10 when moving from 3Tbyte eDR3 data set to the expected
30Tbyte DR3 dataset.
Firstly, as the size of the data set increases the proportion
of resources put aside for test and devlopment will be a smaller
fraction of the overall system.
Secondly, we hope advances in algorithm design to enable researchers
to spread the single large monolithic steps across multiple concurrent

Taking this into account, we are requesting an equivalent increase
from 6 to 33 compute resources.

However, this has implications for the clustering
algorithm used in the third example.

Scaling this algorithm to work on an equivalent slice of the
full DR3 data simply by increasing the memory on each physical
node by a factor of 10 is not feasible.

It will not be practical to increase the memory available on a single
physical node by a factor of *10.

So solving this problem will involve some increase in the available memory
in combination with developing ways to distribute the clustering algorithm
across multiple nodes.

To address the physical memory aspect we are requesting that
roughly a quarter of the compute nodes are high memory nodes
with 512G of memory.


    Summary

Current 2021 allocation

6 x CascadeLake machines
112 hyperthreaded cores,
192G RAM (1.7G/core)
800G SSD (*) most of which is allocated to the OS and not all available to VMs

112*6 = 672 cores

Request 2022-2024

Keep 3 machines for test/dev
Scale the 3 live machines to fit the x10 data

(3 + 30) CCLake equivalent machines
112*(3 + 30) = 3696 vcores

25 with 192G of memory and 8 high memory nodes with 512G of memory
(25*192) + (8*512) = 8896G memory

72Tbytes of storage on the Ceph storage system
72Tbytes of solid state storage directly attached to the compute nodes
equvalent to approx 2 Tbytes of DAS SDD on each compute node

        Request 2025

            recalculate for x100



