#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Problems with available resources on the Openstack system.

    We need to increase the local disc space allocated to our VMs to provide temp space for Spark and Hadoop.
    Tried putting the temp space on a mounted volume, but we think that incurrs a high performance penalty.

    Tried increasing the size of our worker nodes from small to medium to increase the local disc space.
    (*) medium VMs have a second local disc volume mounted as /dev/vdc

    Ran into resource problems allocating the medium VMs.
    Currently system uses 6 small nodes.
    Updating that to 8 small nodes works, updating that to 6 medium nodes fails.

        6 small
        PASS

        8 small
        PASS

        6 medium
        FAIL

        4 medium
        PASS

    Slack conversation with PaulBrowne and JohnGarbutt ..

    >   Dave Morris
    >   Is the ephemeral disc of a general.v1.medium created with direct attached storage, or is it network mounted ?
    >   
    >   Paul Browne
    >   Local storage to the compute node, but it's just a single SSD disk rather than NVMe or striped array.
    >   
    >   Dave Morris
    >   We have an issue with available resources at the moment.
    >   A new science use case that Nigel is working on needs to load a lot of data in memory.
    >   When Spark runs out of memory space, it caches the overflow in temp space on disc.
    >   Spark was configured to use /tmp on the VMs for it's temp data, which would go to local storage on the host.
    >   We were running Spark in 6 small VMs, which only have 20G of local disc. We tried to increase that to 6 medium VMs but run into "no available resource" issues.
    >   We have tried configuring Spark to use a Cinder volume for the temp data, but that has a huge hit on performance.
    >   
    >   John Garbutt
    >   Local storage is defined by the flavor
    >   I.e. a fixed ratio
    >   Cinder volume / Manila / S3 is separate, quite a lot of it ~100TB now ~0.5PB soon, but is on remote spinning disks
    >   ... although newer hardware has double the disk to RAM ratio
    >   
    >   Dave Morris
    >   If we get errors trying to create 6 medium VMs, we don't know if that due to limits on cpu, ram or local disc.
    >   What would be the best way of increasing the local temp space available to Spark?
    >   
    >   John Garbutt
    >   If you spin down stage and dev, might be room for a cluster with bigger instances I guess? @Paul Browne I guess they have their full allocation? Could they get cascade lake for more local storage?
    >   
    >   Paul Browne
    >   They have 4 full Cascade Lake nodes all to themselves
    >   RAM is probably limiting factor, then local disk
    >   Each of these 4 nodes have these physical resources ;
    >   
    >       cpu-p-633:
    >       local_gb: 880
    >       memory_mb: 191855 (188G)
    >       vcpus: 110

    The comment from John about reducing dev and test to get more on the live system is worrying.
    I thought they were supposed to be isolated from each other.

    We need to figure out what the numbers from Paul mean for us ...
    TODO resource calculations - possibly a spread sheet ?









