#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#


    #
    # Quote from the Cambridge cloud website
    # https://rse-cambridge.github.io/iris-openstack/cambridge#cascade-lake
    
        Each hypervisor (Dell PowerEdge C6420) has two Intel Xeon Platinum 8276
        (i.e. a total of 112 hyperthreaded cores runing at 2.20-4.00 GHz per hypervisor)
        with 192GB RAM (i.e. 1.7GB per hyperthreaded core) and around 800GB of local SSD.
        
        There is a single 50GbE Mellanox ConnectEx-6 ethernet link (with the option for RoCEv2 via SR-IOV).

        The hardware also includes a (currently unused by IRIS) HDR100 Mellanox Infiniband connection.

        For VM sizing, two 90GB VMs, using under 400GB of local disk, should fit into a single hypervisor.
        Typiucally there are 108 vCPUs available for VMs.
        If you are in a dedicated aggregate, this can be 1:1 hyperthreads to vCPUs.
        
----------------------------------------------------------------

    On Tue, Oct 19, 2021 at 7:44 AM Dave Morris <dmr@roe.ac.uk> wrote:

        Hi John,

        I'm putting together our IRIS resource request for 2022+ and I'm
        researching the current state of the art in terms of cloud compute for
        machine learning on BigData.

        In a recent Slack discussion you said

             "This sounds like something you want on a hyperconverged file system
        that uses the local SSD storage .."
             "Certainly local NVMe is more typical for this sort of ML pipeline."

        and

             "Yep, that is my current ML recommendation, hyperconverged on local
        disk."

        What would your recommendation be for a Spark ML application like ours?
        and may I quote you in our resource request?

        Cheers,
        -- Dave

    On 2021-10-19 09:46, John Garbutt wrote:

        Good questions. (I am including JohnT for visibility.)

        Essentially, many Machine Learning algorithms can be data parallel.
        This allows for shards of data to be staged (or cached) locally.
        Generally, training appears to be very IO bound. If you are not
        careful, expensive CPU and GPU resources are starved due to poor
        storage.

        However, there are some workloads that need shared storage, as those
        algorithms are not data parallel, or at least not predictably so.
        Sometimes for only part of a pipeline.

        This mix suggested we need a way to have substantial fast local
        storage for those that need it, but a way to convert some (or most) of
        that to shared storage, as required. Currently we are looking at
        Rook.io, OpenEBS and other similar hyperconverged solutions. Similar
        to the Ceph hyperconverged solution we tried with Euclid within slurm.
        The hope is that kubernetes operators reduce the operational overhead
        of such a solution, particularly when created via the cloud portal we
        are developing.

        Certainly we are seeing systems optimised for ML having many local
        NVMe drives, as opposed to a single spinning disk, so 20GB/s locally
        on each node shouldn't be a problem (with Gen4 PCIe)

        Also having good Ceph storage, that is based on SSD or NVMe and not
        spinning disk, will make a huge difference to additional storage on
        nodes that don't have enough local storage.

        I think it is too early to suggest that this has been "solved" or that
        there is much consensus yet.

        I hope that helps?

        Thanks,
        John

----------------------------------------------------------------

On Tue, 19 Oct 2021 at 07:18, Dave Morris <dmr@roe.ac.uk<mailto:dmr@roe.ac.uk>> wrote:

    Hi Paul,

    I'm writing up our IRIS resource request for 2022+.

    As a baseline for next years request, could you confirm exactly what
    resources we currently have.

    At the start of this year I know our tasks were pinned to four Cascade
    Lake hosts, but since then we have been through a couple of rounds of
    new deployments.

    Cheers,
    -- Dave

On 2021-10-19 14:23, Paul Browne wrote:

    Hi Dave,

    Currently you have access to an aggregate of 8 of our Cascade Lake hosts;

    (oscli) [pfb29@cumulus-seed ansible]$ openstack aggregate show gaia-cclake-agg
    +-------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | Field             | Value                                                                                                                                                            |
    +-------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | availability_zone | None                                                                                                                                                             |
    | created_at        | 2020-09-29T22:11:14.000000                                                                                                                                       |
    | deleted           | False                                                                                                                                                            |
    | deleted_at        | None                                                                                                                                                             |
    | hosts             | cpu-p-629, cpu-p-630, cpu-p-631, cpu-p-632, cpu-p-633, cpu-p-634, cpu-p-635, cpu-p-636                                                                           |
    | id                | 17                                                                                                                                                               |
    | name              | gaia-cclake-agg                                                                                                                                                  |
    | properties        | filter_tenant_id1='08e24c6d87f94740aa59c172462ed927', filter_tenant_id2='21b4ae3a2ea44bc5a9c14005ed2963af', filter_tenant_id3='bea28e83e6aa47a8962b59c3b24495fe' |
    | updated_at        | None                                                                                                                                                             |
    +-------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+

    Thanks,
    Paul B.


