#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Target:

        Update the Ansible deploy to test different topologies.

    Results:

        Work in progress ....


# -----------------------------------------------------
# Update the Openstack cloud name.
#[user@desktop]

    cloudname=gaia-dev

    sed -i '
        s/^\(AGLAIS_CLOUD\)=.*$/\1='${cloudname:?}'/
        ' "${HOME}/aglais.env"


# -----------------------------------------------------
# Create a container to work with.
# (*) extra volume mount for /common
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/common:/common:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/hadoop-yarn:/hadoop-yarn:ro,z" \
        atolmis/ansible-client:2020.12.02 \
        bash


# -----------------------------------------------------
# Create our Aglais configuration.
#[root@kubernator]

cat > '/tmp/aglais-config.yml' << EOF
aglais:
    version: 1.0
    spec:
        openstack:
            cloud: '${cloudname:?}'

EOF


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /openstack/bin/delete-all.sh \
            "${cloudname:?}"

    >   real    2m55.690s
    >   user    1m5.349s
    >   sys     0m9.532s


# -----------------------------------------------------
# Create everything.
#[root@ansibler]

    time \
        /hadoop-yarn/bin/create-all.sh

    >   ....
    >   ....
    >   TASK [Create [/etc/profile.d/java.sh]] *********************************************************
    >   fatal: [master01]: FAILED! => {"msg": "Timeout (12s) waiting for privilege escalation prompt: "}
    >   ....
    >   ....
    >   TASK [Install the public key] ************************************************************************************
    >   fatal: [worker01]: FAILED! => {"msg": "The task includes an option with an undefined variable.
    >       The error was: 'ansible.vars.hostvars.HostVarsVars object' has no attribute 'publickey'
    >           The error appears to be in '/hadoop-yarn/ansible/12-config-ssh-access.yml': line 92, column 7, but maybe elsewhere in the file depending on the exact syntax problem.
    >           The offending line appears to be:
    >               - name: \"Install the public key\"
    >                     ^ here
    >   ....
    >   ....
    >   PLAY RECAP *******************************************************************************************************
    >   gateway                    : ok=8    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   localhost                  : ok=78   changed=63   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0
    >   master01                   : ok=5    changed=3    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0
    >   worker01                   : ok=21   changed=16   unreachable=0    failed=1    skipped=0    rescued=0    ignored=0
    >   worker02                   : ok=21   changed=16   unreachable=0    failed=1    skipped=0    rescued=0    ignored=0
    >   worker03                   : ok=21   changed=16   unreachable=0    failed=1    skipped=0    rescued=0    ignored=0
    >   worker04                   : ok=21   changed=16   unreachable=0    failed=1    skipped=0    rescued=0    ignored=0
    >   zeppelin                   : ok=51   changed=46   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0

    >   real    28m21.488s
    >   user    6m29.692s
    >   sys     1m9.864s

    #
    # The Ansible scripts are very sensitive to network delays.
    # With the bash error trap enabled this means we are seeing >50% failure rate.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Increase the default timeout for Ansible.
#[user@desktop]

    source "${HOME:?}/agais.env"
    pushd  "${AGLAIS_CODE:?}"

        gedit experiments/hadoop-yarn/ansible/ansible.cfg

        +   # https://stackoverflow.com/questions/39533532/ansible-timeout-12s-waiting-for-privilege-escalation-prompt
        +   # https://stackoverflow.com/a/39535679
        +   timeout=30

    popd


# -----------------------------------------------------
# Create a container to work with.
# (*) extra volume mount for /common
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/common:/common:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/hadoop-yarn:/hadoop-yarn:ro,z" \
        atolmis/ansible-client:2020.12.02 \
        bash


# -----------------------------------------------------
# Create our Aglais configuration.
#[root@kubernator]

cat > '/tmp/aglais-config.yml' << EOF
aglais:
    version: 1.0
    spec:
        openstack:
            cloud: '${cloudname:?}'

EOF


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /openstack/bin/delete-all.sh \
            "${cloudname:?}"

    >   real    2m50.128s
    >   user    1m3.700s
    >   sys     0m9.065s


# -----------------------------------------------------
# Create everything.
#[root@ansibler]

    time \
        /hadoop-yarn/bin/create-all.sh

    >   ....
    >   ....
    >   PLAY RECAP *******************************************************************************************************
    >   gateway                    : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   master01                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker01                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker02                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker03                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker04                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   zeppelin                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0

    >   real    47m37.934s
    >   user    11m4.035s
    >   sys     2m42.580s


# -----------------------------------------------------
# Check the results.
#[root@ansibler]

    cat '/tmp/aglais-status.yml'

    >   aglais:
    >     spec:
    >       openstack:
    >         cloud: gaia-dev
    >     status:
    >       deployment:
    >         type: hadoop-yarn
    >         name: gaia-dev-20210205
    >         date: 20210205T151415

    deployname=$(
        yq read \
            '/tmp/aglais-status.yml' \
                'aglais.status.deployment.name'
        )

    echo "Deployment [${deployname}]"

    >   Deployment [gaia-dev-20210205]

# -----------------------------------------------------
# Get the public IP address of our Zeppelin node.
#[root@ansibler]

    zeppelinid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            server list \
                --format json \
        | jq -r '.[] | select(.Name == "'${deployname:?}'-zeppelin") | .ID'
        )

    zeppelinip=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            server show \
                --format json \
                "${zeppelinid:?}" \
        | jq -r '.addresses' \
        | sed '
            s/[[:space:]]//
            s/.*=\(.*\)/\1/
            s/.*,\(.*\)/\1/
            '
        )

cat << EOF
Zeppelin ID [${zeppelinid:?}]
Zeppelin IP [${zeppelinip:?}]
EOF

    >   Zeppelin ID [1df56b36-97ee-4f52-9094-1ab961942455]
    >   Zeppelin IP [128.232.227.218]

# -----------------------------------------------------
# -----------------------------------------------------

    # Update our DNS ..

# -----------------------------------------------------
# -----------------------------------------------------
# Login to the Zeppelin node and check the data shares.
#[root@ansibler]

    sharelist='/common/manila/datashares.yaml'

    for shareid in $(
        yq read "${sharelist:?}" 'shares.[*].id'
        )
    do
        echo ""
        echo "---- ----"
        echo "Share [${shareid:?}]"
        echo "----"

        sharename=$(yq read "${sharelist:?}" "shares.(id==${shareid:?}).sharename")
        mountpath=$(yq read "${sharelist:?}" "shares.(id==${shareid:?}).mountpath")

        ssh "fedora@${zeppelinip:?}" \
            "
            date
            hostname
            echo '----'
            df -h  '${mountpath:?}'
            echo '----'
            ls -al '${mountpath:?}' | tail
            "
    done

    >   Share [GDR2]
    >   ----
    >   Fri Feb  5 16:05:40 UTC 2021
    >   gaia-dev-20210205-zeppelin.novalocal
    >   ----
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ----
    >   -rw-r--r--. 1 fedora fedora     30825240 Oct 24 17:59 part-06504-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
    >   -rw-r--r--. 1 fedora fedora     31802127 Oct 24 17:59 part-06505-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
    >   -rw-r--r--. 1 fedora fedora     31538538 Oct 24 17:59 part-06506-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
    >   -rw-r--r--. 1 fedora fedora     31218434 Oct 24 17:59 part-06507-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
    >   -rw-r--r--. 1 fedora fedora     30815074 Oct 24 17:59 part-06508-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
    >   -rw-r--r--. 1 fedora fedora     30406730 Oct 24 17:59 part-06509-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
    >   -rw-r--r--. 1 fedora fedora     29995058 Oct 24 17:59 part-06510-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
    >   -rw-r--r--. 1 fedora fedora     29447614 Oct 24 17:59 part-06511-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
    >   -rw-r--r--. 1 fedora fedora     28448646 Oct 24 17:59 part-06512-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
    >   -rw-r--r--. 1 fedora fedora      6317774 Oct 24 17:59 part-06513-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
    >   
    >   ---- ----
    >   Share [GEDR3]
    >   ----
    >   Fri Feb  5 16:05:42 UTC 2021
    >   gaia-dev-20210205-zeppelin.novalocal
    >   ----
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ----
    >   -rw-r--r--. 1 root root     36858229 Jan 11 22:27 part-11922-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     35391788 Jan 11 22:27 part-11923-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     39969879 Jan 11 22:27 part-11924-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     38923149 Jan 11 22:27 part-11925-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     36280019 Jan 11 22:27 part-11926-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     39559908 Jan 11 22:27 part-11927-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     34715127 Jan 11 22:27 part-11928-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     35453747 Jan 11 22:27 part-11929-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     30599245 Jan 11 22:27 part-11930-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     10852913 Jan 11 22:27 part-11931-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   
    >   ---- ----
    >   Share [ALLWISE]
    >   ----
    >   Fri Feb  5 16:05:44 UTC 2021
    >   gaia-dev-20210205-zeppelin.novalocal
    >   ----
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ----
    >   -rw-r--r--. 1 root root     21195981 Jan 11 21:26 part-09124-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     20760761 Jan 11 21:26 part-09125-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     37549253 Jan 11 21:26 part-09126-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     32687920 Jan 11 21:26 part-09127-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     30215740 Jan 11 21:26 part-09128-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     26528776 Jan 11 21:26 part-09129-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     36999673 Jan 11 21:26 part-09130-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     30382801 Jan 11 21:26 part-09131-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     31622359 Jan 11 21:26 part-09132-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
    >   -rw-r--r--. 1 root root      9956618 Jan 11 21:26 part-09133-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
    >   
    >   ---- ----
    >   Share [PS1]
    >   ----
    >   Fri Feb  5 16:05:46 UTC 2021
    >   gaia-dev-20210205-zeppelin.novalocal
    >   ----
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ----
    >   -rw-r--r--. 1 root root     27803868 Jan 11 19:43 part-07723-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     22025506 Jan 11 19:43 part-07724-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     25756891 Jan 11 19:43 part-07725-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     31396660 Jan 11 19:43 part-07726-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     26859792 Jan 11 19:44 part-07727-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     24735889 Jan 11 19:44 part-07728-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     25470955 Jan 11 19:44 part-07729-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     25640631 Jan 11 19:44 part-07730-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     22504695 Jan 11 19:44 part-07731-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
    >   -rw-r--r--. 1 root root     13200198 Jan 11 19:44 part-07732-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
    >   
    >   ---- ----
    >   Share [2MASS]
    >   ----
    >   Fri Feb  5 16:05:48 UTC 2021
    >   gaia-dev-20210205-zeppelin.novalocal
    >   ----
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ----
    >   -rw-r--r--. 1 root root    16875933 Jan 11 17:44 part-01176-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
    >   -rw-r--r--. 1 root root    31847987 Jan 11 17:44 part-01177-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
    >   -rw-r--r--. 1 root root    33978033 Jan 11 17:45 part-01178-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
    >   -rw-r--r--. 1 root root    33170642 Jan 11 17:45 part-01179-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
    >   -rw-r--r--. 1 root root    33115257 Jan 11 17:45 part-01180-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
    >   -rw-r--r--. 1 root root    33854964 Jan 11 17:45 part-01181-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
    >   -rw-r--r--. 1 root root    31874821 Jan 11 17:45 part-01182-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
    >   -rw-r--r--. 1 root root    33091386 Jan 11 17:45 part-01183-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
    >   -rw-r--r--. 1 root root    31078087 Jan 11 17:45 part-01184-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
    >   -rw-r--r--. 1 root root    14460710 Jan 11 17:45 part-01185-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet


# -----------------------------------------------------
# Login to Zeppelin ...
#[user@desktop]

    firefox --new-window "http://zeppelin.metagrid.xyz:8080/" &


# -----------------------------------------------------
# Run test notebooks ..
#[user@zeppelin]


    Import notebooks from GitHu, clear the output and run all the cells ...

    Good astrometric solutions via ML Random Forrest classifier
    https://raw.githubusercontent.com/wfau/aglais-notebooks/main/2FRPC4BFS/note.json

        Started  [Fri 5 Feb 16:16:28 GMT 2021]
        Finished []

    >   Took  3 sec. Last updated by gaiauser at February 05 2021, 4:17:34 PM.
    >   Took 55 sec. Last updated by gaiauser at February 05 2021, 4:18:29 PM.
    >   Took 25 min 10 sec. Last updated by gaiauser at February 05 2021, 4:43:39 PM.
    >   Took 7 sec. Last updated by gaiauser at February 05 2021, 4:43:46 PM.
    >   Took 0 sec. Last updated by gaiauser at February 05 2021, 4:43:46 PM.
    >   Took 10 sec. Last updated by gaiauser at February 05 2021, 4:43:56 PM.
    >   Took 1 sec. Last updated by gaiauser at February 05 2021, 4:43:57 PM.
    >   
    >   
    >   Py4JJavaError: An error occurred while calling o195.fit.
    >   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 5145 in stage 34.0 failed 4 times, most recent failure: Lost task 5145.3 in stage 34.0 (TID 122201, worker02, executor 3): java.io.IOException: No space left on device
    >   Took 6 min 31 sec. Last updated by gaiauser at February 05 2021, 4:50:28 PM.
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------
# Login to each node and monitor disc use.
#[root@ansibler]

    nodes=(
        master01
        worker01
        worker02
        worker03
        worker04
        zeppelin
        )

    for node in "${nodes[@]}"
    do
        echo "---- ----"
        echo "Node [${node:?}]"

        ssh "fedora@${node:?}" \
            '
            date
            hostname
            df -h
            '
    done

    >   ---- ----
    >   Node [master01]
    >   Fri Feb  5 16:38:23 UTC 2021
    >   gaia-dev-20210205-master01.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         11G     0   11G   0% /dev
    >   tmpfs            11G     0   11G   0% /dev/shm
    >   tmpfs            11G  536K   11G   1% /run
    >   tmpfs            11G     0   11G   0% /sys/fs/cgroup
    >   /dev/vda1        20G  3.4G   16G  19% /
    >   tmpfs           2.2G     0  2.2G   0% /run/user/1000
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ceph-fuse        10T  5.5T  4.6T  55% /user/nch
    >   ceph-fuse       1.0T   30G  995G   3% /user/zrq
    >   ceph-fuse       1.0T     0  1.0T   0% /user/stv
    >   ---- ----
    >   Node [worker01]
    >   Fri Feb  5 16:38:23 UTC 2021
    >   gaia-dev-20210205-worker01.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         23G     0   23G   0% /dev
    >   tmpfs            23G     0   23G   0% /dev/shm
    >   tmpfs            23G  572K   23G   1% /run
    >   tmpfs            23G     0   23G   0% /sys/fs/cgroup
    >   /dev/vda1        20G  3.5G   16G  19% /
    >   /dev/vdc        512G  252M  510G   1% /data-01
    >   tmpfs           4.5G     0  4.5G   0% /run/user/1000
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ceph-fuse        10T  5.5T  4.6T  55% /user/nch
    >   ceph-fuse       1.0T   30G  995G   3% /user/zrq
    >   ceph-fuse       1.0T     0  1.0T   0% /user/stv
    >   ---- ----
    >   Node [worker02]
    >   Fri Feb  5 16:38:24 UTC 2021
    >   gaia-dev-20210205-worker02.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         23G     0   23G   0% /dev
    >   tmpfs            23G     0   23G   0% /dev/shm
    >   tmpfs            23G  572K   23G   1% /run
    >   tmpfs            23G     0   23G   0% /sys/fs/cgroup
    >   /dev/vda1        20G  3.5G   16G  19% /
    >   /dev/vdc        512G   73M  510G   1% /data-01
    >   tmpfs           4.5G     0  4.5G   0% /run/user/1000
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ceph-fuse        10T  5.5T  4.6T  55% /user/nch
    >   ceph-fuse       1.0T   30G  995G   3% /user/zrq
    >   ceph-fuse       1.0T     0  1.0T   0% /user/stv
    >   ---- ----
    >   Node [worker03]
    >   Fri Feb  5 16:38:24 UTC 2021
    >   gaia-dev-20210205-worker03.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         23G     0   23G   0% /dev
    >   tmpfs            23G     0   23G   0% /dev/shm
    >   tmpfs            23G  572K   23G   1% /run
    >   tmpfs            23G     0   23G   0% /sys/fs/cgroup
    >   /dev/vda1        20G  3.5G   16G  19% /
    >   /dev/vdc        512G   75M  510G   1% /data-01
    >   tmpfs           4.5G     0  4.5G   0% /run/user/1000
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ceph-fuse        10T  5.5T  4.6T  55% /user/nch
    >   ceph-fuse       1.0T   30G  995G   3% /user/zrq
    >   ceph-fuse       1.0T     0  1.0T   0% /user/stv
    >   ---- ----
    >   Node [worker04]
    >   Fri Feb  5 16:38:24 UTC 2021
    >   gaia-dev-20210205-worker04.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         23G     0   23G   0% /dev
    >   tmpfs            23G     0   23G   0% /dev/shm
    >   tmpfs            23G  572K   23G   1% /run
    >   tmpfs            23G     0   23G   0% /sys/fs/cgroup
    >   /dev/vda1        20G  3.4G   16G  18% /
    >   /dev/vdc        512G  250M  510G   1% /data-01
    >   tmpfs           4.5G     0  4.5G   0% /run/user/1000
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ceph-fuse        10T  5.5T  4.6T  55% /user/nch
    >   ceph-fuse       1.0T   30G  995G   3% /user/zrq
    >   ceph-fuse       1.0T     0  1.0T   0% /user/stv
    >   ---- ----
    >   Node [zeppelin]
    >   Fri Feb  5 16:38:24 UTC 2021
    >   gaia-dev-20210205-zeppelin.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         11G     0   11G   0% /dev
    >   tmpfs            11G     0   11G   0% /dev/shm
    >   tmpfs            11G  536K   11G   1% /run
    >   tmpfs            11G     0   11G   0% /sys/fs/cgroup
    >   /dev/vda1        20G  4.7G   15G  25% /
    >   /dev/vdb        512G   17M  510G   1% /data-02
    >   tmpfs           2.2G     0  2.2G   0% /run/user/1000
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ceph-fuse        10T  5.5T  4.6T  55% /user/nch
    >   ceph-fuse       1.0T   30G  995G   3% /user/zrq
    >   ceph-fuse       1.0T     0  1.0T   0% /user/stv


    #
    # First thing to notice is the extra disc isn't mounted ...
    #

    >   ....
    >   /dev/vda1        20G  3.4G   16G  18% /
    >   /dev/vdc        512G  250M  510G   1% /data-01
    >   ....

    #
    # Nothing for /dev/vdb on the worker nodes ...
    #


# -----------------------------------------------------
# Login to each node and monitor disc use.
# While the RandomForestClassifier is training.
#[root@ansibler]

    nodes=(
        master01
        worker01
        worker02
        worker03
        worker04
        zeppelin
        )

    for node in "${nodes[@]}"
    do
        echo "---- ----"
        echo "Node [${node:?}]"

        ssh "fedora@${node:?}" \
            '
            date
            hostname
            df -h /
            '
    done

    >   ---- ----
    >   Node [master01]
    >   Fri Feb  5 16:50:04 UTC 2021
    >   gaia-dev-20210205-master01.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  3.4G   16G  19% /
    >   ---- ----
    >   Node [worker01]
    >   Fri Feb  5 16:50:04 UTC 2021
    >   gaia-dev-20210205-worker01.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G   17G  2.6G  87% /
    >   ---- ----
    >   Node [worker02]
    >   Fri Feb  5 16:50:04 UTC 2021
    >   gaia-dev-20210205-worker02.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G   17G  2.3G  89% /
    >   ---- ----
    >   Node [worker03]
    >   Fri Feb  5 16:50:05 UTC 2021
    >   gaia-dev-20210205-worker03.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G   17G  2.4G  88% /
    >   ---- ----
    >   Node [worker04]
    >   Fri Feb  5 16:50:06 UTC 2021
    >   gaia-dev-20210205-worker04.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  3.4G   16G  18% /
    >   ---- ----
    >   Node [zeppelin]
    >   Fri Feb  5 16:50:06 UTC 2021
    >   gaia-dev-20210205-zeppelin.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.7G   15G  25% /


# -----------------------------------------------------
# Login to each node and monitor disc use.
# After the RandomForestClassifier fails with no space error ....
#[root@ansibler]

    nodes=(
        master01
        worker01
        worker02
        worker03
        worker04
        zeppelin
        )

    for node in "${nodes[@]}"
    do
        echo "---- ----"
        echo "Node [${node:?}]"

        ssh "fedora@${node:?}" \
            '
            date
            hostname
            df -h /
            '
    done


    >   ---- ----
    >   Node [master01]
    >   Fri Feb  5 16:52:27 UTC 2021
    >   gaia-dev-20210205-master01.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  3.4G   16G  19% /
    >   ---- ----
    >   Node [worker01]
    >   Fri Feb  5 16:52:28 UTC 2021
    >   gaia-dev-20210205-worker01.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G   16G  2.8G  86% /
    >   ---- ----
    >   Node [worker02]
    >   Fri Feb  5 16:52:28 UTC 2021
    >   gaia-dev-20210205-worker02.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G   17G  2.8G  86% /
    >   ---- ----
    >   Node [worker03]
    >   Fri Feb  5 16:52:28 UTC 2021
    >   gaia-dev-20210205-worker03.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G   16G  3.3G  83% /
    >   ---- ----
    >   Node [worker04]
    >   Fri Feb  5 16:52:28 UTC 2021
    >   gaia-dev-20210205-worker04.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  3.4G   16G  18% /
    >   ---- ----
    >   Node [zeppelin]
    >   Fri Feb  5 16:52:28 UTC 2021
    >   gaia-dev-20210205-zeppelin.novalocal
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  4.7G   15G  25% /

    #
    # worker[01-03] have 2.8G available, 86% used
    # worker04 still has 16G available, 18% used
    # master01 still has 16G available, 19% used
    # zeppelin has 15G available, 25% used
    #
    # So who is running out of space ?
    #


# -----------------------------------------------------
# Check what discs are mounted on a worker node.
#[root@ansibler]

    ssh 'fedora@worker01' \
        '
        date
        hostname
        cat /etc/fstab
        '

    >   Fri Feb  5 16:45:13 UTC 2021
    >   gaia-dev-20210205-worker01.novalocal
    >   
    >   #
    >   # /etc/fstab
    >   # Created by anaconda on Fri Apr 26 02:02:19 2019
    >   #
    >   # Accessible filesystems, by reference, are maintained under '/dev/disk/'.
    >   # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info.
    >   #
    >   # After editing this file, run 'systemctl daemon-reload' to update systemd
    >   # units generated from this file.
    >   #
    >   UUID=ea711a29-e460-4879-9d70-9da99ae021f9 /                       ext4    defaults        1 1
    >   /dev/vdc /data-01 btrfs defaults 0 0
    >   none /data/gaia/dr2 fuse.ceph ceph.id=aglais-gaia-dr2-ro,ceph.client_mountpoint=/volumes/_nogroup/2cdefe41-6c04-4865-9144-c0a7a183b424 ....
    >   none /data/gaia/edr3 fuse.ceph ceph.id=aglais-gaia-edr3-ro,ceph.client_mountpoint=/volumes/_nogroup/15d34c17-bd89-453e-98b7-478f93d45620 ....
    >   none /data/wise/allwise fuse.ceph ceph.id=aglais-wise-allwise-ro,ceph.client_mountpoint=/volumes/_nogroup/364a179c-010f-47a3-8698-5a3b5fa8fe15 ....
    >   none /data/panstarrs/dr1 fuse.ceph ceph.id=aglais-panstarrs-dr1-ro,ceph.client_mountpoint=/volumes/_nogroup/6e81787f-d07f-4f2e-b836-5d54a61955d8 ....
    >   none /data/twomass/allsky fuse.ceph ceph.id=aglais-twomass-allsky-ro,ceph.client_mountpoint=/volumes/_nogroup/fbfbaa88-01a4-4fc4-9ab2-3c70d67a6341 ....
    >   none /user/nch fuse.ceph ceph.id=userdata-nch-rw,ceph.client_mountpoint=/volumes/_nogroup/cfffdb2c-ceb4-4b1d-bf4a-01d6b9de73b1 ....
    >   none /user/zrq fuse.ceph ceph.id=userdata-zrq-rw,ceph.client_mountpoint=/volumes/_nogroup/21f1eaa9-c259-4744-9189-c4feae88611a ....
    >   none /user/stv fuse.ceph ceph.id=aglais-user-stv-rw,ceph.client_mountpoint=/volumes/_nogroup/38e37088-19a3-45f1-9a44-b6b2066b282d ....


# -----------------------------------------------------
# Check what discs are available on a worker node.
# https://unix.stackexchange.com/a/157155
#[root@ansibler]


    ssh 'fedora@worker01' \
        '
        date
        hostname
        lsblk
        '

    >   Fri Feb  5 16:59:05 UTC 2021
    >   gaia-dev-20210205-worker01.novalocal
    >   NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
    >   vda    252:0    0   20G  0 disk
    >   └─vda1 252:1    0   20G  0 part /
    >   vdb    252:16   0   60G  0 disk
    >   vdc    252:32   0  512G  0 disk /data-01

    #
    # So the extra 60G disc is not mounted anywhere ...
    #







