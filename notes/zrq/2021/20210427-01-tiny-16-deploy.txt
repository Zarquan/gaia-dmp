#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Target:

        Request to change the live deplyment configuration to 16 tiny worker nodes.
        https://github.com/wfau/aglais/issues/444

    Result:

        Work in progress
        Deployment with single worker succeded.
        File paths and links are all OK.
        Test notebook fails.


# -----------------------------------------------------
# Checkout the deployment branch.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

            git checkout '20210422-zrq-deployment'

    popd


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    AGLAIS_CLOUD=gaia-prod

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        atolmis/ansible-client:2020.12.02 \
        bash


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"



# -----------------------------------------------------
# Create everything, using the tiny-16 config.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}" \
            'tiny-16'

    >   real    71m22.710s
    >   user    20m3.402s
    >   sys     7m1.991s


# -----------------------------------------------------
# Add the Zeppelin user accounts.
#[root@ansibler]

    ssh zeppelin

        pushd zeppelin-0.8.2-bin-all/
            pushd conf/

                # Manual edit to add names and passwords
                vi shiro.ini

                    ....
                    ....
            popd

            # Restart Zeppelin for the changes to take.
            ./bin/zeppelin-daemon.sh restart

        popd


# -----------------------------------------------------
# Check the deployment status.
#[root@ansibler]

    cat '/tmp/aglais-status.yml'

    >   aglais:
    >     status:
    >       deployment:
    >         type: hadoop-yarn
    >         conf: tiny-16
    >         name: gaia-prod-20210427
    >         date: 20210427T003022
    >     spec:
    >       openstack:
    >         cloud: gaia-prod


# -----------------------------------------------------
# Get the public IP address of our Zeppelin node.
#[root@ansibler]

    deployname=$(
        yq read \
            '/tmp/aglais-status.yml' \
                'aglais.status.deployment.name'
        )

    zeppelinid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            server list \
                --format json \
        | jq -r '.[] | select(.Name == "'${deployname:?}'-zeppelin") | .ID'
        )

    zeppelinip=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            server show \
                --format json \
                "${zeppelinid:?}" \
        | jq -r '.addresses' \
        | sed '
            s/[[:space:]]//
            s/.*=\(.*\)/\1/
            s/.*,\(.*\)/\1/
            '
        )

cat << EOF
Zeppelin ID [${zeppelinid:?}]
Zeppelin IP [${zeppelinip:?}]
EOF

    >   Zeppelin ID [4d3791de-8582-4fc0-b9e8-632e65de3467]
    >   Zeppelin IP [128.232.227.150]


# -----------------------------------------------------
# Update our DNS entries.
#[root@ansibler]

    ssh root@infra-ops.aglais.uk

        vi /var/aglais/dnsmasq/hosts/gaia-prod.hosts

        ~   128.232.227.150  zeppelin.gaia-prod.aglais.uk


        podman kill --signal SIGHUP dnsmasq

        podman logs dnsmasq | tail

    >   ....
    >   dnsmasq[1]: cleared cache
    >   dnsmasq[1]: bad address at /etc/dnsmasq/hosts/gaia-prod.hosts line 1
    >   dnsmasq[1]: read /etc/dnsmasq/hosts/gaia-prod.hosts - 0 addresses
    >   dnsmasq[1]: read /etc/dnsmasq/hosts/gaia-test.hosts - 1 addresses
    >   dnsmasq[1]: read /etc/dnsmasq/hosts/gaia-dev.hosts - 1 addresses


# -----------------------------------------------------
# -----------------------------------------------------
# Login via Firefox
#[user@desktop]

    firefox --new-window "http://zeppelin.aglais.uk:8080/" &


# -----------------------------------------------------
# -----------------------------------------------------


    Import our Random Forest notebook from GitHub, clear the output and run all the cells ...

    Good astrometric solutions via ML Random Forest classifier
    https://raw.githubusercontent.com/wfau/aglais-notebooks/main/2FRPC4BFS/note.json


    RandomForestClassifier training fails
    Watching the Zeppelin server logs, it looks like it is working, then it goes through a section of errors, then recovers then fails ...

    Zeppelin eventually gives up and fails the cell.

    Stack trace in the Zeppelin notebook cell:

    >   ....
    >   ....
    >   Py4JJavaError: An error occurred while calling o191.fit.
    >   : org.apache.spark.SparkException:
    >       Job aborted due to stage failure:
    >           ResultStage 23 (collectAsMap at RandomForest.scala:567) has failed the maximum allowable number of times: 4.
    >           Most recent failure reason:
    >               org.apache.spark.shuffle.MetadataFetchFailedException:
    >                   Missing an output location for shuffle 9 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
    >                       at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
    >                           at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    >                               at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    >                                   at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
    >                                       at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
    >                                           at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
    >                                               at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
    >                                                   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >                                                       at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >                                                           at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    >                                                               at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >                                                                   at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >                                                                       at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    >                                                                           at org.apache.spark.scheduler.Task.run(Task.scala:123)
    >                                                                               at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    >                                                                                   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    >                                                                                       at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    >                                                                                           at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >                                                                                               at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >                                                                                                   at java.lang.Thread.run(Thread.java:748)
    >   	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)
    >   	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)
    >   	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)
    >   	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    >   	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    >   	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)
    >   	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1517)
    >   	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2143)
    >   	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)
    >   	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)
    >   	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    >   	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)
    >   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
    >   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
    >   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
    >   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
    >   	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)
    >   	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    >   	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    >   	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
    >   	at org.apache.spark.rdd.RDD.collect(RDD.scala:989)
    >   	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)
    >   	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)
    >   	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    >   	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    >   	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
    >   	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)
    >   	at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)
    >   	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)
    >   	at org.apache.spark.ml.classification.RandomForestClassifier$$anonfun$train$1.apply(RandomForestClassifier.scala:142)
    >   	at org.apache.spark.ml.classification.RandomForestClassifier$$anonfun$train$1.apply(RandomForestClassifier.scala:120)
    >   	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
    >   	at scala.util.Try$.apply(Try.scala:192)
    >   	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
    >   	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:120)
    >   	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)
    >   	at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)
    >   	at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    >   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >   	at java.lang.reflect.Method.invoke(Method.java:498)
    >   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    >   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    >   	at py4j.Gateway.invoke(Gateway.java:282)
    >   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    >   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    >   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    >   	at java.lang.Thread.run(Thread.java:748)


# -----------------------------------------------------
# Tail the zeppelin server log.
#[user@zeppelin]

    pushd /home/fedora/zeppelin-0.8.2-bin-all/logs

        tail -f zeppelin-interpreter-spark-fedora-gaia-prod-20210427-zeppelin.novalocal.log

    >   ....
    >   ....
    >    INFO [2021-04-27 08:02:33,265] ({dispatcher-event-loop-11} Logging.scala[logInfo]:54) - Starting task 137.0 in stage 23.0 (TID 68804, worker07, executor 1, partition 137, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:33,265] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 93.0 in stage 23.0 (TID 68760) in 2267 ms on worker07 (executor 1) (94/5721)
    >    INFO [2021-04-27 08:02:33,375] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 138.0 in stage 23.0 (TID 68805, worker09, executor 11, partition 138, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:33,375] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 99.0 in stage 23.0 (TID 68766) in 2012 ms on worker09 (executor 11) (95/5721)
    >    INFO [2021-04-27 08:02:33,622] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Starting task 139.0 in stage 23.0 (TID 68806, worker09, executor 11, partition 139, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:33,623] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 98.0 in stage 23.0 (TID 68765) in 2313 ms on worker09 (executor 11) (96/5721)
    >    INFO [2021-04-27 08:02:33,776] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Starting task 140.0 in stage 23.0 (TID 68807, worker15, executor 10, partition 140, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:33,776] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 84.0 in stage 23.0 (TID 68751) in 3375 ms on worker15 (executor 10) (97/5721)
    >    INFO [2021-04-27 08:02:33,895] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 141.0 in stage 23.0 (TID 68808, worker15, executor 10, partition 141, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:33,895] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 85.0 in stage 23.0 (TID 68752) in 3244 ms on worker15 (executor 10) (98/5721)
    >    INFO [2021-04-27 08:02:33,967] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Disabling executor 7.
    >    INFO [2021-04-27 08:02:33,974] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 7 (epoch 9)
    >    INFO [2021-04-27 08:02:33,974] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Trying to remove executor 7 from BlockManagerMaster.
    >    WARN [2021-04-27 08:02:33,976] ({dispatcher-event-loop-9} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_1720 !
    >    WARN [2021-04-27 08:02:33,976] ({dispatcher-event-loop-9} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_1592 !
    >    WARN [2021-04-27 08:02:33,976] ({dispatcher-event-loop-9} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_169 !
    >    WARN [2021-04-27 08:02:33,976] ({dispatcher-event-loop-9} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_2691 !
    >    WARN [2021-04-27 08:02:33,976] ({dispatcher-event-loop-9} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_752 !
    >    WARN [2021-04-27 08:02:33,976] ({dispatcher-event-loop-9} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_2835 !
    >    WARN [2021-04-27 08:02:33,976] ({dispatcher-event-loop-9} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_1473 !
    >    WARN [2021-04-27 08:02:33,976] ({dispatcher-event-loop-9} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_4337 !
    >   ....
    >   ....
    >   ....
    >   ....
    >    WARN [2021-04-27 08:02:34,009] ({dispatcher-event-loop-9} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_4189 !
    >    WARN [2021-04-27 08:02:34,009] ({dispatcher-event-loop-9} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_1797 !
    >    WARN [2021-04-27 08:02:34,009] ({dispatcher-event-loop-9} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_2039 !
    >    WARN [2021-04-27 08:02:34,009] ({dispatcher-event-loop-9} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_35 !
    >    INFO [2021-04-27 08:02:34,010] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(7, worker11, 42161, None)
    >    INFO [2021-04-27 08:02:34,011] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 7 successfully in removeExecutor
    >    INFO [2021-04-27 08:02:34,012] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 7 (epoch 9)
    >    INFO [2021-04-27 08:02:34,023] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 142.0 in stage 23.0 (TID 68809, worker15, executor 10, partition 142, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:34,024] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 86.0 in stage 23.0 (TID 68753) in 3303 ms on worker15 (executor 10) (99/5721)
    >    INFO [2021-04-27 08:02:34,144] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Starting task 143.0 in stage 23.0 (TID 68810, worker04, executor 8, partition 143, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:34,144] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 102.0 in stage 23.0 (TID 68769) in 2278 ms on worker04 (executor 8) (100/5721)
    >    INFO [2021-04-27 08:02:34,192] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 144.0 in stage 23.0 (TID 68811, worker15, executor 10, partition 144, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:34,192] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 94.0 in stage 23.0 (TID 68761) in 3137 ms on worker15 (executor 10) (101/5721)
    >    INFO [2021-04-27 08:02:34,266] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Starting task 145.0 in stage 23.0 (TID 68812, worker06, executor 4, partition 145, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:34,266] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Starting task 146.0 in stage 23.0 (TID 68813, worker04, executor 8, partition 146, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:34,266] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 103.0 in stage 23.0 (TID 68770) in 2299 ms on worker06 (executor 4) (102/5721)
    >    INFO [2021-04-27 08:02:34,267] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 100.0 in stage 23.0 (TID 68767) in 2552 ms on worker04 (executor 8) (103/5721)
    >    INFO [2021-04-27 08:02:34,361] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 122.0 in stage 23.0 (TID 68789) in 1854 ms on worker03 (executor 3) (104/5721)
    >    INFO [2021-04-27 08:02:34,361] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 147.0 in stage 23.0 (TID 68814, worker03, executor 3, partition 147, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:34,449] ({dispatcher-event-loop-11} Logging.scala[logInfo]:54) - Starting task 148.0 in stage 23.0 (TID 68815, worker06, executor 4, partition 148, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:34,449] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 107.0 in stage 23.0 (TID 68774) in 2327 ms on worker06 (executor 4) (105/5721)
    >   ....
    >   ....
    >    INFO [2021-04-27 08:02:34,566] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 155.0 in stage 23.0 (TID 68822, worker02, executor 5, partition 155, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:34,566] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 123.0 in stage 23.0 (TID 68790) in 2022 ms on worker02 (executor 5) (112/5721)
    >    INFO [2021-04-27 08:02:34,573] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Starting task 156.0 in stage 23.0 (TID 68823, worker04, executor 8, partition 156, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:34,573] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 110.0 in stage 23.0 (TID 68777) in 2384 ms on worker04 (executor 8) (113/5721)
    >    WARN [2021-04-27 08:02:34,608] ({dispatcher-event-loop-2} Logging.scala[logWarning]:66) - Requesting driver to remove executor 7 for reason Container from a bad node: container_1619486685111_0001_01_000009 on host: worker11. Exit status: 1. Diagnostics: [2021-04-27 08:02:34.032]Exception from container-launch.
    >   Container id: container_1619486685111_0001_01_000009
    >   Exit code: 1
    >   
    >   [2021-04-27 08:02:34.078]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
    >   Last 4096 bytes of prelaunch.err :
    >   Last 4096 bytes of stderr :
    >   k 3.0 in stage 23.0 (TID 68670). 1927 bytes result sent to driver
    >   2021-04-27 08:02:29,359 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68726
    >   2021-04-27 08:02:29,380 INFO executor.Executor: Running task 59.0 in stage 23.0 (TID 68726)
    >   2021-04-27 08:02:29,421 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:29,427 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 120 ms
    >   2021-04-27 08:02:29,472 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 71 ms
    >   2021-04-27 08:02:29,623 INFO executor.Executor: Finished task 14.0 in stage 23.0 (TID 68681). 1927 bytes result sent to driver
    >   2021-04-27 08:02:29,644 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68739
    >   2021-04-27 08:02:29,672 INFO executor.Executor: Running task 72.0 in stage 23.0 (TID 68739)
    >   ....
    >   ....
    >   2021-04-27 08:02:32,131 INFO executor.Executor: Running task 108.0 in stage 23.0 (TID 68775)
    >   2021-04-27 08:02:32,133 INFO executor.Executor: Finished task 72.0 in stage 23.0 (TID 68739). 1927 bytes result sent to driver
    >   2021-04-27 08:02:32,137 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68776
    >   2021-04-27 08:02:32,137 INFO executor.Executor: Running task 109.0 in stage 23.0 (TID 68776)
    >   2021-04-27 08:02:32,144 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:32,154 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:32,189 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 50 ms
    >   2021-04-27 08:02:32,239 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 106 ms
    >   2021-04-27 08:02:32,425 INFO executor.Executor: Finished task 81.0 in stage 23.0 (TID 68748). 1884 bytes result sent to driver
    >   2021-04-27 08:02:32,428 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68785
    >   2021-04-27 08:02:32,438 INFO executor.Executor: Running task 118.0 in stage 23.0 (TID 68785)
    >   2021-04-27 08:02:32,487 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:32,578 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 123 ms
    >   OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000007a4300000, 457179136, 0) failed; error='Cannot allocate memory' (errno=12)
    >   ....
    >   ....
    >   [2021-04-27 08:02:34.078]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
    >   Last 4096 bytes of prelaunch.err :
    >   Last 4096 bytes of stderr :
    >   k 3.0 in stage 23.0 (TID 68670). 1927 bytes result sent to driver
    >   2021-04-27 08:02:29,359 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68726
    >   2021-04-27 08:02:29,380 INFO executor.Executor: Running task 59.0 in stage 23.0 (TID 68726)
    >   2021-04-27 08:02:29,421 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:29,427 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 120 ms
    >   2021-04-27 08:02:29,472 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 71 ms
    >   2021-04-27 08:02:29,623 INFO executor.Executor: Finished task 14.0 in stage 23.0 (TID 68681). 1927 bytes result sent to driver
    >   2021-04-27 08:02:29,644 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68739
    >   2021-04-27 08:02:29,672 INFO executor.Executor: Running task 72.0 in stage 23.0 (TID 68739)
    >   ....
    >   ....
    >   2021-04-27 08:02:32,131 INFO executor.Executor: Running task 108.0 in stage 23.0 (TID 68775)
    >   2021-04-27 08:02:32,133 INFO executor.Executor: Finished task 72.0 in stage 23.0 (TID 68739). 1927 bytes result sent to driver
    >   2021-04-27 08:02:32,137 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68776
    >   2021-04-27 08:02:32,137 INFO executor.Executor: Running task 109.0 in stage 23.0 (TID 68776)
    >   2021-04-27 08:02:32,144 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:32,154 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:32,189 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 50 ms
    >   2021-04-27 08:02:32,239 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 106 ms
    >   2021-04-27 08:02:32,425 INFO executor.Executor: Finished task 81.0 in stage 23.0 (TID 68748). 1884 bytes result sent to driver
    >   2021-04-27 08:02:32,428 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68785
    >   2021-04-27 08:02:32,438 INFO executor.Executor: Running task 118.0 in stage 23.0 (TID 68785)
    >   2021-04-27 08:02:32,487 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:32,578 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 123 ms
    >   OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000007a4300000, 457179136, 0) failed; error='Cannot allocate memory' (errno=12)
    >   ....
    >   ....
    >   ERROR [2021-04-27 08:02:34,611] ({dispatcher-event-loop-5} Logging.scala[logError]:70) - Lost executor 7 on worker11: Container from a bad node: container_1619486685111_0001_01_000009 on host: worker11. Exit status: 1. Diagnostics: [2021-04-27 08:02:34.032]Exception from container-launch.
    >   Container id: container_1619486685111_0001_01_000009
    >   Exit code: 1
    >   
    >   [2021-04-27 08:02:34.078]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
    >   Last 4096 bytes of prelaunch.err :
    >   Last 4096 bytes of stderr :
    >   k 3.0 in stage 23.0 (TID 68670). 1927 bytes result sent to driver
    >   2021-04-27 08:02:29,359 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68726
    >   2021-04-27 08:02:29,380 INFO executor.Executor: Running task 59.0 in stage 23.0 (TID 68726)
    >   2021-04-27 08:02:29,421 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:29,427 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 120 ms
    >   2021-04-27 08:02:29,472 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 71 ms
    >   2021-04-27 08:02:29,623 INFO executor.Executor: Finished task 14.0 in stage 23.0 (TID 68681). 1927 bytes result sent to driver
    >   2021-04-27 08:02:29,644 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68739
    >   2021-04-27 08:02:29,672 INFO executor.Executor: Running task 72.0 in stage 23.0 (TID 68739)
    >   ....
    >   ....
    >   2021-04-27 08:02:32,131 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68775
    >   2021-04-27 08:02:32,131 INFO executor.Executor: Running task 108.0 in stage 23.0 (TID 68775)
    >   2021-04-27 08:02:32,133 INFO executor.Executor: Finished task 72.0 in stage 23.0 (TID 68739). 1927 bytes result sent to driver
    >   2021-04-27 08:02:32,137 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68776
    >   2021-04-27 08:02:32,137 INFO executor.Executor: Running task 109.0 in stage 23.0 (TID 68776)
    >   2021-04-27 08:02:32,144 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:32,154 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:32,189 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 50 ms
    >   2021-04-27 08:02:32,239 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 106 ms
    >   2021-04-27 08:02:32,425 INFO executor.Executor: Finished task 81.0 in stage 23.0 (TID 68748). 1884 bytes result sent to driver
    >   2021-04-27 08:02:32,428 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68785
    >   2021-04-27 08:02:32,438 INFO executor.Executor: Running task 118.0 in stage 23.0 (TID 68785)
    >   2021-04-27 08:02:32,487 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:32,578 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 123 ms
    >   OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000007a4300000, 457179136, 0) failed; error='Cannot allocate memory' (errno=12)
    >   ....
    >   ....
    >   [2021-04-27 08:02:34.078]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
    >   Last 4096 bytes of prelaunch.err :
    >   Last 4096 bytes of stderr :
    >   k 3.0 in stage 23.0 (TID 68670). 1927 bytes result sent to driver
    >   2021-04-27 08:02:29,359 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68726
    >   2021-04-27 08:02:29,380 INFO executor.Executor: Running task 59.0 in stage 23.0 (TID 68726)
    >   2021-04-27 08:02:29,421 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   ....
    >   ....
    >   2021-04-27 08:02:32,438 INFO executor.Executor: Running task 118.0 in stage 23.0 (TID 68785)
    >   2021-04-27 08:02:32,487 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:32,578 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 123 ms
    >   OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000007a4300000, 457179136, 0) failed; error='Cannot allocate memory' (errno=12)
    >   ....
    >   ....
    >    WARN [2021-04-27 08:02:34,617] ({dispatcher-event-loop-5} Logging.scala[logWarning]:66) - Lost task 108.0 in stage 23.0 (TID 68775, worker11, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1619486685111_0001_01_000009 on host: worker11. Exit status: 1. Diagnostics: [2021-04-27 08:02:34.032]Exception from container-launch.
    >   Container id: container_1619486685111_0001_01_000009
    >   Exit code: 1
    >   
    >   [2021-04-27 08:02:34.078]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
    >   Last 4096 bytes of prelaunch.err :
    >   Last 4096 bytes of stderr :
    >   k 3.0 in stage 23.0 (TID 68670). 1927 bytes result sent to driver
    >   2021-04-27 08:02:29,359 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68726
    >   2021-04-27 08:02:29,380 INFO executor.Executor: Running task 59.0 in stage 23.0 (TID 68726)
    >   2021-04-27 08:02:29,421 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:29,427 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 120 ms
    >   2021-04-27 08:02:29,472 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 71 ms
    >   2021-04-27 08:02:29,623 INFO executor.Executor: Finished task 14.0 in stage 23.0 (TID 68681). 1927 bytes result sent to driver
    >   2021-04-27 08:02:29,644 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68739
    >   2021-04-27 08:02:29,672 INFO executor.Executor: Running task 72.0 in stage 23.0 (TID 68739)
    >   2021-04-27 08:02:29,698 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:29,792 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 102 ms
    >   2021-04-27 08:02:29,854 INFO executor.Executor: Finished task 36.0 in stage 23.0 (TID 68703). 1970 bytes result sent to driver
    >   2021-04-27 08:02:29,855 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68748
    >   2021-04-27 08:02:29,855 INFO executor.Executor: Running task 81.0 in stage 23.0 (TID 68748)
    >   2021-04-27 08:02:29,877 INFO storage.ShuffleBlockFetcherIterator: Getting 5721 non-empty blocks including 527 local blocks and 5194 remote blocks
    >   2021-04-27 08:02:29,933 INFO storage.ShuffleBlockFetcherIterator: Started 10 remote fetches in 76 ms
    >   2021-04-27 08:02:31,831 INFO executor.Executor: Finished task 59.0 in stage 23.0 (TID 68726). 1927 bytes result sent to driver
    >   2021-04-27 08:02:31,834 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68768
    >   ....
    >   ....
    >   ....
    >   ....
    >    INFO [2021-04-27 08:02:44,828] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Starting task 178.0 in stage 23.0 (TID 68849, worker12, executor 12, partition 178, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:44,829] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Starting task 179.0 in stage 23.0 (TID 68850, worker12, executor 12, partition 179, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:02:44,829] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Starting task 180.0 in stage 23.0 (TID 68851, worker12, executor 12, partition 180, PROCESS_LOCAL, 7673 bytes)
    >    WARN [2021-04-27 08:02:44,830] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 174.0 in stage 23.0 (TID 68845, worker12, executor 12): FetchFailed(null, shuffleId=9, mapId=-1, reduceId=174, message=
    >   org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 9
    >   	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
    >   	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
    >   	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    >   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    >   	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
    >   	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
    >   	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
    >   	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    >   	at org.apache.spark.scheduler.Task.run(Task.scala:123)
    >   	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    >   	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    >   	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   
    >   )
    >    INFO [2021-04-27 08:02:44,831] ({task-result-getter-0} Logging.scala[logInfo]:54) - Task 174.0 in stage 23.0 (TID 68845) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
    >    INFO [2021-04-27 08:02:44,831] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Marking ResultStage 23 (collectAsMap at RandomForest.scala:567) as failed due to a fetch failure from ShuffleMapStage 22 (mapPartitions at RandomForest.scala:538)
    >    WARN [2021-04-27 08:02:44,831] ({task-result-getter-1} Logging.scala[logWarning]:66) - Lost task 177.0 in stage 23.0 (TID 68848, worker12, executor 12): FetchFailed(null, shuffleId=9, mapId=-1, reduceId=177, message=
    >   org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 9
    >   	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
    >   	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
    >   	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    >   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    >   	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
    >   	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
    >   	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
    >   	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    >   	at org.apache.spark.scheduler.Task.run(Task.scala:123)
    >   	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    >   	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    >   	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   ....
    >   ....
    >    INFO [2021-04-27 08:02:49,015] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Starting task 9.0 in stage 21.0 (TID 68861, worker09, executor 11, partition 102, PROCESS_LOCAL, 8426 bytes)
    >    WARN [2021-04-27 08:02:49,015] ({task-result-getter-1} Logging.scala[logWarning]:66) - Lost task 138.0 in stage 23.0 (TID 68805, worker09, executor 11): FetchFailed(BlockManagerId(7, worker11, 42161, None), shuffleId=9, mapId=1271, reduceId=138, message=
    >   org.apache.spark.shuffle.FetchFailedException: Failed to connect to worker11/10.10.1.243:42161
    >   	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:554)
    >   	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:485)
    >   	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
    >   	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
    >   	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
    >   	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    >   	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
    >   	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
    >   	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:156)
    >   	at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)
    >   	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:84)
    >   	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    >   	at org.apache.spark.scheduler.Task.run(Task.scala:123)
    >   	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    >   	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    >   	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.io.IOException: Failed to connect to worker11/10.10.1.243:42161
    >   	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
    >   	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
    >   	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:114)
    >   	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
    >   	at org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)
    >   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    >   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	... 1 more
    >   Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: worker11/10.10.1.243:42161
    >   Caused by: java.net.ConnectException: Connection refused
    >   	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    >   	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
    >   	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)
    >   	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
    >   	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)
    >   	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
    >   	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   ....
    >   ....
    >    INFO [2021-04-27 08:02:50,750] ({task-result-getter-3} Logging.scala[logInfo]:54) - Task 172.0 in stage 23.0 (TID 68843) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
    >    INFO [2021-04-27 08:02:50,753] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Added rdd_4_59 in memory on worker12:44355 (size: 158.7 KB, free: 6.8 GB)
    >    INFO [2021-04-27 08:02:50,776] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Starting task 49.0 in stage 21.0 (TID 68901, worker12, executor 12, partition 535, PROCESS_LOCAL, 8426 bytes)
    >    INFO [2021-04-27 08:02:50,776] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 7.0 in stage 21.0 (TID 68859) in 1961 ms on worker12 (executor 12) (7/527)
    >    INFO [2021-04-27 08:02:50,799] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Starting task 50.0 in stage 21.0 (TID 68902, worker08, executor 6, partition 552, PROCESS_LOCAL, 8426 bytes)
    >    WARN [2021-04-27 08:02:50,799] ({task-result-getter-0} Logging.scala[logWarning]:66) - Lost task 173.0 in stage 23.0 (TID 68844, worker08, executor 6): FetchFailed(BlockManagerId(7, worker11, 42161, None), shuffleId=9, mapId=2, reduceId=173, message=
    >   org.apache.spark.shuffle.FetchFailedException: Failed to connect to worker11/10.10.1.243:42161
    >   	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:554)
    >   	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:485)
    >   	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64)
    >   	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
    >   	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
    >   	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
    >   	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
    >   	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
    >   	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:156)
    >   	at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)
    >   	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:84)
    >   	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    >   	at org.apache.spark.scheduler.Task.run(Task.scala:123)
    >   	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    >   	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    >   	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.io.IOException: Failed to connect to worker11/10.10.1.243:42161
    >   	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
    >   	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
    >   	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:114)
    >   	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
    >   	at org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)
    >   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    >   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	... 1 more
    >   Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: worker11/10.10.1.243:42161
    >   Caused by: java.net.ConnectException: Connection refused
    >   	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    >   	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
    >   	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)
    >   	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
    >   	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)
    >   	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
    >   	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   ....
    >   ....
    >    INFO [2021-04-27 08:02:50,799] ({task-result-getter-0} Logging.scala[logInfo]:54) - Task 173.0 in stage 23.0 (TID 68844) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
    >    INFO [2021-04-27 08:02:50,799] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 23.0, whose tasks have all completed, from pool
    >    INFO [2021-04-27 08:02:50,809] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added rdd_4_88 in memory on worker09:37117 (size: 91.4 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:02:50,812] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 51.0 in stage 21.0 (TID 68903, worker09, executor 11, partition 569, PROCESS_LOCAL, 8426 bytes)
    >    INFO [2021-04-27 08:02:50,812] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 8.0 in stage 21.0 (TID 68860) in 1800 ms on worker09 (executor 11) (8/527)
    >    INFO [2021-04-27 08:02:50,824] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitting failed stages
    >    INFO [2021-04-27 08:02:50,856] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Added rdd_4_56 in memory on worker12:44355 (size: 147.7 KB, free: 6.8 GB)
    >    INFO [2021-04-27 08:02:50,887] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 52.0 in stage 21.0 (TID 68904, worker12, executor 12, partition 571, PROCESS_LOCAL, 8426 bytes)
    >    INFO [2021-04-27 08:02:50,887] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 5.0 in stage 21.0 (TID 68857) in 2077 ms on worker12 (executor 12) (9/527)
    >    INFO [2021-04-27 08:02:50,951] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Added rdd_4_111 in memory on worker15:34207 (size: 155.1 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:02:50,956] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 53.0 in stage 21.0 (TID 68905, worker15, executor 10, partition 596, PROCESS_LOCAL, 8426 bytes)
    >    INFO [2021-04-27 08:02:50,956] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 10.0 in stage 21.0 (TID 68862) in 1926 ms on worker15 (executor 10) (10/527)
    >    INFO [2021-04-27 08:02:51,180] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Added rdd_4_102 in memory on worker09:37117 (size: 126.1 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:02:51,185] ({dispatcher-event-loop-11} Logging.scala[logInfo]:54) - Starting task 54.0 in stage 21.0 (TID 68906, worker09, executor 11, partition 605, PROCESS_LOCAL, 8426 bytes)
    >    INFO [2021-04-27 08:02:51,185] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added rdd_4_166 in memory on worker04:44191 (size: 141.1 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:02:51,186] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 9.0 in stage 21.0 (TID 68861) in 2171 ms on worker09 (executor 11) (11/527)
    >    INFO [2021-04-27 08:02:51,190] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 55.0 in stage 21.0 (TID 68907, worker04, executor 8, partition 610, PROCESS_LOCAL, 8426 bytes)
    >    INFO [2021-04-27 08:02:51,190] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 15.0 in stage 21.0 (TID 68867) in 1856 ms on worker04 (executor 8) (12/527)
    >    INFO [2021-04-27 08:02:51,456] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Added rdd_4_57 in memory on worker12:44355 (size: 200.6 KB, free: 6.8 GB)
    >    INFO [2021-04-27 08:02:51,465] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Starting task 56.0 in stage 21.0 (TID 68908, worker12, executor 12, partition 632, PROCESS_LOCAL, 8426 bytes)
    >    INFO [2021-04-27 08:02:51,465] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 6.0 in stage 21.0 (TID 68858) in 2650 ms on worker12 (executor 12) (13/527)
    >    INFO [2021-04-27 08:02:51,819] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added rdd_4_210 in memory on worker07:41055 (size: 84.3 KB, free: 6.7 GB)
    >   ....
    >   ....
    >    INFO [2021-04-27 08:03:24,661] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 517.0 in stage 21.0 (TID 69369) in 2953 ms on worker06 (executor 4) (523/527)
    >    INFO [2021-04-27 08:03:24,733] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Added rdd_4_5318 in memory on worker06:43541 (size: 67.4 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:03:24,736] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 488.0 in stage 21.0 (TID 69340) in 5194 ms on worker06 (executor 4) (524/527)
    >    INFO [2021-04-27 08:03:24,741] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Added rdd_4_5552 in memory on worker15:34207 (size: 39.2 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:03:24,743] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 510.0 in stage 21.0 (TID 69362) in 3442 ms on worker15 (executor 10) (525/527)
    >    INFO [2021-04-27 08:03:24,769] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Added rdd_4_5657 in memory on worker12:44355 (size: 58.4 KB, free: 6.8 GB)
    >    INFO [2021-04-27 08:03:24,777] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 520.0 in stage 21.0 (TID 69372) in 2882 ms on worker12 (executor 12) (526/527)
    >    INFO [2021-04-27 08:03:24,800] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added rdd_4_5363 in memory on worker16:45157 (size: 41.5 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:03:24,802] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 493.0 in stage 21.0 (TID 69345) in 4811 ms on worker16 (executor 2) (527/527)
    >    INFO [2021-04-27 08:03:24,802] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 21.0, whose tasks have all completed, from pool
    >    INFO [2021-04-27 08:03:24,802] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 21 (rdd at Classifier.scala:82) finished in 39.742 s
    >    INFO [2021-04-27 08:03:24,802] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
    >    INFO [2021-04-27 08:03:24,802] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
    >    INFO [2021-04-27 08:03:24,803] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ShuffleMapStage 22, ResultStage 23)
    >    INFO [2021-04-27 08:03:24,803] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
    >    INFO [2021-04-27 08:03:24,816] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 22 (MapPartitionsRDD[104] at mapPartitions at RandomForest.scala:538), which has no missing parents
    >    INFO [2021-04-27 08:03:24,825] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_24 stored as values in memory (estimated size 223.4 KB, free 6.8 GB)
    >    INFO [2021-04-27 08:03:24,827] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_24_piece0 stored as bytes in memory (estimated size 76.5 KB, free 6.8 GB)
    >    INFO [2021-04-27 08:03:24,827] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_24_piece0 in memory on zeppelin:40829 (size: 76.5 KB, free: 6.8 GB)
    >    INFO [2021-04-27 08:03:24,827] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 24 from broadcast at DAGScheduler.scala:1184
    >    INFO [2021-04-27 08:03:24,828] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 527 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[104] at mapPartitions at RandomForest.scala:538) (first 15 tasks are for partitions Vector(2, 13, 24, 35, 55, 56, 57, 59, 88, 102, 111, 119, 123, 140, 154))
    >    INFO [2021-04-27 08:03:24,828] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 22.1 with 527 tasks
    >    INFO [2021-04-27 08:03:24,829] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 16.0 in stage 22.1 (TID 69379, worker06, executor 4, partition 169, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,829] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 8.0 in stage 22.1 (TID 69380, worker09, executor 11, partition 88, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,829] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 10.0 in stage 22.1 (TID 69381, worker15, executor 10, partition 111, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,830] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 17.0 in stage 22.1 (TID 69382, worker03, executor 3, partition 180, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,830] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 13.0 in stage 22.1 (TID 69383, worker04, executor 8, partition 140, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,830] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 25.0 in stage 22.1 (TID 69384, worker02, executor 5, partition 263, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,830] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 43.0 in stage 22.1 (TID 69385, worker08, executor 6, partition 477, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,830] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 21.0 in stage 22.1 (TID 69386, worker13, executor 9, partition 221, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,830] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 24.0 in stage 22.1 (TID 69387, worker16, executor 2, partition 249, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,830] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 22.1 (TID 69388, worker12, executor 12, partition 2, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,830] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 19.0 in stage 22.1 (TID 69389, worker07, executor 1, partition 210, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,830] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 18.0 in stage 22.1 (TID 69390, worker06, executor 4, partition 205, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,830] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 9.0 in stage 22.1 (TID 69391, worker09, executor 11, partition 102, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:03:24,830] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 11.0 in stage 22.1 (TID 69392, worker15, executor 10, partition 119, PROCESS_LOCAL, 8535 bytes)
    >   ....
    >   ....
    >    INFO [2021-04-27 08:03:28,630] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_25_piece0 in memory on worker07:41055 (size: 6.6 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:03:28,631] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_25_piece0 in memory on worker15:34207 (size: 6.6 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:03:28,631] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_25_piece0 in memory on worker12:44355 (size: 6.6 KB, free: 6.8 GB)
    >    INFO [2021-04-27 08:03:28,631] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_25_piece0 in memory on worker08:41649 (size: 6.6 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:03:28,631] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 9 to 10.10.3.132:40962
    >    INFO [2021-04-27 08:03:28,632] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 9 to 10.10.1.44:50466
    >    INFO [2021-04-27 08:03:28,632] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 9 to 10.10.0.22:44760
    >    INFO [2021-04-27 08:03:28,632] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 9 to 10.10.2.207:60612
    >    INFO [2021-04-27 08:03:28,633] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 9 to 10.10.1.171:33956
    >    INFO [2021-04-27 08:03:28,633] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 9 to 10.10.0.104:37076
    >    INFO [2021-04-27 08:03:28,638] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 9 to 10.10.0.130:40768
    >    INFO [2021-04-27 08:03:28,647] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 9 to 10.10.1.212:48258
    >    INFO [2021-04-27 08:03:36,920] ({dispatcher-event-loop-11} Logging.scala[logInfo]:54) - Starting task 44.0 in stage 23.1 (TID 69950, worker06, executor 4, partition 178, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:03:36,920] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 35.0 in stage 23.1 (TID 69941) in 8297 ms on worker06 (executor 4) (1/5587)
    >    INFO [2021-04-27 08:03:38,904] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Starting task 45.0 in stage 23.1 (TID 69951, worker16, executor 2, partition 179, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:03:38,904] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 6.0 in stage 23.1 (TID 69912) in 10286 ms on worker16 (executor 2) (2/5587)
    >    INFO [2021-04-27 08:03:39,274] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Starting task 46.0 in stage 23.1 (TID 69952, worker07, executor 1, partition 180, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:03:39,275] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 29.0 in stage 23.1 (TID 69935) in 10653 ms on worker07 (executor 1) (3/5587)
    >    INFO [2021-04-27 08:03:39,706] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 47.0 in stage 23.1 (TID 69953, worker02, executor 5, partition 181, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:03:39,706] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 31.0 in stage 23.1 (TID 69937) in 11084 ms on worker02 (executor 5) (4/5587)
    >    INFO [2021-04-27 08:03:39,843] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 48.0 in stage 23.1 (TID 69954, worker02, executor 5, partition 182, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:03:39,843] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 20.0 in stage 23.1 (TID 69926) in 11222 ms on worker02 (executor 5) (5/5587)
    >    INFO [2021-04-27 08:03:39,978] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Starting task 49.0 in stage 23.1 (TID 69955, worker02, executor 5, partition 183, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:03:39,978] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 42.0 in stage 23.1 (TID 69948) in 11354 ms on worker02 (executor 5) (6/5587)
    >    INFO [2021-04-27 08:04:12,627] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Disabling executor 9.
    >    INFO [2021-04-27 08:04:12,627] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 9 (epoch 52)
    >    INFO [2021-04-27 08:04:12,628] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Trying to remove executor 9 from BlockManagerMaster.
    >    WARN [2021-04-27 08:04:12,628] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_1931 !
    >    WARN [2021-04-27 08:04:12,629] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_167 !
    >    WARN [2021-04-27 08:04:12,629] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_5450 !
    >    WARN [2021-04-27 08:04:12,629] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_1358 !
    >    WARN [2021-04-27 08:04:12,629] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_5534 !
    >    WARN [2021-04-27 08:04:12,629] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_2730 !
    >    WARN [2021-04-27 08:04:12,629] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_1570 !
    >    WARN [2021-04-27 08:04:12,629] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_1460 !
    >    WARN [2021-04-27 08:04:12,629] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_5395 !
    >   ....
    >   ....
    >   ....
    >   ....
    >    WARN [2021-04-27 08:04:12,689] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_5356 !
    >    WARN [2021-04-27 08:04:12,689] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_952 !
    >    WARN [2021-04-27 08:04:12,689] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_1776 !
    >    WARN [2021-04-27 08:04:12,689] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_1693 !
    >    WARN [2021-04-27 08:04:12,689] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_4866 !
    >    WARN [2021-04-27 08:04:12,689] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_1409 !
    >    WARN [2021-04-27 08:04:12,689] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_5499 !
    >    WARN [2021-04-27 08:04:12,689] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_103_1902 !
    >    WARN [2021-04-27 08:04:12,689] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - No more replicas available for rdd_4_2818 !
    >    INFO [2021-04-27 08:04:12,689] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(9, worker13, 41359, None)
    >    INFO [2021-04-27 08:04:12,690] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 9 successfully in removeExecutor
    >    INFO [2021-04-27 08:04:12,690] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 9 (epoch 52)
    >   ERROR [2021-04-27 08:04:13,240] ({dispatcher-event-loop-1} Logging.scala[logError]:70) - Lost executor 9 on worker13: Container from a bad node: container_1619486685111_0001_01_000011 on host: worker13. Exit status: 137. Diagnostics: [2021-04-27 08:04:12.705]Container killed on request. Exit code is 137
    >   [2021-04-27 08:04:12.708]Container exited with a non-zero exit code 137.
    >   [2021-04-27 08:04:12.723]Killed by external signal
    >   .
    >    WARN [2021-04-27 08:04:13,240] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - Lost task 41.0 in stage 23.1 (TID 69947, worker13, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1619486685111_0001_01_000011 on host: worker13. Exit status: 137. Diagnostics: [2021-04-27 08:04:12.705]Container killed on request. Exit code is 137
    >   [2021-04-27 08:04:12.708]Container exited with a non-zero exit code 137.
    >   [2021-04-27 08:04:12.723]Killed by external signal
    >   .
    >    WARN [2021-04-27 08:04:13,240] ({dispatcher-event-loop-0} Logging.scala[logWarning]:66) - Requesting driver to remove executor 9 for reason Container from a bad node: container_1619486685111_0001_01_000011 on host: worker13. Exit status: 137. Diagnostics: [2021-04-27 08:04:12.705]Container killed on request. Exit code is 137
    >   [2021-04-27 08:04:12.708]Container exited with a non-zero exit code 137.
    >   [2021-04-27 08:04:12.723]Killed by external signal
    >   .
    >    WARN [2021-04-27 08:04:13,241] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - Lost task 8.0 in stage 23.1 (TID 69914, worker13, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1619486685111_0001_01_000011 on host: worker13. Exit status: 137. Diagnostics: [2021-04-27 08:04:12.705]Container killed on request. Exit code is 137
    >   [2021-04-27 08:04:12.708]Container exited with a non-zero exit code 137.
    >   [2021-04-27 08:04:12.723]Killed by external signal
    >   .
    >    WARN [2021-04-27 08:04:13,241] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - Lost task 19.0 in stage 23.1 (TID 69925, worker13, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1619486685111_0001_01_000011 on host: worker13. Exit status: 137. Diagnostics: [2021-04-27 08:04:12.705]Container killed on request. Exit code is 137
    >   [2021-04-27 08:04:12.708]Container exited with a non-zero exit code 137.
    >   [2021-04-27 08:04:12.723]Killed by external signal
    >   .
    >    WARN [2021-04-27 08:04:13,241] ({dispatcher-event-loop-1} Logging.scala[logWarning]:66) - Lost task 30.0 in stage 23.1 (TID 69936, worker13, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1619486685111_0001_01_000011 on host: worker13. Exit status: 137. Diagnostics: [2021-04-27 08:04:12.705]Container killed on request. Exit code is 137
    >   [2021-04-27 08:04:12.708]Container exited with a non-zero exit code 137.
    >   [2021-04-27 08:04:12.723]Killed by external signal
    >   .
    >    INFO [2021-04-27 08:04:13,241] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removal of executor 9 requested
    >    INFO [2021-04-27 08:04:13,241] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Asked to remove non-existent executor 9
    >    INFO [2021-04-27 08:04:13,241] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Trying to remove executor 9 from BlockManagerMaster.
    >    INFO [2021-04-27 08:04:24,587] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.10.1.243:58622) with ID 13
    >    INFO [2021-04-27 08:04:24,588] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 30.1 in stage 23.1 (TID 69956, worker11, executor 13, partition 164, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:04:24,589] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 19.1 in stage 23.1 (TID 69957, worker11, executor 13, partition 153, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:04:24,589] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 8.1 in stage 23.1 (TID 69958, worker11, executor 13, partition 142, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:04:24,589] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 41.1 in stage 23.1 (TID 69959, worker11, executor 13, partition 175, PROCESS_LOCAL, 7673 bytes)
    >    INFO [2021-04-27 08:04:24,700] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Registering block manager worker11:34691 with 6.8 GB RAM, BlockManagerId(13, worker11, 34691, None)
    >    INFO [2021-04-27 08:04:25,132] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Added broadcast_25_piece0 in memory on worker11:34691 (size: 6.6 KB, free: 6.8 GB)
    >    INFO [2021-04-27 08:04:25,474] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 9 to 10.10.1.243:58622
    >    INFO [2021-04-27 08:04:25,580] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 50.0 in stage 23.1 (TID 69960, worker11, executor 13, partition 184, PROCESS_LOCAL, 7673 bytes)
    >    WARN [2021-04-27 08:04:25,580] ({task-result-getter-2} Logging.scala[logWarning]:66) - Lost task 41.1 in stage 23.1 (TID 69959, worker11, executor 13): FetchFailed(null, shuffleId=9, mapId=-1, reduceId=175, message=
    >   org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 9
    >   	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
    >   	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
    >   	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    >   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    >   	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
    >   	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
    >   	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
    >   	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    >   	at org.apache.spark.scheduler.Task.run(Task.scala:123)
    >   	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    >   	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    >   	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   
    >   )
    >    INFO [2021-04-27 08:04:25,580] ({task-result-getter-2} Logging.scala[logInfo]:54) - Task 41.1 in stage 23.1 (TID 69959) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
    >    INFO [2021-04-27 08:04:25,581] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Marking ResultStage 23 (collectAsMap at RandomForest.scala:567) as failed due to a fetch failure from ShuffleMapStage 22 (mapPartitions at RandomForest.scala:538)
    >    WARN [2021-04-27 08:04:25,581] ({task-result-getter-3} Logging.scala[logWarning]:66) - Lost task 30.1 in stage 23.1 (TID 69956, worker11, executor 13): FetchFailed(null, shuffleId=9, mapId=-1, reduceId=164, message=
    >   org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 9
    >   	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
    >   	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
    >   	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    >   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    >   	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
    >   	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
    >   	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
    >   	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    >   	at org.apache.spark.scheduler.Task.run(Task.scala:123)
    >   	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    >   	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    >   	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   
    >   )
    >    INFO [2021-04-27 08:04:25,581] ({task-result-getter-3} Logging.scala[logInfo]:54) - Task 30.1 in stage 23.1 (TID 69956) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
    >    INFO [2021-04-27 08:04:25,581] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 23 (collectAsMap at RandomForest.scala:567) failed in 56.973 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 9
    >   	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
    >   	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
    >   	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    >   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    >   	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
    >   	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
    >   	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
    >   	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    >   	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
    >   	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
    >   	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    >   	at org.apache.spark.scheduler.Task.run(Task.scala:123)
    >   	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    >   	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    >   	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   ....
    >   ....
    >   ....
    >   ....
    >   ....
    >   ....
    >    INFO [2021-04-27 08:05:07,108] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 531.0 in stage 21.1 (TID 70492) in 3561 ms on worker02 (executor 5) (543/558)
    >    INFO [2021-04-27 08:05:07,116] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Added rdd_4_5450 in memory on worker08:41649 (size: 52.3 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:05:07,119] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 532.0 in stage 21.1 (TID 70493) in 3466 ms on worker08 (executor 6) (544/558)
    >   ....
    >   ....
    >    INFO [2021-04-27 08:05:08,000] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 556.0 in stage 21.1 (TID 70517) in 2410 ms on worker07 (executor 1) (557/558)
    >    INFO [2021-04-27 08:05:08,395] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Added rdd_4_5669 in memory on worker04:44191 (size: 143.2 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:05:08,398] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 553.0 in stage 21.1 (TID 70514) in 3367 ms on worker04 (executor 8) (558/558)
    >    INFO [2021-04-27 08:05:08,398] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 21.1, whose tasks have all completed, from pool
    >    INFO [2021-04-27 08:05:08,398] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ShuffleMapStage 21 (rdd at Classifier.scala:82) finished in 42.597 s
    >    INFO [2021-04-27 08:05:08,398] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - looking for newly runnable stages
    >    INFO [2021-04-27 08:05:08,398] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - running: Set()
    >    INFO [2021-04-27 08:05:08,398] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - waiting: Set(ShuffleMapStage 22, ResultStage 23)
    >    INFO [2021-04-27 08:05:08,398] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - failed: Set()
    >    INFO [2021-04-27 08:05:08,411] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ShuffleMapStage 22 (MapPartitionsRDD[104] at mapPartitions at RandomForest.scala:538), which has no missing parents
    >    INFO [2021-04-27 08:05:08,420] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_27 stored as values in memory (estimated size 223.4 KB, free 6.8 GB)
    >    INFO [2021-04-27 08:05:08,422] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_27_piece0 stored as bytes in memory (estimated size 76.5 KB, free 6.8 GB)
    >    INFO [2021-04-27 08:05:08,422] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_27_piece0 in memory on zeppelin:40829 (size: 76.5 KB, free: 6.8 GB)
    >    INFO [2021-04-27 08:05:08,423] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 27 from broadcast at DAGScheduler.scala:1184
    >    INFO [2021-04-27 08:05:08,424] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 558 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[104] at mapPartitions at RandomForest.scala:538) (first 15 tasks are for partitions Vector(1, 12, 23, 34, 67, 68, 69, 89, 112, 122, 126, 130, 155, 161, 164))
    >    INFO [2021-04-27 08:05:08,424] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 22.2 with 558 tasks
    >    INFO [2021-04-27 08:05:08,425] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 14.0 in stage 22.2 (TID 70519, worker15, executor 10, partition 164, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:05:08,425] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 22.2 (TID 70520, worker16, executor 2, partition 68, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:05:08,425] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 22.0 in stage 22.2 (TID 70521, worker12, executor 12, partition 238, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:05:08,425] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 7.0 in stage 22.2 (TID 70522, worker03, executor 3, partition 89, PROCESS_LOCAL, 8535 bytes)
    >   ....
    >   ....
    >    INFO [2021-04-27 08:05:08,428] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 37.0 in stage 22.2 (TID 70560, worker04, executor 8, partition 368, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:05:08,428] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 27.0 in stage 22.2 (TID 70561, worker08, executor 6, partition 280, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:05:08,428] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 38.0 in stage 22.2 (TID 70562, worker09, executor 11, partition 382, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:05:08,434] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Added broadcast_27_piece0 in memory on worker16:45157 (size: 76.5 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:05:08,434] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Added broadcast_27_piece0 in memory on worker03:46639 (size: 76.5 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:05:08,434] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Added broadcast_27_piece0 in memory on worker07:41055 (size: 76.5 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:05:08,435] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_27_piece0 in memory on worker06:43541 (size: 76.5 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:05:08,435] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_27_piece0 in memory on worker04:44191 (size: 76.5 KB, free: 6.7 GB)
    >   ....
    >   ....
    >    INFO [2021-04-27 08:05:08,486] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Added rdd_103_302 in memory on worker09:37117 (size: 101.3 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:05:08,487] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Added rdd_103_126 in memory on worker03:46639 (size: 166.1 KB, free: 6.7 GB)
    >    INFO [2021-04-27 08:05:08,492] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 51.0 in stage 22.2 (TID 70563, worker07, executor 1, partition 491, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:05:08,493] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 25.0 in stage 22.2 (TID 70534) in 67 ms on worker07 (executor 1) (1/558)
    >    INFO [2021-04-27 08:05:08,494] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 49.0 in stage 22.2 (TID 70564, worker08, executor 6, partition 480, PROCESS_LOCAL, 8535 bytes)
    >   ....
    >   ....
    >    INFO [2021-04-27 08:05:08,547] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 30.0 in stage 22.2 (TID 70542) in 120 ms on worker16 (executor 2) (10/558)
    >    INFO [2021-04-27 08:05:08,550] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Starting task 73.0 in stage 22.2 (TID 70573, worker08, executor 6, partition 744, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:05:08,551] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 8.0 in stage 22.2 (TID 70528) in 125 ms on worker08 (executor 6) (11/558)
    >    INFO [2021-04-27 08:05:08,552] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 63.0 in stage 22.2 (TID 70574, worker03, executor 3, partition 639, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:05:08,552] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 74.0 in stage 22.2 (TID 70575, worker03, executor 3, partition 757, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:05:08,552] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 32.0 in stage 22.2 (TID 70544) in 125 ms on worker03 (executor 3) (12/558)
    >   ....
    >   ....
    >    INFO [2021-04-27 08:05:08,562] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 72.0 in stage 22.2 (TID 70583, worker15, executor 10, partition 736, PROCESS_LOCAL, 8535 bytes)
    >    INFO [2021-04-27 08:05:08,562] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 46.0 in stage 22.2 (TID 70565) in 57 ms on worker15 (executor 10) (21/558)
    >    INFO [2021-04-27 08:05:08,564] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Added rdd_103_490 in memory on worker16:45157 (size: 16.0 B, free: 6.7 GB)
    >   ....
    >   ....
    >   ....
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------

    #
    # Run the RandomForestClassifier training cell again from the Zewppelin UI.
    # Fails with the same error as before.
    # Took 25 min 6 sec. Last updated by zrq at April 27 2021, 11:45:26 AM.
    #
    # Final stages of the Zeppelin server log shows ClosedChannelException
    # errors while trying to connect to the workers.
    #

# -----------------------------------------------------
# Tail the zeppelin server log.
#[user@zeppelin]

    pushd /home/fedora/zeppelin-0.8.2-bin-all/logs

        tail -f zeppelin-interpreter-spark-fedora-gaia-prod-20210427-zeppelin.novalocal.log

    >   ....
    >   ....
    >   ....
    >   ....
    >   ....
    >   ....
    >    INFO [2021-04-27 10:54:37,330] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 974
    >    INFO [2021-04-27 10:54:37,330] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1589
    >    INFO [2021-04-27 10:54:37,330] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1536
    >    INFO [2021-04-27 10:54:37,330] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1097
    >    INFO [2021-04-27 10:54:37,330] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1263
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1568
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1598
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1570
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1310
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1587
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1553
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1272
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 973
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1543
    >   ERROR [2021-04-27 10:54:37,331] ({rpc-server-4-8} TransportClient.java[operationComplete]:336) - Failed to send RPC RPC 7876516133116856383 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   java.nio.channels.ClosedChannelException
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >   	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
    >   	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned shuffle 15
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1544
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1599
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 982
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 971
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1287
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1290
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1548
    >    INFO [2021-04-27 10:54:37,331] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1110
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1555
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1592
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1277
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1094
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1585
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1109
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1103
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 979
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1261
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1579
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1098
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1108
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1269
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1557
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1298
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1602
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1560
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1283
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1597
    >    INFO [2021-04-27 10:54:37,332] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1550
    >   ERROR [2021-04-27 10:54:37,332] ({rpc-server-4-8} TransportClient.java[operationComplete]:336) - Failed to send RPC RPC 5779305627091068292 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   java.nio.channels.ClosedChannelException
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >   	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
    >   	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >    INFO [2021-04-27 10:54:37,333] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Removed broadcast_44_piece0 on zeppelin:40829 in memory (size: 76.5 KB, free: 6.8 GB)
    >    WARN [2021-04-27 10:54:37,333] ({block-manager-ask-thread-pool-755} Logging.scala[logWarning]:87) - Error trying to remove broadcast 44 from block manager BlockManagerId(2, worker16, 45157, None)
    >   java.io.IOException: Failed to send RPC RPC 5779305627091068292 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   	at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:362)
    >   	at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:339)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)
    >   	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)
    >   	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)
    >   	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >   	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
    >   	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.nio.channels.ClosedChannelException
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >   	... 12 more
    >    INFO [2021-04-27 10:54:37,334] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_44_piece0 on worker05:36419 in memory (size: 76.5 KB, free: 6.7 GB)
    >    INFO [2021-04-27 10:54:37,345] ({dispatcher-event-loop-11} Logging.scala[logInfo]:54) - Removed broadcast_44_piece0 on worker10:37993 in memory (size: 76.5 KB, free: 6.7 GB)
    >    INFO [2021-04-27 10:54:37,350] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Removed broadcast_44_piece0 on worker11:34691 in memory (size: 76.5 KB, free: 6.6 GB)
    >    INFO [2021-04-27 10:54:37,403] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Removed broadcast_44_piece0 on worker08:38635 in memory (size: 76.5 KB, free: 6.7 GB)
    >    INFO [2021-04-27 10:54:37,434] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_44_piece0 on worker07:43931 in memory (size: 76.5 KB, free: 6.7 GB)
    >    INFO [2021-04-27 10:54:37,435] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1595
    >    INFO [2021-04-27 10:54:37,435] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1303
    >    INFO [2021-04-27 10:54:37,435] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1546
    >    INFO [2021-04-27 10:54:37,435] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1603
    >    INFO [2021-04-27 10:54:37,435] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1569
    >    INFO [2021-04-27 10:54:37,435] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1101
    >    INFO [2021-04-27 10:54:37,435] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 975
    >    INFO [2021-04-27 10:54:37,435] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1566
    >    INFO [2021-04-27 10:54:37,435] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1608
    >   ERROR [2021-04-27 10:54:37,436] ({rpc-server-4-8} TransportClient.java[operationComplete]:336) - Failed to send RPC RPC 5918819581121256883 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   java.nio.channels.ClosedChannelException
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >   	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
    >   	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned shuffle 10
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 977
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1580
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1539
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1538
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1593
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 972
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1591
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1306
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1104
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1567
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1270
    >    INFO [2021-04-27 10:54:37,436] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1099
    >   ERROR [2021-04-27 10:54:37,436] ({rpc-server-4-8} TransportClient.java[operationComplete]:336) - Failed to send RPC RPC 6324932883571440703 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   java.nio.channels.ClosedChannelException
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >   	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
    >   	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >    WARN [2021-04-27 10:54:37,437] ({block-manager-ask-thread-pool-694} Logging.scala[logWarning]:87) - Error trying to remove broadcast 43 from block manager BlockManagerId(2, worker16, 45157, None)
    >   java.io.IOException: Failed to send RPC RPC 6324932883571440703 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   	at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:362)
    >   	at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:339)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)
    >   	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)
    >   	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)
    >   	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >   	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
    >   	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.nio.channels.ClosedChannelException
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >   	... 12 more
    >    INFO [2021-04-27 10:54:37,438] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Removed broadcast_43_piece0 on worker05:36419 in memory (size: 7.7 KB, free: 6.7 GB)
    >    INFO [2021-04-27 10:54:37,438] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Removed broadcast_43_piece0 on zeppelin:40829 in memory (size: 7.7 KB, free: 6.8 GB)
    >    INFO [2021-04-27 10:54:37,438] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Removed broadcast_43_piece0 on worker04:43887 in memory (size: 7.7 KB, free: 6.7 GB)
    >    INFO [2021-04-27 10:54:37,438] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Removed broadcast_43_piece0 on worker03:43195 in memory (size: 7.7 KB, free: 6.7 GB)
    >    INFO [2021-04-27 10:54:37,438] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Removed broadcast_43_piece0 on worker11:34691 in memory (size: 7.7 KB, free: 6.6 GB)
    >    INFO [2021-04-27 10:54:37,440] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_43_piece0 on worker01:36523 in memory (size: 7.7 KB, free: 6.8 GB)
    >    INFO [2021-04-27 10:54:37,443] ({dispatcher-event-loop-11} Logging.scala[logInfo]:54) - Removed broadcast_43_piece0 on worker02:33701 in memory (size: 7.7 KB, free: 6.7 GB)
    >    WARN [2021-04-27 10:54:37,443] ({block-manager-ask-thread-pool-705} Logging.scala[logWarning]:87) - Failed to remove shuffle 10 - Failed to send RPC RPC 5918819581121256883 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   java.io.IOException: Failed to send RPC RPC 5918819581121256883 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   	at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:362)
    >   	at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:339)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)
    >   	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)
    >   	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)
    >   	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >   	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
    >   	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.nio.channels.ClosedChannelException
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >   	... 12 more
    >    INFO [2021-04-27 10:54:37,447] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Removed broadcast_43_piece0 on worker06:45029 in memory (size: 7.7 KB, free: 6.7 GB)
    >    INFO [2021-04-27 10:54:37,450] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Removed broadcast_43_piece0 on worker08:38635 in memory (size: 7.7 KB, free: 6.7 GB)
    >    INFO [2021-04-27 10:54:37,486] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_43_piece0 on worker07:43931 in memory (size: 7.7 KB, free: 6.7 GB)
    >    INFO [2021-04-27 10:54:37,603] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Removed broadcast_43_piece0 on worker10:37993 in memory (size: 7.7 KB, free: 6.7 GB)
    >    INFO [2021-04-27 10:54:37,627] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1574
    >    INFO [2021-04-27 10:54:37,627] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1285
    >    INFO [2021-04-27 10:54:37,628] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1273
    >    INFO [2021-04-27 10:54:37,628] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 983
    >   ERROR [2021-04-27 10:54:37,628] ({rpc-server-4-8} TransportClient.java[operationComplete]:336) - Failed to send RPC RPC 5914482168896428041 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   java.nio.channels.ClosedChannelException
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >   	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
    >   	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >    INFO [2021-04-27 10:54:37,628] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned shuffle 13
    >   ERROR [2021-04-27 10:54:37,653] ({rpc-server-4-8} TransportClient.java[operationComplete]:336) - Failed to send RPC RPC 4982844082337659715 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   java.nio.channels.ClosedChannelException
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >   	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
    >   	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >    INFO [2021-04-27 10:54:37,657] ({block-manager-slave-async-thread-pool-225} Logging.scala[logInfo]:54) - Removing RDD 166
    >    WARN [2021-04-27 10:54:37,658] ({block-manager-ask-thread-pool-756} Logging.scala[logWarning]:87) - Error trying to remove RDD 166 from block manager BlockManagerId(2, worker16, 45157, None)
    >   java.io.IOException: Failed to send RPC RPC 4982844082337659715 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   	at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:362)
    >   	at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:339)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)
    >   	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)
    >   	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)
    >   	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >   	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
    >   	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.nio.channels.ClosedChannelException
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >   	... 12 more
    >    INFO [2021-04-27 10:54:37,721] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned RDD 166
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1297
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1576
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 980
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1308
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 978
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1545
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1573
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1262
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1264
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1265
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1309
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1577
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1575
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1556
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 1289
    >    INFO [2021-04-27 10:54:37,722] ({Spark Context Cleaner} Logging.scala[logInfo]:54) - Cleaned accumulator 985
    >    WARN [2021-04-27 10:54:37,867] ({block-manager-ask-thread-pool-699} Logging.scala[logWarning]:87) - Failed to remove shuffle 13 - Failed to send RPC RPC 5914482168896428041 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   java.io.IOException: Failed to send RPC RPC 5914482168896428041 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   	at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:362)
    >   	at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:339)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)
    >   	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)
    >   	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)
    >   	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >   	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
    >   	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.nio.channels.ClosedChannelException
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >   	... 12 more
    >    WARN [2021-04-27 10:54:37,880] ({block-manager-ask-thread-pool-703} Logging.scala[logWarning]:87) - Failed to remove shuffle 15 - Failed to send RPC RPC 7876516133116856383 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   java.io.IOException: Failed to send RPC RPC 7876516133116856383 to /10.10.2.207:60612: java.nio.channels.ClosedChannelException
    >   	at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:362)
    >   	at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:339)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)
    >   	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)
    >   	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)
    >   	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)
    >   	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
    >   	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
    >   	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
    >   	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
    >   	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.nio.channels.ClosedChannelException
    >   	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
    >   	... 12 more

# -----------------------------------------------------
# -----------------------------------------------------

    #
    # Checking stderr for the application container on a worker shows connection errors between the workers ...
    #

# -----------------------------------------------------
# Tail the zeppelin server log.
#[user@]

    pushd /var/hadoop/logs

        less userlogs/application_1619486685111_0001/container_1619486685111_0001_01_000051/stderr

    >   ....
    >   ....
    >   ....
    >   ....
    >   2021-04-27 10:45:27,899 ERROR storage.ShuffleBlockFetcherIterator: Failed to get block(s) from worker12:44355
    >   java.io.IOException: Failed to connect to worker12/10.10.0.130:44355
    >           at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
    >           at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
    >           at org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:114)
    >           at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
    >           at org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)
    >           at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    >           at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    >           at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >           at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >           at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >           at java.lang.Thread.run(Thread.java:748)
    >   Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: worker12/10.10.0.130:44355
    >   Caused by: java.net.ConnectException: Connection refused
    >           at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    >           at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
    >           at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)
    >           at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
    >           at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)
    >           at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
    >           at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
    >           at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
    >           at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >           at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >           at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >           at java.lang.Thread.run(Thread.java:748)
    >   2021-04-27 10:45:27,899 ERROR storage.ShuffleBlockFetcherIterator: Failed to get block(s) from worker12:44355
    >   java.io.IOException: Failed to connect to worker12/10.10.0.130:44355
    >           at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
    >           at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
    >           at org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:114)
    >           at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
    >           at org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)
    >           at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    >           at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    >           at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >           at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >           at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >           at java.lang.Thread.run(Thread.java:748)
    >   Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: worker12/10.10.0.130:44355
    >   Caused by: java.net.ConnectException: Connection refused
    >           at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    >           at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
    >           at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)
    >           at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
    >           at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)
    >           at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
    >           at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
    >           at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
    >           at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >           at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >           at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >           at java.lang.Thread.run(Thread.java:748)
    >   2021-04-27 10:54:37,663 INFO storage.BlockManager: Removing RDD 166
    >   (END)

