#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Email from Nigel:

        Subject: Gaia IRIS platform live system zombified?

        Yesterday following our discussions I killed my bulk data shuffling job on the live system
        and then relaunched with 1% of the volume (just 37 csv files, ~10 GB).
        The process seems to be navel-gazing: 14 hours and counting, no data written. I think the
        Spark back-end on the live system must be utterly zombified, so I think we’d better do a
        reset at some level. Whether that means simply restarting Spark or rebuilding the entire
        live system I don’t know - either way let us know (e.g. if Dennis should save any notebooks
        before the platform gets whisked away).

    Target:

        Restart the Zeppelin and Spark processes on the live deployment.

    Result:

        Zeppelin and Yarn restarted ...


# -----------------------------------------------------
# Restart Zeppelin and tail the logs ..
#[user@desktop]

    ssh fedora@zeppelin.aglais.uk

        pushd zeppelin-0.8.2-bin-all/

            ./bin/zeppelin-daemon.sh restart

    >   Zeppelin stop           [  OK  ]
    >   Zeppelin start          [  OK  ]


            tail -f ./logs/zeppelin-fedora-gaia-prod-20210223-zeppelin.novalocal.log

    >   ....
    >   ....
    >    INFO [2021-03-03 10:19:34,076] ({main} ContextHandler.java[doStart]:855) - Started o.e.j.w.WebAppContext@13acb0d1{zeppelin-web,/,file:///home/fedora/zeppelin-0.8.2-bin-all/webapps/webapp/,AVAILABLE}{/home/fedora/zeppelin-0.8.2-bin-all/zeppelin-web-0.8.2.war}
    >    INFO [2021-03-03 10:19:34,088] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@fb2e3fd{HTTP/1.1,[http/1.1]}{10.10.0.248:8080}
    >    INFO [2021-03-03 10:19:34,089] ({main} Server.java[doStart]:407) - Started @5407ms
    >    INFO [2021-03-03 10:19:34,089] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started


# -----------------------------------------------------
# Restart Yarn and tail the logs ..
#[user@desktop]

    ssh fedora@zeppelin.aglais.uk

        ssh master01

    stop-yarn.sh

    >   Stopping nodemanagers
    >   worker04: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker01: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker03: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker02: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   Stopping resourcemanager

    start-yarn.sh

    >   Starting resourcemanager
    >   Starting nodemanagers

            tail -f /var/hadoop/logs/hadoop-fedora-resourcemanager-gaia-prod-20210223-master01.novalocal.log

    >   2021-03-03 10:08:19,259 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8032
    >   2021-03-03 10:08:19,261 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
    >   2021-03-03 10:08:19,262 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
    >   2021-03-03 10:08:19,262 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8032: starting
    >   2021-03-03 10:08:19,268 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to active state
    >   2021-03-03 10:08:21,449 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node worker04(cmPort: 42855 httpPort: 8042) registered with capability: <memory:43008, vCores:13>, assigned nodeId worker04:42855
    >   2021-03-03 10:08:21,456 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker04:42855 Node Transitioned from NEW to RUNNING
    >   2021-03-03 10:08:21,464 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker04:42855 clusterResource: <memory:43008, vCores:13>
    >   2021-03-03 10:08:21,606 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node worker01(cmPort: 38761 httpPort: 8042) registered with capability: <memory:43008, vCores:13>, assigned nodeId worker01:38761
    >   2021-03-03 10:08:21,606 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker01:38761 Node Transitioned from NEW to RUNNING
    >   2021-03-03 10:08:21,607 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker01:38761 clusterResource: <memory:86016, vCores:26>
    >   2021-03-03 10:08:21,705 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node worker03(cmPort: 43235 httpPort: 8042) registered with capability: <memory:43008, vCores:13>, assigned nodeId worker03:43235
    >   2021-03-03 10:08:21,705 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker03:43235 Node Transitioned from NEW to RUNNING
    >   2021-03-03 10:08:21,707 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker03:43235 clusterResource: <memory:129024, vCores:39>
    >   2021-03-03 10:08:21,890 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node worker02(cmPort: 38587 httpPort: 8042) registered with capability: <memory:43008, vCores:13>, assigned nodeId worker02:38587
    >   2021-03-03 10:08:21,890 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker02:38587 Node Transitioned from NEW to RUNNING
    >   2021-03-03 10:08:21,891 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker02:38587 clusterResource: <memory:172032, vCores:52>
    >   ....


    >   ....
    >   2021-03-03 10:47:28,087 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Reserved container=container_1614766099129_0001_01_000005, on node=host: worker01:38761 #containers=1 available=<memory:14336, vCores:12> used=<memory:28672, vCores:1> with resource=<memory:28672, vCores:1>
    >   2021-03-03 10:47:28,088 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
    >   2021-03-03 10:47:28,335 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: Reserved container=container_1614766099129_0001_01_000006, on node=host: worker04:42855 #containers=1 available=<memory:14336, vCores:12> used=<memory:28672, vCores:1> with resource=<memory:28672, vCores:1>
    >   ....


# -----------------------------------------------------
# Tail the logs on a worker
#[user@desktop]

    ssh fedora@zeppelin.aglais.uk

        ssh worker01

            tail -f /var/hadoop/logs/hadoop-fedora-nodemanager-gaia-prod-20210223-worker01.novalocal.log

    >   2021-03-03 10:08:21,514 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app node started at 8042
    >   2021-03-03 10:08:21,515 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : worker01:38761
    >   2021-03-03 10:08:21,517 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
    >   2021-03-03 10:08:21,522 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at master01/10.10.0.216:8031
    >   2021-03-03 10:08:21,559 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
    >   2021-03-03 10:08:21,567 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
    >   2021-03-03 10:08:21,615 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -996074147
    >   2021-03-03 10:08:21,615 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 909178784
    >   2021-03-03 10:08:21,615 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as worker01:38761 with total resource of <memory:43008, vCores:13>
    >   2021-03-03 10:18:20,523 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 0, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   ....
    >   ....


# -----------------------------------------------------
# Watch cpu use on the workers.
#[user@desktop]

    ssh fedora@zeppelin.aglais.uk
        ssh worker01
            htop

    ssh fedora@zeppelin.aglais.uk
        ssh worker02
            htop

    ssh fedora@zeppelin.aglais.uk
        ssh worker03
            htop

    ssh fedora@zeppelin.aglais.uk
        ssh worker04
            htop

