#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: []
#

    Target:

        Trying to figure out our RSAP allocation.
        What we have now, what RSAP have granted for 23/24,
        and what we need to as for in 24/25.

        Cambridge are only just starting to look at the resource request from last year.
        Request submitted in December 2022, installed in December 2023, available in January 2024.

        Which implies
        Request submitted in December 2023, installed in December 2024, available in January 2025.
        Request submitted in December 2024, installed in December 2025, available in January 2026.

        Our timeline is to go live December 2025.
        So request in December 2024 won't be ready in time.
        We will need to add final resources in this request, December 2023. to be ready for December 2025.
        We need to be practicing with mock data during 2024, but we will have to to that with the resources from last year's request.


    Result:

        Work in progress ...



# -----------------------------------------------------
Date 09/10/2023
To iris-twg@jiscmail.ac.uk
From IRIS Technical Working Group <IRIS-TWG@JISCMAIL.AC.UK> on behalf of Daniela Bauer <DANIELA.BAUER@IMPERIAL.AC.UK>

The call for resource requests to IRIS is open from today.
The deadline for the initial submission is November 6th.

The required information for a resource request is roughly in line with previous years,
but please check the guidance for updates and adapt your submission accordingly.

You can find an outline of the process, guidelines and all required documents on the IRIS RSAP webpage:
https://www.iris.ac.uk/rsap/



# -----------------------------------------------------
# -----------------------------------------------------
Date 03/10/2023
From Dave
To Paul

Hi Paul,

We are starting work on this year's RSAP request and would like to have a stock take of where we are.

Can you give us a break down of the resources the Gaia data mining project currently has allocated.

Could you also let us know what the results of last year's RSAP request were from your perspective ?
What did they actually end up asking you to install for us?

Cheers,
-- Dave

# -----------------------------------------------------
Date 03/10/2023
From Paul
To Dave

Hi Dave,

Currently your Gaia projects have 1120 vCPU worth of physical node resource allocated to them.

Last year's RSAP requests and allocation results have still not been shared with us, as Iris
has moved to a November-November allocation year.

We're attempting to figure this out with Iris at the moment.

Kind regards,
Paul Browne


# -----------------------------------------------------
# -----------------------------------------------------

From Dave
To Nigel % Bob

In order to design a system capable of DR4 I think we need to start looking at obtaining,
or creating, some DR4 test data as soon as we can. It was clear from our discussion with
Mark that managing and curating the DR4 data will be a significant challenge in its own
right that we will need to learn how to do.
  
We also need more contact with science users who are pushing the boundaries of what the
platform can do. The contribution Dennis made was invaluable in shaping the current system
for machine learning.

I'm concerned that we are unlikely to get another PhD student using the system in 2024.
I think it would be really useful to have someone pushing the limits of what the system can do,
developing new approaches to machine learning algorithms on the DR3 data, and testing out
their algorithms on the mock DR4 data.

Without this, we will just be building the same system but bigger, and we might not be ready
to support the new analysis methods that will emerge as researchers develop new algorithms
and tools to work with the larger datasets.
 
Cheers,
-- Dave 

# -----------------------------------------------------

From Nigel
To Dave, Bob

> In order to design a system capable of DR4 I think we need to start looking at obtaining,
> or creating, some DR4 test data as soon as we can. It was clear from our discussion with
> Mark that managing and curating the DR4 data will be a significant challenge in its own
> right that we will need to learn how to do.

I’m responsible for the data model development for Gaia DR4 and already we have the
(proto-)type definitions in place against which to generate arbitrary mock datasets,
at scale if we want. It's a question of writing a few mocking apps (probably java)
and running them (subject to disk space limits of course).

> We also need more contact with science users who are pushing the boundaries of what the
> platform can do. The contribution Dennis made was invaluable in shaping the current system
> for machine learning.

> I'm concerned that we are unlikely to get another PhD student using the system in 2024.
> I think it would be really useful to have someone pushing the limits of what the system can do,
> developing new approaches to machine learning algorithms on the DR3 data, and testing out
> their algorithms on the mock DR4 data.

SPACIOUS will help here: there are big science challenges for both Gaia and Euclid in there,
and they will be commencing study early next year. So we can use those to bolster our larger
scale usage scenarios.


# -----------------------------------------------------
# -----------------------------------------------------
Date 17/05/2023
From Daniela Bauer <daniela.bauer@imperial.ac.uk>
To Nic Walton, Nigel Hambly, Dave Morris Jonathan Hays, Andrew Sansum, RSAP Team <rsap@iris.ac.uk>

Dear GAIA Team,

Please find attached your IRIS RSAP Allocation letter regarding GAIA
for the 2023/2024 allocation cycle. If this is not what you expect, please
contact me ASAP.

The allocation cycle will run from September 2023 to September 2024.
However if resources become available earlier, they may be made
available to you sooner.

# -----------------------------------------------------
Date 27/06/2023
From Nigel
To Jonathan Hays, Deniza Chekrygina, Daniela Bauer, Andrew Sansum, Dave Morris, Nigel Hambly

Dear Jonathan and Deniza,

Further to a short discussion at today’s IRIS-TWG, and in relation to the RSAP allocation as
communicated below by Daniela on behalf of IRIS-RSAP, we would like to enquire about the smallest
(but most expensive!) of our 4 “storage areas” allocated for the Gaia data mining platform aspect.

This 32TB of directly attached SSD is the only new part in this allocation with respect to previous
rounds. We’d like to understand how this will be actioned in practice with our provider at Cambridge.
As agreed in the TWG we’re contacting you to get the ball rolling on this new and non-obvious
(to coin a phrase from below) part of our storage allocation.
How should we move forward with this?

    #
    # No follow up as far as I can see.
    #

# -----------------------------------------------------
# -----------------------------------------------------
Date 23/08/22
From Paul Browne (Jira) <support@hpc.cam.ac.uk>
To Dave Morris

Paul Browne commented:

Hello Dave,

At no point have we been informed by IRIS about any of these kind of requirements or requests;
it's a distinct flaw in their RSAP process that this information may be being collected but is
not being disseminated to us as a site at all. So no, this part of the request did not make it to us.
I'm informed that DiRAC does things quite differently and there is a detailed technical assessment
process that involves sites as part of the process.

The himem cores you have available are taken from the pool of CSD3 Intel Cascade Lake nodes,
with 56 physical cores (112 threads) to 384GiB RAM, for mem/core ratio of ~6.8GiB/physical core.
This is exactly double what you would have had on your previously available nodes.
Accordingly, your custom flavors for their himem versions have had available RAM doubled.

View request: https://jira.hpc.cam.ac.uk/servicedesk/customer/portal/2/HPCSSUP-47743?sda_source=notification-email

# -----------------------------------------------------

Date 13/10/2023
From Dave
To Paul

Hi Paul,

I'm looking back at emails from a year ago, and this looks like it is in response to a question about the RSAP process that I posted on your Jira system.

However, the Jira link no longer works.
https://jira.hpc.cam.ac.uk/servicedesk/customer/portal/2/HPCSSUP-47743?sda_source=notification-email

Any chance you could look in the new system to find out what the question was that this is the answer to.

Cheers,
-- Dave

# -----------------------------------------------------

Jira access enabled:
https://ucam-rcs.atlassian.net/servicedesk/customer/portal/4/HPCSSUP-47743

# -----------------------------------------------------
# -----------------------------------------------------
Date  21/08/2022
From Paul Browne
To Dave Morris, S.J. Rankin

Hello Dave,

These allocations are active now, incorporating Intel Cascade Lake nodes with the higher mem/core ratio compared to the previous CCLake hypervisors you would have used. 

Accordingly, new flavors to target this type of node have been added to your projects.

Any issues, just forward a new support request to support@hpc.cam.ac.uk and we'll pick it up from there.

Kind regards,
Paul Browne

# -----------------------------------------------------
Date  22/08/2022
From Dave Morris
To Paul Browne

Hi Paul,

> These allocations are active now, incorporating Intel Cascade Lake nodes with the higher
> mem/core ratio compared to the previous CCLake hypervisors you would have used.

Could you give us some more detail on what these actually are ? How many physical machines
and what resources they have ?

It has been a while since I looked at the resources and I may have out of date information.
Is it possible to get a list of the current resources, old and new, allocated to our project ?

Our 2022/23 resource request specified direct attached SSDs for part of our storage allocation,
did that part of the request make it to the final specification ?

# -----------------------------------------------------
Date 23/08/22
Form Paul Browne
To Dave Morris

Hello Dave,

At no point have we been informed by IRIS about any of these kind of requirements or requests;
it's a distinct flaw in their RSAP process that this information may be being collected
but is not being disseminated to us as a site at all. So no, this part of the request
did not make it to us.

I'm informed that DiRAC does things quite differently and there is a detailed technical
assessment process that involves sites as part of the process.

The himem cores you have available are taken from the pool of CSD3 Intel Cascade Lake nodes,
with 56 physical cores (112 threads) to 384GiB RAM, for mem/core ratio of ~6.8GiB/physical core.
This is exactly double what you would have had on your previously available nodes.
Accordingly, your custom flavors for their himem versions have had available RAM doubled.

# -----------------------------------------------------
# -----------------------------------------------------



# -----------------------------------------------------
# -----------------------------------------------------
Date 19/10/21
From Paul Browne
To Dave Morris
P.S.

Unfortunately I have gotten pulled onto other things interrupting planning your projects'
migrations onto the newer cloud, apologies for that. I'll try to circle back to this for you soon.

- Paul

# -----------------------------------------------------
Date 19/10/2021
From Paul Browne
To Dave Morris

Hi Dave,

Currently you have access to an aggregate of 8 of our Cascade Lake hosts;

(oscli) [pfb29@cumulus-seed ansible]$ openstack aggregate show gaia-cclake-agg

    +-------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | Field             | Value                                                                                                                                                            |
    +-------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | availability_zone | None                                                                                                                                                             |
    | created_at        | 2020-09-29T22:11:14.000000                                                                                                                                       |
    | deleted           | False                                                                                                                                                            |
    | deleted_at        | None                                                                                                                                                             |
    | hosts             | cpu-p-629, cpu-p-630, cpu-p-631, cpu-p-632, cpu-p-633, cpu-p-634, cpu-p-635, cpu-p-636                                                                           |
    | id                | 17                                                                                                                                                               |
    | name              | gaia-cclake-agg                                                                                                                                                  |
    | properties        | filter_tenant_id1='08e24c6d87f94740aa59c172462ed927', filter_tenant_id2='21b4ae3a2ea44bc5a9c14005ed2963af', filter_tenant_id3='bea28e83e6aa47a8962b59c3b24495fe' |
    | updated_at        | None                                                                                                                                                             |
    +-------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+

Thanks,
Paul B. 

# -----------------------------------------------------

Date 19/10/2021
From Dave Morris
To Paul Browne

Hi Paul,

I'm writing up our IRIS resource request for 2022+.

As a baseline for next years request, could you confirm exactly what
resources we currently have.

At the start of this year I know our tasks were pinned to four Cascade
Lake hosts, but since then we have been through a couple of rounds of
new deployments.

Cheers,
-- Dave

# -----------------------------------------------------
# -----------------------------------------------------

Date 14/06/2021
From Paul Browne <pfb29@cam.ac.uk>
To Dave Morris
CC Paul Browne, John Garbutt, Nigel C. Hambly, Stelios Voutsinas

I performed some scaling tests over the weekend in the iris-gaia-test project and
recorded results against those predicted in the attached spreadsheet.
Anything still running in iris-gaia-test under my pfb29 ID can safely be deleted.

One of your assigned Cascade Lake nodes was always failing builds, but once it was
recovered I saw no deviation from predicted maximum carrying capacities for all 4
nodes for a given flavor. We may need to take this node out for hardware maintenance.

Hopefully the spreadsheet calculations are clear, but if not feel free to ask any questions.

The main issues with a reliance on an additional ephemeral disk rather than Ceph volume
storage are

1) lack of resilience; these nodes don't have a redundant disk array for ephemeral disk
since they're HPC compute nodes cycled to hypervisor duty.
For data resilience you would need to build a distributed storage layer on top (e.g. Ceph, etc)

and 2) Since ephemeral disks are local to hypervisor node, the carrying capacity is still
substantially reduced for larger flavors with larger ephemeral disk, even if the (small)
root disk is put on Ceph volume, so you don't really get the expected benefit of removing
disk bottleneck by going to volume.

We'll be adding 2021-2022 allocations this week.

# -----------------------------------------------------
Date 10/06/2021
From Dave Morris
To Paul Browne

Hi Paul,

OK, all yours.
Everything deleted, our dashboard shows three empty projects.

The source code for our tests is here:
https://github.com/wfau/aglais/blob/68d876560b173358f4234e5e0cda08a0f6123c4f/notes/zrq/20210609-01-resource-tests.txt#L263-L419

The results for the gaia.v1.special.tiny flavor are here:
https://github.com/wfau/aglais/blob/68d876560b173358f4234e5e0cda08a0f6123c4f/notes/zrq/20210609-01-resource-tests.txt#L630-L678

The list of error messages we get are here:
https://github.com/wfau/aglais/blob/68d876560b173358f4234e5e0cda08a0f6123c4f/notes/zrq/20210610-01-resource-tests.txt#L88-L102

Appreciate your comments/feedback on the tests.

Cheers,
-- Dave

# -----------------------------------------------------

Date 10/06/2021
From Dave Morris
To Paul Browne

Hi Paul,

I re-ran the tests with all the "gaia.v1.special.tiny" flavor.

Based on the available resources we would expect to be able to create
292 instances (see below for calculation).
Running a test to create 300 instances, we get 205 with ACTIVE
status.

The rest have a mixture of two error messages:

  [No valid host was found. There are not enough hosts available.]
  [Exceeded maximum number of retries. Exhausted all hosts
available
for retrying build failures for instance
61067ed0-9a77-45e1-8af9-ea4b8695f9d3.]

Thanks,
-- Dave

# -----------------------------------------------------

Date 03/06/21
From Paul Browne <pfb29@cam.ac.uk>
To Dave Morris
Hello Dave,

Please run your tests again using flavor "gaia.v1.special.tiny" and see what your new maximum is.

To re-iterate what is happening there, again; in spawning very many VMs in this way you will always
eventually hit a host limit of either CPU, disk or RAM in an OpenStack cloud, even absent any quota
as your projects now have.

Whichever one you hit first will stop further instances being scheduled to host, of that specific flavor.
Most likely in your tests you are hitting local disk limit first, despite the hosts you have available
having greater than the number of CPU schedulable; the nodes have relatively small local storage.

To get around hitting limits of CPU or RAM, go smaller in flavor (such as with gaia.v1.special.tiny)

To get around limits of local disk, you could boot instances to Cinder volumes so that there is no
local disk consumed on the host. For a host OS image not expected to do much IOPs, this may well be
fine for your instances and workload.

Thanks,
Paul Browne


