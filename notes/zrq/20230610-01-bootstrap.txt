#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#
# AIMetrics: [{"name": "ChatGPT","contribution": {"value": 50,"units": "%"}}]
#


    Target:

        Customising the StackHPC capi-helm-charts.
        https://github.com/stackhpc/capi-helm-charts

        Going the long way round to lean how it works.

        Using ChatGPT as an aid to debugging.

    Result:

        Work in progress ...
        Generally unsatisfactory.
        ChatGPT produced a lot of helpful sounding words and some example commands.
        ... but we didn't end up solving the issue.

        ChatGPT can suggest the most common commands in its dataset,
        but despite what it says it doesn't know _why_ these might be useful.

        What are the common commands people have used when debugging an error like <this>.
        What are the common options people have used for <this> command.



# -----------------------------------------------------
# Try connecting to our cluster ...
#[root@bootstrap]

    kubectl config view

    >   apiVersion: v1
    >   clusters:
    >   - cluster:
    >       certificate-authority-data: DATA+OMITTED
    >       server: https://127.0.0.1:43949
    >     name: kind-kind
    >   contexts:
    >   - context:
    >       cluster: kind-kind
    >       user: kind-kind
    >     name: kind-kind
    >   current-context: kind-kind
    >   kind: Config
    >   preferences: {}
    >   users:
    >   - name: kind-kind
    >     user:
    >       client-certificate-data: DATA+OMITTED
    >       client-key-data: DATA+OMITTED


    kubectl config set-context aglais-alpha-cxt
    kubectl config use-context aglais-alpha-cxt

    kubectl config set-cluster aglais-one --server=https://128.232.227.90:6443/
    kubectl config set-context aglais-alpha-cxt --cluster aglais-one

    kubectl get pods

    >   Please enter Username: frog
    >   Please enter Password: toad
    >   E0610 15:42:35.758626   66640 memcache.go:265] couldn't get current server API group list: Get "https://128.232.227.90:6443/api?timeout=32s": tls: failed to verify certificate: x509: certificate signed by unknown authority
    >   E0610 15:42:35.776270   66640 memcache.go:265] couldn't get current server API group list: Get "https://128.232.227.90:6443/api?timeout=32s": tls: failed to verify certificate: x509: certificate signed by unknown authority
    >   E0610 15:42:35.780422   66640 memcache.go:265] couldn't get current server API group list: Get "https://128.232.227.90:6443/api?timeout=32s": tls: failed to verify certificate: x509: certificate signed by unknown authority
    >   E0610 15:42:35.784143   66640 memcache.go:265] couldn't get current server API group list: Get "https://128.232.227.90:6443/api?timeout=32s": tls: failed to verify certificate: x509: certificate signed by unknown authority
    >   E0610 15:42:35.788118   66640 memcache.go:265] couldn't get current server API group list: Get "https://128.232.227.90:6443/api?timeout=32s": tls: failed to verify certificate: x509: certificate signed by unknown authority
    >   Unable to connect to the server: tls: failed to verify certificate: x509: certificate signed by unknown authority
    >   ....
    >   ....


    #
    # To access the cluster, use clusterctl to generate a kubeconfig file.
    # https://github.com/stackhpc/capi-helm-charts/tree/main/charts/openstack-cluster#accessing-a-workload-cluster

    clusterctl get kubeconfig aglais-one

    >   Error: failed to connect to the management cluster: action failed after 9 attempts: Get "https://128.232.227.90:6443/api?timeout=30s": x509: certificate signed by unknown authority


# -----------------------------------------------------
# -----------------------------------------------------
# ChatGPT

    >   can you explain this kubectl error message
    >   Unable to connect to the server: tls: failed to verify certificate: x509: certificate signed by unknown authority

    >   The error message "Unable to connect to the server: tls: failed to verify certificate: x509: certificate signed
    >   by unknown authority" is a common error encountered when using the `kubectl` command-line tool to interact with
    >   a Kubernetes cluster.
    >
    >   This error typically occurs when the client (your local machine) is unable to verify the authenticity of the
    >   server's TLS certificate. Kubernetes uses TLS (Transport Layer Security) to secure the communication between
    >   the client and the cluster.
    >
    >   When you establish a connection to the Kubernetes API server using `kubectl`, the API server presents its TLS
    >   certificate to the client as a form of identification. The client then checks whether the certificate is signed
    >   by a trusted certificate authority (CA). If the certificate is not signed by a trusted CA, the client raises the
    >   "certificate signed by unknown authority" error.
    >
    >   This error can be caused by several reasons:
    >
    >   1. **Self-signed certificate:** If the Kubernetes cluster is using a self-signed certificate, which is not signed
    >   by a trusted CA, your local machine won't recognize it as valid. By default, `kubectl` expects certificates signed
    >   by a trusted CA.
    >
    >   2. **Incorrect cluster configuration:** If you're using a `kubeconfig` file to configure `kubectl`, there might be
    >   an error in the configuration file. Check if the server's certificate authority data or certificate file is correctly
    >   specified.
    >
    >   3. **Missing or outdated CA certificates:** It's possible that your local machine is missing the CA certificates
    >   required to verify the Kubernetes server's certificate. Updating the CA certificates on your machine may resolve
    >   the issue.
    >
    >   To address the error, you can try the following solutions:
    >
    >   1. **Using `--insecure-skip-tls-verify` (not recommended):** As a temporary workaround, you can use the
    >   `--insecure-skip-tls-verify` flag with the `kubectl` command. This flag disables the TLS certificate verification,
    >   allowing the connection to proceed. However, this is not recommended for production environments as it exposes you
    >   to security risks.
    >
    >   2. **Using a trusted certificate:** If you have a trusted certificate for the Kubernetes cluster, you can update the
    >   `kubeconfig` file or the server's configuration to use that certificate. This ensures that the certificate is signed
    >   by a known CA.
    >
    >   3. **Updating CA certificates:** Update the CA certificates on your local machine. The process for updating CA certificates
    >   depends on the operating system you're using. For example, on Linux, you can update the certificates using the package
    >   manager or by manually adding the CA certificate to the trusted store.
    >
    >   4. **Verify cluster configuration:** Double-check the cluster configuration, especially the `kubeconfig` file, to ensure
    >   that the server's certificate and certificate authority data are correctly specified.
    >
    >   By addressing one of these potential causes, you should be able to resolve the "certificate signed by unknown authority"
    >   error and establish a connection to the Kubernetes cluster using `kubectl`.

# -----------------------------------------------------

    #
    # Try acessing that endpoint.

    curl "https://128.232.227.90:6443/api"

    >   curl: (60) SSL certificate problem: unable to get local issuer certificate
    >   More details here: https://curl.se/docs/sslcerts.html
    >
    >   curl failed to verify the legitimacy of the server and therefore could not
    >   establish a secure connection to it. To learn more about this situation and
    >   how to fix it, please visit the web page mentioned above.


    curl --insecure "https://128.232.227.90:6443/api"

    >   {
    >     "kind": "Status",
    >     "apiVersion": "v1",
    >     "metadata": {},
    >     "status": "Failure",
    >     "message": "forbidden: User \"system:anonymous\" cannot get path \"/api\"",
    >     "reason": "Forbidden",
    >     "details": {},
    >     "code": 403
    >   }

# -----------------------------------------------------
# -----------------------------------------------------
# ChatGPT

    >   clusterapi get x509 certificate authority

    >   To obtain the X.509 certificate authority (CA) for a Cluster API (CAPI) cluster,
    >   you need to consider the specific implementation of Cluster API you are using,
    >   as there are multiple providers available.
    >   Cluster API is an open-source project that enables declarative cluster lifecycle
    >   management for Kubernetes.
    >
    >   Cluster API Provider for Kubernetes (CAPK):
    >   CAPK uses the native Kubernetes resources and is one of the most popular Cluster API providers.
    >   To get the CA certificate for a CAPK cluster, you can use the kubectl command:
    >
    >       kubectl get secret -n <namespace> <cluster-name>-kubeconfig -o jsonpath='{.data.ca\.crt}' | base64 -d
    >
    >   Replace <namespace> with the namespace of your cluster and <cluster-name> with the name of your cluster.
    >   This command extracts the CA certificate from the kubeconfig secret and decodes it.


# -----------------------------------------------------
# Explore the suggestion from ChatGPT.
#[root@bootstrap]

    kubectl get secrets

    >   NAME                                               TYPE                      DATA   AGE
    >   aglais-one-ca                                      cluster.x-k8s.io/secret   2      24h
    >   aglais-one-ccm-openstack-config                    Opaque                    2      24h
    >   aglais-one-cloud-credentials                       Opaque                    1      24h
    >   aglais-one-cni-calico-config                       Opaque                    2      24h
    >   aglais-one-control-plane-7fdml                     cluster.x-k8s.io/secret   2      24h
    >   aglais-one-csi-cinder-config                       Opaque                    2      24h
    >   aglais-one-etcd                                    cluster.x-k8s.io/secret   2      24h
    >   aglais-one-kubeconfig                              cluster.x-k8s.io/secret   1      24h
    >   aglais-one-md-0-99910806-b9qjk                     cluster.x-k8s.io/secret   2      2m47s
    >   aglais-one-md-0-99910806-n5cp4                     cluster.x-k8s.io/secret   2      2m46s
    >   aglais-one-md-0-99910806-p7k9k                     cluster.x-k8s.io/secret   2      2m45s
    >   aglais-one-mellanox-network-operator-config        Opaque                    2      24h
    >   aglais-one-metrics-server-config                   Opaque                    2      24h
    >   aglais-one-node-feature-discovery-config           Opaque                    2      24h
    >   aglais-one-nvidia-gpu-operator-config              Opaque                    2      24h
    >   aglais-one-proxy                                   cluster.x-k8s.io/secret   2      24h
    >   aglais-one-sa                                      cluster.x-k8s.io/secret   2      24h
    >   sh.helm.release.v1.aglais-one.v1                   helm.sh/release.v1        1      24h
    >   sh.helm.release.v1.cluster-api-addon-provider.v1   helm.sh/release.v1        1      24h


    kubectl get secret \
        aglais-one-ca

    >   NAME            TYPE                      DATA   AGE
    >   aglais-one-ca   cluster.x-k8s.io/secret   2      24h


    kubectl describe secret \
        aglais-one-ca

    >   Name:         aglais-one-ca
    >   Namespace:    default
    >   Labels:       cluster.x-k8s.io/cluster-name=aglais-one
    >   Annotations:  <none>
    >
    >   Type:  cluster.x-k8s.io/secret
    >
    >   Data
    >   ====
    >   tls.crt:  1070 bytes
    >   tls.key:  1679 bytes


    kubectl describe secret \
        aglais-one-kubeconfig

    >   Name:         aglais-one-kubeconfig
    >   Namespace:    default
    >   Labels:       cluster.x-k8s.io/cluster-name=aglais-one
    >   Annotations:  <none>
    >
    >   Type:  cluster.x-k8s.io/secret
    >
    >   Data
    >   ====
    >   value:  5574 bytes


    kubectl get secret \
        aglais-one-kubeconfig \
            -o jsonpath='{.data}'

    >   {"value":"YXBpVmVy .... dExRbz0K"}


    kubectl get secret \
        aglais-one-kubeconfig \
            -o jsonpath='{.data.value}' \
    | base64 -d \
    | tee "${HOME}/.kube/aglais-one-kubeconfig"

    >   apiVersion: v1
    >   clusters:
    >   - cluster:
    >       certificate-authority-data: LS0tLS1C .... tLS0tLQo=
    >       server: https://128.232.227.90:6443
    >     name: aglais-one
    >   contexts:
    >   - context:
    >       cluster: aglais-one
    >       user: aglais-one-admin
    >     name: aglais-one-admin@aglais-one
    >   current-context: aglais-one-admin@aglais-one
    >   kind: Config
    >   preferences: {}
    >   users:
    >   - name: aglais-one-admin
    >     user:
    >       client-certificate-data: LS0tLS1C .... tLS0tLQo=
    >       client-key-data: LS0tLS1C .... tLS0tLQo=


    kubectl \
        --kubeconfig "${HOME}/.kube/aglais-one-kubeconfig" \
        cluster-info

    >   E0610 17:53:25.528821   71644 memcache.go:287] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
    >   E0610 17:53:25.533756   71644 memcache.go:121] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
    >   E0610 17:53:25.540340   71644 memcache.go:121] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
    >   E0610 17:53:25.543411   71644 memcache.go:121] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
    >   Kubernetes control plane is running at https://128.232.227.90:6443
    >   CoreDNS is running at https://128.232.227.90:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy


# -----------------------------------------------------
# -----------------------------------------------------
# ChatGPT

    >   can you explain this kubectl error message
    >   memcache.go:287] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request

    >   The error message you provided, "memcache.go:287] couldn't get resource list for metrics.k8s.io/v1beta1:
    >   the server is currently unable to handle the request," indicates a problem with retrieving the resource
    >   list for metrics from the Kubernetes API server using `kubectl`.
    >
    >   This error typically occurs when the metrics server, which provides resource usage metrics for Kubernetes
    >   objects, is either not installed or not functioning correctly in the cluster.
    >
    >   The metrics server is a component that collects resource usage data (CPU, memory, etc.) from the Kubernetes
    >   nodes and makes it available through the Kubernetes API. It is commonly used by monitoring and autoscaling
    >   systems to gather insights about the cluster's resource utilization.
    >
    >   Here are a few possible reasons for this error:
    >
    >   1. **Metrics server not installed:** The metrics server might not be deployed in your cluster. By default,
    >   some Kubernetes distributions do not include the metrics server, so you need to install it separately.
    >   The absence of the metrics server will result in the error you encountered when attempting to retrieve metrics.
    >
    >   2. **Metrics server not functioning:** Even if the metrics server is installed, it may encounter issues
    >   preventing it from handling the request. This could be due to misconfiguration, resource constraints,
    >   or other factors. In such cases, the server may fail to respond to the resource list request, leading to the
    >   error message.
    >
    >   To resolve this error, you can take the following steps:
    >
    >   1. **Install or upgrade the metrics server:** Ensure that the metrics server is properly installed and
    >   functioning in your cluster. You can refer to the official Kubernetes documentation or consult the
    >   documentation specific to your Kubernetes distribution to learn how to install or upgrade the metrics server.
    >
    >   2. **Check metrics server logs:** Inspect the logs of the metrics server pods to identify any specific
    >   errors or issues it might be encountering. Use the `kubectl logs` command to view the logs of the metrics
    >   server pods.
    >
    >   3. **Verify connectivity to the metrics server:** Ensure that the metrics server is reachable from your
    >   cluster and that there are no network connectivity issues. Verify that the metrics server pods are running
    >   and healthy.
    >
    >   4. **Check resource constraints:** If the metrics server is running but still unable to handle requests,
    >   check if it has sufficient resources (CPU, memory, etc.) allocated to function properly. You may need to
    >   adjust the resource limits or scale up the metrics server deployment if necessary.
    >
    >   By addressing these potential causes, you should be able to resolve the "couldn't get resource list for
    >   metrics.k8s.io/v1beta1" error and retrieve the metrics successfully using `kubectl`.


# -----------------------------------------------------
# Explore the suggestion from ChatGPT.
#[root@bootstrap]

    #
    # Settings for the metrics server.
    # https://github.com/stackhpc/capi-helm-charts/blob/1cd905bde6ecdd5c35fceb23dd13ee9edfac2081/charts/cluster-addons/values.yaml#L124-L135
    #
    # Enabled by default.
    #

    kubectl get cluster-api

    >   NAME                                                                                        CLUSTER      BOOTSTRAP   TARGET NAMESPACE         RELEASE NAME                PHASE        REVISION   AGE
    >   manifests.addons.stackhpc.com/aglais-one-cloud-config                                       aglais-one   true        openstack-system         cloud-config                Deployed     1          25h
    >   manifests.addons.stackhpc.com/aglais-one-csi-cinder-storageclass                            aglais-one   true        openstack-system         csi-cinder-storageclass     Deployed     1          25h
    >
    >   NAME                                                                                        CLUSTER      BOOTSTRAP   TARGET NAMESPACE         RELEASE NAME                PHASE        REVISION   CHART NAME                           CHART VERSION   AGE
    >   helmrelease.addons.stackhpc.com/aglais-one-ccm-openstack                                    aglais-one   true        openstack-system         ccm-openstack               Deployed     1          openstack-cloud-controller-manager   1.3.0           25h
    >   helmrelease.addons.stackhpc.com/aglais-one-cni-calico                                       aglais-one   true        tigera-operator          cni-calico                  Installing              tigera-operator                      v3.23.3         25h
    >   helmrelease.addons.stackhpc.com/aglais-one-csi-cinder                                       aglais-one   true        openstack-system         csi-cinder                  Upgrading               openstack-cinder-csi                 2.2.0           25h
    >   helmrelease.addons.stackhpc.com/aglais-one-mellanox-network-operator                        aglais-one   true        network-operator         mellanox-network-operator   Upgrading               network-operator                     1.3.0           25h
    >   helmrelease.addons.stackhpc.com/aglais-one-metrics-server                                   aglais-one   true        kube-system              metrics-server              Upgrading               metrics-server                       3.8.2           25h
    >   helmrelease.addons.stackhpc.com/aglais-one-node-feature-discovery                           aglais-one   true        node-feature-discovery   node-feature-discovery      Upgrading               node-feature-discovery               0.11.2          25h
    >   helmrelease.addons.stackhpc.com/aglais-one-nvidia-gpu-operator                              aglais-one   true        gpu-operator             nvidia-gpu-operator         Upgrading               gpu-operator                         v1.11.1         25h
    >
    >   NAME                                                                                        AGE
    >   kubeadmconfigtemplate.bootstrap.cluster.x-k8s.io/aglais-one-md-0-99910806                   25h
    >
    >   NAME                                                                                        CLUSTER      AGE
    >   kubeadmconfig.bootstrap.cluster.x-k8s.io/aglais-one-control-plane-7fdml                     aglais-one   25h
    >   kubeadmconfig.bootstrap.cluster.x-k8s.io/aglais-one-md-0-99910806-44czz                     aglais-one   7m7s
    >   kubeadmconfig.bootstrap.cluster.x-k8s.io/aglais-one-md-0-99910806-ck4mz                     aglais-one   7m9s
    >   kubeadmconfig.bootstrap.cluster.x-k8s.io/aglais-one-md-0-99910806-q6c4m                     aglais-one   7m7s
    >
    >   NAME                                                                                        CLUSTER      REPLICAS   READY   UPDATED   UNAVAILABLE   PHASE       AGE   VERSION
    >   machinedeployment.cluster.x-k8s.io/aglais-one-md-0                                          aglais-one   3                  3         3             ScalingUp   25h   v1.25.4
    >
    >   NAME                                                                                        CLUSTER      REPLICAS   READY   AVAILABLE   AGE   VERSION
    >   machineset.cluster.x-k8s.io/aglais-one-md-0-86f4d96889x9qh9z                                aglais-one   3                              25h   v1.25.4
    >
    >   NAME                                                                                        CLUSTER      EXPECTEDMACHINES   MAXUNHEALTHY   CURRENTHEALTHY   AGE
    >   machinehealthcheck.cluster.x-k8s.io/aglais-one-control-plane                                aglais-one   1                  100%                            25h
    >   machinehealthcheck.cluster.x-k8s.io/aglais-one-md-0                                         aglais-one   3                  100%                            25h
    >
    >   NAME                                                                                        PHASE         AGE   VERSION
    >   cluster.cluster.x-k8s.io/aglais-one                                                         Provisioned   25h
    >
    >   NAME                                                                                        CLUSTER      NODENAME   PROVIDERID                                          PHASE         AGE    VERSION
    >   machine.cluster.x-k8s.io/aglais-one-control-plane-tq5gc                                     aglais-one              openstack:///21a28eb9-4526-4e4c-82d0-601f5eadd425   Provisioned   25h    v1.25.4
    >   machine.cluster.x-k8s.io/aglais-one-md-0-86f4d96889x9qh9z-7xxz4                             aglais-one              openstack:///6fb41356-90af-41c1-a6b0-ad3b0927f144   Provisioned   7m7s   v1.25.4
    >   machine.cluster.x-k8s.io/aglais-one-md-0-86f4d96889x9qh9z-lz764                             aglais-one              openstack:///6255320e-4622-4a15-8004-f2bf66dd6964   Provisioned   7m9s   v1.25.4
    >   machine.cluster.x-k8s.io/aglais-one-md-0-86f4d96889x9qh9z-vfr6g                             aglais-one              openstack:///9ad8b2e2-e53e-4cbe-b670-3ea2e2572546   Provisioned   7m7s   v1.25.4
    >
    >   NAME                                                                                        CLUSTER      INITIALIZED   API SERVER AVAILABLE   REPLICAS   READY   UPDATED   UNAVAILABLE   AGE   VERSION
    >   kubeadmcontrolplane.controlplane.cluster.x-k8s.io/aglais-one-control-plane                  aglais-one   true                                 1                  1         1             25h   v1.25.4
    >
    >   NAME                                                                                        CLUSTER      READY   NETWORK                                SUBNET                                 BASTION IP   AGE
    >   openstackcluster.infrastructure.cluster.x-k8s.io/aglais-one                                 aglais-one   true    8576e3f4-8f28-47f3-8cfd-fff6f85653a6   1ff19481-1ddc-41a2-9465-5a3f9c584ff8                25h
    >
    >   NAME                                                                                        AGE
    >   openstackmachinetemplate.infrastructure.cluster.x-k8s.io/aglais-one-control-plane-c787b906  25h
    >   openstackmachinetemplate.infrastructure.cluster.x-k8s.io/aglais-one-md-0-c787b906           25h
    >
    >   NAME                                                                                        CLUSTER      INSTANCESTATE   READY   PROVIDERID                                          MACHINE                                  AGE
    >   openstackmachine.infrastructure.cluster.x-k8s.io/aglais-one-control-plane-c787b906-2b7qv    aglais-one   ACTIVE          true    openstack:///21a28eb9-4526-4e4c-82d0-601f5eadd425   aglais-one-control-plane-tq5gc           25h
    >   openstackmachine.infrastructure.cluster.x-k8s.io/aglais-one-md-0-c787b906-c86dg             aglais-one   ACTIVE          true    openstack:///6255320e-4622-4a15-8004-f2bf66dd6964   aglais-one-md-0-86f4d96889x9qh9z-lz764   7m9s
    >   openstackmachine.infrastructure.cluster.x-k8s.io/aglais-one-md-0-c787b906-hvgnp             aglais-one   ACTIVE          true    openstack:///9ad8b2e2-e53e-4cbe-b670-3ea2e2572546   aglais-one-md-0-86f4d96889x9qh9z-vfr6g   7m7s
    >   openstackmachine.infrastructure.cluster.x-k8s.io/aglais-one-md-0-c787b906-rv2rc             aglais-one   ACTIVE          true    openstack:///6fb41356-90af-41c1-a6b0-ad3b0927f144   aglais-one-md-0-86f4d96889x9qh9z-7xxz4   7m7s


# -----------------------------------------------------
# -----------------------------------------------------
# ChatGPT

    >   can you explain why the tigera-operator in a clusterapi deployemnt is stuck at PHASE Installing

    >   When the Tigera Operator in a Cluster API (CAPI) deployment is stuck at the "Installing" phase,
    >   it typically indicates that there is an issue preventing the operator from progressing to the
    >   next phase. The "Installing" phase refers to the stage where the operator is being deployed and
    >   initialized in the cluster.
    >
    >   Here are some possible reasons why the Tigera Operator might be stuck at the "Installing" phase:
    >
    >   1. **Image pull or registry issues:** The operator image specified in the deployment might not be
    >   available or accessible in the container registry. Verify that the image repository and tag are
    >   correct and that the cluster has proper access to pull the image.
    >
    >   2. **Resource constraints:** The cluster might be experiencing resource constraints, such as
    >   insufficient CPU or memory, that prevent the operator from being deployed successfully. Check
    >   the resource allocation for the nodes and ensure that there are enough resources available for
    >   the operator to run.
    >
    >   3. **Dependency conflicts:** The Tigera Operator might have dependencies or prerequisites that
    >   are not met in the cluster, causing it to fail during installation. Make sure all required
    >   dependencies are satisfied, such as specific Kubernetes versions or installed components.
    >
    >   4. **Networking issues:** Connectivity or network configuration problems can hinder the installation
    >   process. Check if the cluster can communicate with the necessary external resources, such as DNS
    >   servers or container registries, required for the operator deployment.
    >
    >   5. **Incorrect configuration:** Review the configuration parameters provided for the Tigera Operator
    >   deployment. Ensure that all necessary configuration options are correctly specified, such as
    >   authentication credentials or networking settings.
    >
    >   To troubleshoot and resolve the issue, you can follow these steps:
    >
    >   1. **Check logs and events:** Examine the logs and events associated with the Tigera Operator pods
    >   to identify any specific error messages or failures that could explain the stuck installation. Use
    >   the `kubectl logs` command to view the logs of the operator pods, and check the cluster events using
    >   `kubectl get events` to gather more information.
    >
    >   2. **Verify prerequisites:** Ensure that all prerequisites and dependencies required by the Tigera
    >   Operator are met. Check the operator's documentation or deployment guide for any specific requirements
    >   and make sure they are fulfilled in your cluster.
    >
    >   3. **Inspect resource allocation:** Review the resource allocation and availability in your cluster.
    >   Verify that there are sufficient resources for the operator pods to deploy and run without constraints.
    >
    >   4. **Check network connectivity:** Validate the cluster's network connectivity and DNS configuration.
    >   Ensure that the necessary network access is available for the operator to download images, connect to
    >   external resources, and communicate with other components.
    >
    >   5. **Verify configuration:** Double-check the configuration provided for the Tigera Operator deployment.
    >   Ensure that all required parameters are correctly specified, and there are no typos or missing values.
    >
    >   If the above steps do not resolve the issue, it may be necessary to seek additional support from the
    >   Tigera Operator's documentation, community forums, or Tigera's support channels for further assistance
    >   with troubleshooting the specific deployment scenario.


# -----------------------------------------------------
# Explore the suggestion from ChatGPT.
#[root@bootstrap]

     kubectl get events

    >   LAST SEEN   TYPE      REASON                            OBJECT                                             MESSAGE
    >   3m37s       Normal    MachineMarkedUnhealthy            machine/aglais-one-control-plane-tq5gc             Machine default/aglais-one-control-plane/aglais-one-control-plane-tq5gc/ has been marked as unhealthy
    >   4m14s       Warning   ControlPlaneUnhealthy             kubeadmcontrolplane/aglais-one-control-plane       Waiting for control plane to pass preflight checks to continue reconciliation: [machine aglais-one-control-plane-tq5gc does not have APIServerPodHealthy condition, machine aglais-one-control-plane-tq5gc does not have ControllerManagerPodHealthy condition, machine aglais-one-control-plane-tq5gc does not have SchedulerPodHealthy condition, machine aglais-one-control-plane-tq5gc does not have EtcdPodHealthy condition, machine aglais-one-control-plane-tq5gc does not have EtcdMemberHealthy condition]
    >
    >   60m         Normal    DetectedUnhealthy                 machine/aglais-one-md-0-86f4d96889x9qh9z-4l8ns     Machine default/aglais-one-md-0/aglais-one-md-0-86f4d96889x9qh9z-4l8ns/ has unhealthy node
    >   50m         Normal    MachineMarkedUnhealthy            machine/aglais-one-md-0-86f4d96889x9qh9z-4l8ns     Machine default/aglais-one-md-0/aglais-one-md-0-86f4d96889x9qh9z-4l8ns/ has been marked as unhealthy
    >   ....
    >   ....
    >   3m42s       Normal    DetectedUnhealthy                 machine/aglais-one-md-0-86f4d96889x9qh9z-zn9mr     Machine default/aglais-one-md-0/aglais-one-md-0-86f4d96889x9qh9z-zn9mr/ has unhealthy node
    >   50m         Normal    MachineMarkedUnhealthy            machine/aglais-one-md-0-86f4d96889x9qh9z-zvk57     Machine default/aglais-one-md-0/aglais-one-md-0-86f4d96889x9qh9z-zvk57/ has been marked as unhealthy
    >
    >   50m         Normal    SuccessfulCreate                  machineset/aglais-one-md-0-86f4d96889x9qh9z        Created machine "aglais-one-md-0-86f4d96889x9qh9z-rp7bh"
    >   50m         Normal    SuccessfulCreate                  machineset/aglais-one-md-0-86f4d96889x9qh9z        Created machine "aglais-one-md-0-86f4d96889x9qh9z-pvlw4"
    >   ....
    >   ....
    >   9m14s       Normal    SuccessfulCreate                  machineset/aglais-one-md-0-86f4d96889x9qh9z        Created machine "aglais-one-md-0-86f4d96889x9qh9z-vgxpq"
    >   9m13s       Normal    SuccessfulCreate                  machineset/aglais-one-md-0-86f4d96889x9qh9z        Created machine "aglais-one-md-0-86f4d96889x9qh9z-mg797"
    >
    >   29m         Normal    Successfulcreateport              openstackmachine/aglais-one-md-0-c787b906-2fp7k    Created port aglais-one-md-0-c787b906-2fp7k-0 with id 5d036824-a27f-4917-a2b6-d920bb8c1f93
    >   29m         Normal    Successfulcreateserver            openstackmachine/aglais-one-md-0-c787b906-2fp7k    Created server aglais-one-md-0-c787b906-2fp7k with id b2923263-a5d2-46ba-970c-46d23b26ba19
    >   ....
    >   ....
    >   50m         Normal    Successfuldeleteport              openstackmachine/aglais-one-md-0-c787b906-xxt84    Deleted port with id e71b6c5e-a76a-41a4-9eac-68cfacd9a0f8
    >   50m         Normal    Successfuldeleteserver            openstackmachine/aglais-one-md-0-c787b906-xxt84    Deleted server aglais-one-md-0-c787b906-xxt84 with id 94b7b689-2fa3-4512-bd7d-006a8957db37
    >
    >   44m         Error     Logging                           helmrelease/aglais-one-cni-calico                  Handler 'handle_addon_updated' failed with an exception. Will retry....
    >   46m         Error     Logging                           helmrelease/aglais-one-cni-calico                  Handler 'handle_addon_updated' failed with an exception. Will retry....
    >   ....
    >   ....
    >   10m         Error     Logging                           helmrelease/aglais-one-cni-calico                  Handler 'handle_addon_updated' failed with an exception. Will retry....
    >   26m         Error     Logging                           helmrelease/aglais-one-cni-calico                  Handler 'handle_addon_updated' failed with an exception. Will retry....


# -----------------------------------------------------
# -----------------------------------------------------
# ChatGPT

    >   how can I get the details of a kubectl event

    >   To retrieve the details of a specific `kubectl` event, you can use the `kubectl describe` command
    >   followed by the event type and the name of the event object. The `describe` command provides more
    >   detailed information about the specified Kubernetes resource, including events.
    >
    >   Here's the syntax for retrieving the details of a `kubectl` event:
    >
    >   ```
    >   kubectl describe event <event-type> <event-name>
    >   ```
    >
    >   Replace `<event-type>` with the type of event (e.g., Pod, Deployment, Service) and `<event-name>`
    >   with the name of the specific event you want to examine.
    >
    >   For example, to get the details of a Pod event named "my-pod-event", you would run:
    >
    >   ```
    >   kubectl describe event Pod my-pod-event
    >   ```
    >
    >   This command will display the comprehensive information associated with the specified event, including
    >   the event type, involved objects, event message, event reason, and event timestamps. The output will
    >   provide insights into the event's status, such as warnings, errors, or other relevant details.
    >
    >   Please note that you will need appropriate permissions to access events in the Kubernetes cluster.
    >   Additionally, events are only stored for a limited period, so if an event has been purged, it will no
    >   longer be available for retrieval.
    >
    >   By using the `kubectl describe event` command with the correct event type and name, you can obtain the
    >   specific details related to that event and troubleshoot any issues or understand the status of Kubernetes
    >   resources more effectively.


    >   how can I get the details of a kubectl Error Logging event

    >   To retrieve the details of a specific `kubectl` Error Logging event, you can use the `kubectl get events`
    >   command in combination with filters to narrow down the results and find the specific error event you are
    >   interested in.
    >
    >   Here's an example command to fetch the details of `kubectl` Error Logging events:
    >
    >   ```shell
    >   kubectl get events --field-selector type=Warning
    >   ```
    >
    >   This command filters the events based on their type, specifically selecting events with the type "Warning".
    >   Error events are usually categorized as warnings in Kubernetes.
    >
    >   If you want to filter events further and search for a specific resource, such as a Pod or Deployment, you
    >   can use additional field selectors. For example:
    >
    >   ```shell
    >   kubectl get events --field-selector type=Warning,involvedObject.kind=Pod,involvedObject.name=my-pod
    >   ```
    >
    >   Replace `involvedObject.kind` with the appropriate resource type and `involvedObject.name` with the name
    >   of the specific resource you are interested in (e.g., Pod, Deployment).
    >
    >   Once you've identified the error event you want to investigate, you can use the `kubectl describe event`
    >   command, as mentioned in the previous response, to obtain more detailed information about that specific event.
    >
    >   ```shell
    >   kubectl describe event <event-name>
    >   ```
    >
    >   Replace `<event-name>` with the name of the event you want to examine. The `kubectl describe event` command
    >   will provide comprehensive details about the event, including the event type, involved objects, event message,
    >   reason, and timestamps.
    >
    >   By using `kubectl get events` with appropriate field selectors and then `kubectl describe event`, you can
    >   retrieve the details of `kubectl` Error Logging events and gain insights into any errors or warnings that
    >   have occurred in your Kubernetes cluster.


# -----------------------------------------------------
# Explore the suggestion from ChatGPT.
#[root@bootstrap]

    kubectl get events --field-selector type=Warning

    >   LAST SEEN   TYPE      REASON                  OBJECT                                         MESSAGE
    >   5m          Warning   ControlPlaneUnhealthy   kubeadmcontrolplane/aglais-one-control-plane   Waiting for control plane to pass preflight checks to continue reconciliation: [ .... ]

    >   [
    >   machine aglais-one-control-plane-tq5gc does not have APIServerPodHealthy condition,
    >   machine aglais-one-control-plane-tq5gc does not have ControllerManagerPodHealthy condition,
    >   machine aglais-one-control-plane-tq5gc does not have SchedulerPodHealthy condition,
    >   machine aglais-one-control-plane-tq5gc does not have EtcdPodHealthy condition,
    >   machine aglais-one-control-plane-tq5gc does not have EtcdMemberHealthy condition
    >   ]


    kubectl get events --field-selector type=Error

    >   LAST SEEN   TYPE    REASON    OBJECT                                             MESSAGE
    >   38m         Error   Logging   helmrelease/aglais-one-csi-cinder                  Handler 'handle_addon_updated' failed with an exception. Will retry....
    >   2m16s       Error   Logging   helmrelease/aglais-one-cni-calico                  Handler 'handle_addon_updated' failed with an exception. Will retry....
    >   ....
    >   ....
    >   34m         Error   Logging   helmrelease/aglais-one-cni-calico                  Handler 'handle_addon_updated' failed with an exception. Will retry....
    >   58m         Error   Logging   helmrelease/aglais-one-cni-calico                  Handler 'handle_addon_updated' failed with an exception. Will retry....


    kubectl get events \
        --all-namespaces \
        --field-selector \
            type=Error,involvedObject.name=helmrelease/aglais-one-cni-calico

    >   No resources found

    #
    # Primary suspect is Calico, but we can't find further details.
    #




