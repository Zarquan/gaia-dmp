#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2022, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Deployment to test the latest changes.

    Result:

        Work in progress ...


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --publish 3000:3000 \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        ghcr.io/wfau/atolmis/ansible-client:2022.03.19 \
        bash


# -----------------------------------------------------
# Set the target configuration.
#[root@ansibler]

    cloudbase='arcus'
    cloudname='iris-gaia-blue'
    configname=zeppelin-54.86-spark-6.26.43


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

    >   real    4m7.953s
    >   user    1m43.856s
    >   sys     0m11.387s

# -----------------------------------------------------
# Create everything.
# (*) apart from the user database.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}" \
            "${configname:?}" \
        | tee /tmp/create-all.log

    >   real    45m16.801s
    >   user    13m41.399s
    >   sys     3m36.819s


# -----------------------------------------------------
# Create our shiro-auth database.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-auth-database.sh \
            "${cloudname:?}" \
            "${configname:?}" \
        | tee /tmp/create-auth-database.log

    >   real    1m13.265s
    >   user    0m20.799s
    >   sys     0m4.145s


# -----------------------------------------------------
# Copy notebooks from the live server.
#[root@ansibler]

    ssh zeppelin \
        '
        sshuser=fedora
        sshhost=zeppelin.aglais.uk

        sudo mkdir -p '/var/local/backups'
        sudo mv "/home/fedora/zeppelin/notebook" \
           "/var/local/backups/notebook-$(date '+%Y%m%d%H%M%S')"

        ssh-keyscan "${sshhost:?}" >> "${HOME}/.ssh/known_hosts"

        rsync \
            --perms \
            --times \
            --group \
            --owner \
            --stats \
            --progress \
            --human-readable \
            --checksum \
            --recursive \
            "${sshuser:?}@${sshhost:?}:zeppelin/notebook/" \
            "/home/fedora/zeppelin/notebook"
        '

    >   ....
    >   ....
    >   Number of files: 712 (reg: 490, dir: 222)
    >   Number of created files: 712 (reg: 490, dir: 222)
    >   Number of deleted files: 0
    >   Number of regular files transferred: 490
    >   Total file size: 153.52M bytes
    >   ....
    >   ....


# -----------------------------------------------------
# re-start Zeppelin.
#[root@ansibler]

    ssh zeppelin \
        '
        zeppelin-daemon.sh restart
        '

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]


# -----------------------------------------------------
# Add the ssh key for our data node.
# This is used by the getpasshash function in the client container.
#[root@ansibler]

    ssh-keyscan 'data.aglais.uk' >> "${HOME}/.ssh/known_hosts"

    >   # data.aglais.uk:22 SSH-2.0-OpenSSH_8.0
    >   # data.aglais.uk:22 SSH-2.0-OpenSSH_8.0
    >   # data.aglais.uk:22 SSH-2.0-OpenSSH_8.0
    >   # data.aglais.uk:22 SSH-2.0-OpenSSH_8.0
    >   # data.aglais.uk:22 SSH-2.0-OpenSSH_8.0


# -----------------------------------------------------
# Create a test user.
#[root@ansibler]

    source /deployments/zeppelin/bin/create-user-tools.sh

    testusername=$(
        pwgen 8 1
        )

    createusermain \
        "${testusername}" \
    | tee "/tmp/${testusername}.json" | jq '.'

    testuserpass=$(
        jq -r '.shirouser.pass' "/tmp/${testusername}.json"
        )

    >   mkdir: No FileSystem for scheme "null"
    >   chown: No FileSystem for scheme "null"
    >   {
    >     "linuxuser": {
    >       "name": "caiqu4Za",
    >       "type": "test",
    >       "home": "/home/caiqu4Za",
    >       "uid": 20001
    >     },
    >     "shirouser": {
    >       "name": "caiqu4Za",
    >       "type": "test",
    >       "pass": "Xo8Aew6OongieKu5Gez7Aiz7booghe",
    >       "hash": "$shiro1$SHA-256$500000$UtrhiEneL+gLcF5n3Ho8vw==$Ar+7Pl3YgDCUGBQ+VO1gYyPUUSy3vgrUoQvZYxTWnhs="
    >     },
    >     "hdfsspace": {
    >       "user": "caiqu4Za",
    >       "type": "test",
    >       "path": "//user-hdfs/caiqu4Za"
    >     },
    >     ....
    >     ....
    >     ....
    >   }


    #
    # Fix the '//' prefix in /opt/aglais/bin/create_hdfs_user.sh
    #

# -----------------------------------------------------
# Create a test user.
#[root@ansibler]

    source /deployments/zeppelin/bin/create-user-tools.sh

    testusername=$(
        pwgen 8 1
        )

    createusermain \
        "${testusername}" \
    | tee "/tmp/${testusername}.json" | jq '.'

    testuserpass=$(
        jq -r '.shirouser.pass' "/tmp/${testusername}.json"
        )

    >   {
    >     "linuxuser": {
    >       "name": "ahth0aiS",
    >       "type": "test",
    >       "home": "/home/ahth0aiS",
    >       "uid": 20002
    >     },
    >     "shirouser": {
    >       "name": "ahth0aiS",
    >       "type": "test",
    >       "pass": "ohshePoZoofeitiej6we3iequ8ahta",
    >       "hash": "$shiro1$SHA-256$500000$W+QHRFbm9YUZvMz9kbvOfg==$914VJVYooxBa2TgYZEG4Xocza9ktKs3U+lnwvvOvae4="
    >     },
    >     "hdfsspace": {
    >       "user": "ahth0aiS",
    >       "type": "test",
    >       "path": "/user-hdfs/ahth0aiS"
    >     },
    >     "notebooks": [
    >       {
    >         "status": "OK",
    >         "message": "",
    >         "body": {
    >           "principal": "ahth0aiS",
    >           "ticket": "e5912786-2383-4d3d-9110-cb7b43e91835",
    >           "roles": "[\"user\"]"
    >         }
    >       },
    >       {
    >         "status": "OK",
    >         "message": "",
    >         "body": "2H46RQ2AY"
    >       },
    >       {
    >         "status": "OK",
    >         "message": "",
    >         "body": "2H5W7538F"
    >       },
    >       {
    >         "status": "OK",
    >         "message": "",
    >         "body": "2H48SR12C"
    >       },
    >       {
    >         "status": "OK",
    >         "message": "",
    >         "body": "2H3QSBM7A"
    >       },
    >       {
    >         "status": "OK",
    >         "message": "",
    >         "body": "2H3GA7JGT"
    >       },
    >       {
    >         "status": "OK",
    >         "message": "",
    >         "body": "2H4PGVB6U"
    >       },
    >       {
    >         "status": "OK",
    >         "message": "",
    >         "body": "2H3S4JDJV"
    >       },
    >       {
    >         "status": "OK",
    >         "message": "",
    >         "body": "2H39D6556"
    >       }
    >     ]
    >   }


# -----------------------------------------------------
# Check the user's home, data and hdfs directories have been created.
#[root@ansibler]

    ssh zeppelin \
        "
        date
        hostname

        echo
        linuxhome=\$(getent passwd \"${testusername}\" | cut -d: -f6 )
        echo \"Linx home [\${linuxhome}]\"
        stat --format '
            "linuxhome": {
            \"path\": \"%n\",
            \"size\": %s,
            \"type\": \"%F\",
            \"owner\": \"%U\",
            \"group\": \"%G\",
            \"chmod\": \"%A\"
            }
            ' \"\${linuxhome}\"

        echo
        cephdata=/user/${testusername}
        echo \"Ceph data [\${cephdata}]\"
        stat --format '
            "cephdata": {
            \"path\": \"%n\",
            \"size\": %s,
            \"type\": \"%F\",
            \"owner\": \"%U\",
            \"group\": \"%G\",
            \"chmod\": \"%A\"
            }
            ' \"\${cephdata}\"

        echo
        hdfshome=/user-hdfs/${testusername}
        echo \"HDFS home [\${hdfshome}]\"
        hdfs dfs -stat '
            "hdfshome": {
            \"name\": \"%n\",
            \"size\": %b,
            \"type\": \"%F\",
            \"owner\": \"%u\",
            \"group\": \"%g\"
            \"chmod\": \"%A\"
            }' \
            \"\${hdfshome}\"
        "


    >   Thu 26 May 2022 01:38:38 PM UTC
    >   iris-gaia-blue-20220526-zeppelin
    >   
    >   Linx home [/home/ahth0aiS]
    >   
    >               linuxhome: {
    >               "path": "/home/ahth0aiS",
    >               "size": 4096,
    >               "type": "directory",
    >               "owner": "ahth0aiS",
    >               "group": "ahth0aiS",
    >               "chmod": "drwx------"
    >               }
    >   
    >   
    >   Ceph data [/user/ahth0aiS]
    >   stat: cannot stat '/user/ahth0aiS': No such file or directory
    >   
    >   HDFS home [/user-hdfs/ahth0aiS]
    >   
    >               hdfshome: {
    >               "name": "ahth0aiS",
    >               "size": 0,
    >               "type": "directory",
    >               "owner": "ahth0aiS",
    >               "group": "supergroup"
    >               "chmod": "rwxr-xr-x"
    >               }


# -----------------------------------------------------
# Check we can ssh from fedora to the new user.
# This simulates the ssh login used by Zeppelin for user impersonation.
#[root@ansibler]

    ssh zeppelin \
        "
        date
        hostname
        whoami
        id
        echo
        ssh ${testusername}@localhost \
            '
            date
            hostname
            whoami
            id
            '
        echo
        date
        hostname
        whoami
        id
        "

    >   Thu 26 May 2022 01:39:22 PM UTC
    >   iris-gaia-blue-20220526-zeppelin
    >   fedora
    >   uid=1000(fedora) gid=1000(fedora) groups=1000(fedora),4(adm),10(wheel),190(systemd-journal) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
    >   
    >   Thu 26 May 2022 01:39:22 PM UTC
    >   iris-gaia-blue-20220526-zeppelin
    >   ahth0aiS
    >   uid=20002(ahth0aiS) gid=20002(ahth0aiS) groups=20002(ahth0aiS),100(users),1006(zeppelinusers) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
    >   
    >   Thu 26 May 2022 01:39:22 PM UTC
    >   iris-gaia-blue-20220526-zeppelin
    >   fedora
    >   uid=1000(fedora) gid=1000(fedora) groups=1000(fedora),4(adm),10(wheel),190(systemd-journal) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023



# -----------------------------------------------------
# -----------------------------------------------------
# Update our DuckDNS record.
# TODO Configure our secret function on data node.
#[user@desktop]

    duckname=iris-gaia-blue
    ducktoken=$(secret 'aglais.duckdns.token')
    zeppelinip=128.232.222.6

    curl "https://www.duckdns.org/update/${duckname:?}/${ducktoken:?}/${zeppelinip:?}"

    >   OK


# -----------------------------------------------------
# -----------------------------------------------------
# Login to Zeppelin as the test user.
#[root@ansibler]

    zeppelinhost=blue.aglais.uk
    zeppelinport=8080
    zeppelinurl=http://${zeppelinhost:?}:${zeppelinport:?}

    source '/deployments/zeppelin/bin/zeppelin-rest-tools.sh'

    zeplogin "${testusername:?}" "${testuserpass:?}"

    >   {
    >     "status": "OK",
    >     "message": "",
    >     "body": {
    >       "principal": "ahth0aiS",
    >       "ticket": "e5912786-2383-4d3d-9110-cb7b43e91835",
    >       "roles": "[\"user\"]"
    >     }
    >   }


# -----------------------------------------------------
# List the user's notebooks
#[root@ansibler]

    curl \
        --silent \
        --cookie "${zepcookies:?}" \
        "${zeppelinurl:?}/api/notebook" \
    | jq "[.body[] | select(.path | startswith(\"/Users/${testusername:?}\"))]"

    >   [
    >     {
    >       "id": "2H46RQ2AY",
    >       "path": "/Users/ahth0aiS/1. Start here"
    >     },
    >     {
    >       "id": "2H5W7538F",
    >       "path": "/Users/ahth0aiS/2. Data holdings"
    >     },
    >     {
    >       "id": "2H48SR12C",
    >       "path": "/Users/ahth0aiS/3. Source counts over the sky"
    >     },
    >     {
    >       "id": "2H3QSBM7A",
    >       "path": "/Users/ahth0aiS/4. Mean proper motions over the sky"
    >     },
    >     {
    >       "id": "2H3GA7JGT",
    >       "path": "/Users/ahth0aiS/5. Working with Gaia XP spectra"
    >     },
    >     {
    >       "id": "2H4PGVB6U",
    >       "path": "/Users/ahth0aiS/6. Working with cross-matched surveys"
    >     },
    >     {
    >       "id": "2H3S4JDJV",
    >       "path": "/Users/ahth0aiS/7. Good astrometric solutions via ML Random Forrest classifier"
    >     },
    >     {
    >       "id": "2H39D6556",
    >       "path": "/Users/ahth0aiS/8. Tips and tricks"
    >     }
    >   ]


# -----------------------------------------------------
# Run one of the user's notebooks.
#[root@ansibler]

    noteid=2H48SR12C

    zepnbclear "${noteid}"

    >   {
    >     "status": "OK",
    >     "message": ""
    >   }


    zepnbexecstep "${noteid}"

    >   Para [20210507-084613_357121151][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS"
    >     }
    >   }
    >   Result [SUCCESS]
    >   
    >   Para [20200826-105718_1698521515][Set the resolution level and define the query]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "ERROR"
    >     }
    >   }
    >   Result [ERROR]


# -----------------------------------------------------
# Same error as before.
#[fedora@zeppelin]

    cat /home/fedora/zeppelin/logs/zeppelin-interpreter-spark-ahth0aiS-ahth0aiS-fedora-iris-gaia-blue-20220526-zeppelin.log

    >    ....
    >    ....
    >    INFO [2022-05-26 13:41:31,999] ({FIFOScheduler-interpreter_928197588-Worker-1} Logging.scala[logInfo]:57) - Preparing resources for our AM container
    >   ERROR [2022-05-26 13:41:32,099] ({FIFOScheduler-interpreter_928197588-Worker-1} Logging.scala[logError]:94) - Error initializing SparkContext.
    >   org.apache.hadoop.security.AccessControlException: Permission denied: user=ahth0aiS, access=WRITE, inode="/":fedora:supergroup:drwxr-xr-x
    >           at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
    >    ....
    >           at java.lang.Thread.run(Thread.java:748)
    >   Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=ahth0aiS, access=WRITE, inode="/":fedora:supergroup:drwxr-xr-x
    >           at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
    >    ....

    "Caused by .. RemoteException" suggests the error is being thrown on the master or one of the worker nodes.
    Could still be a side effect of a local config setting.

    Tried adding this property to the config on Zeppelin

        sudo vi /opt/hadoop/etc/hadoop/hdfs-site.xml

            <property>
                <name>dfs.user.home.base.dir</name>
                <value>/user-hdfs</value>
            </property>

    Try adding the /user directory into HDFS.

        hdfs dfs -chown -R 'ahth0aiS:supergroup' /user/ahth0aiS


# -----------------------------------------------------
# Run one of the user's notebooks.
#[root@ansibler]

    noteid=2H48SR12C

    zepnbexecstep "${noteid}"

    >   
    >   Para [20210507-084613_357121151][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS"
    >     }
    >   }
    >   Result [SUCCESS]
    >   
    >   Para [20200826-105718_1698521515][Set the resolution level and define the query]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS"
    >     }
    >   }
    >   Result [SUCCESS]
    >   
    >   Para [20200826-110030_2095441495][Plot up the results]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS"
    >     }
    >   }
    >   Result [SUCCESS]
    >   
    >   Para [20210507-091244_670006530][Further reading and resources]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS"
    >     }
    >   }
    >   Result [SUCCESS]
    >   
    >   Para [paragraph_1648610499944_1376690736][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS"
    >     }
    >   }
    >   Result [SUCCESS]

    #
    # So we know what the problem is, but we can't figure out how to change the default path.
    #

# -----------------------------------------------------
# The location of the directory Spark uses for staging data.
#[root@ansibler]

    ssh zeppelin

        hdfs dfs -ls /user/ahth0aiS/.sparkStaging/application_1653574728480_0002

    >   Found 5 items
    >   -rw-r--r--   3 ahth0aiS supergroup     260126 2022-05-26 14:41 /user/ahth0aiS/.sparkStaging/application_1653574728480_0002/__spark_conf__.zip
    >   -rw-r--r--   3 ahth0aiS supergroup  228545969 2022-05-26 14:41 /user/ahth0aiS/.sparkStaging/application_1653574728480_0002/__spark_libs__4631964462504279886.zip
    >   -rw-r--r--   3 ahth0aiS supergroup      41587 2022-05-26 14:41 /user/ahth0aiS/.sparkStaging/application_1653574728480_0002/py4j-0.10.9-src.zip
    >   -rw-r--r--   3 ahth0aiS supergroup     886596 2022-05-26 14:41 /user/ahth0aiS/.sparkStaging/application_1653574728480_0002/pyspark.zip
    >   -rw-r--r--   3 ahth0aiS supergroup    2180681 2022-05-26 14:41 /user/ahth0aiS/.sparkStaging/application_1653574728480_0002/sparkr.zip


        hdfs dfs -ls /user/ahth0aiS/.sparkStaging/

    >   Found 1 items
    >   drwx------   - ahth0aiS supergroup          0 2022-05-26 14:41 /user/ahth0aiS/.sparkStaging/application_1653574728480_0002


        hdfs dfs -ls /user

    >   Found 1 items
    >   drwxr-xr-x   - ahth0aiS supergroup          0 2022-05-26 14:41 /user/ahth0aiS


        hdfs dfs -ls /

    >   Found 3 items
    >   drwxr-xr-x   - fedora supergroup          0 2022-05-26 12:30 /spark-log
    >   drwxr-xr-x   - fedora supergroup          0 2022-05-26 14:39 /user
    >   drwxr-xr-x   - fedora supergroup          0 2022-05-26 12:55 /user-hdfs

    To be able to write to these directories, Hadoop needs to be running as either 'ahth0aiS' or be a member of 'supergroup'.

    'supergroup' isn't a Linux group on any of the machines

        cat /etc/group | grep super

    'ahth0aiS' only exists on the Zeppelin node

        id 'ahth0aiS'


    At the moment:

        The Hadoop jobs must be accessing the HDFS system as 'supergroup'.
        The user's HDFS home is fixed as /user/<username>

    Found a clue:

        worker01
        /var/hadoop/logs/application_1653574728480_0002/container_1653574728480_0002_01_000005

        vi launch_container.sh

            ....
            export LOCAL_DIRS="/var/hadoop/data/usercache/ahth0aiS/appcache/application_1653574728480_0002"
            export LOCAL_USER_DIRS="/var/hadoop/data/usercache/ahth0aiS/"
            export PWD="/var/hadoop/data/usercache/ahth0aiS/appcache/application_1653574728480_0002/container_1653574728480_0002_01_000005"
            export SPARK_YARN_STAGING_DIR="hdfs://master01:9000/user/ahth0aiS/.sparkStaging/application_1653574728480_0002"
            ....

        Documentation for the staging directory
        https://spark.apache.org/docs/latest/running-on-yarn.html#spark-properties

            spark.yarn.stagingDir        Staging directory used while submitting applications.

            Defaults to
                "Current user's home directory in the filesystem"

            So that implies something is setting it to a HDFS directory ?

        Try setting this on the live system.

        zeppelin
        vi /opt/spark/conf/spark-defaults.conf

            spark.yarn.stagingDir   hdfs://master01:9000/spark-log

    Found another clue:

        Spark staging on HDFS
        https://stackoverflow.com/questions/52843010/why-is-spark-sparkstaging-folder-under-hdfs-when-running-spark-on-yarn-in-local


# -----------------------------------------------------
# Run one of the user's notebooks.
#[root@ansibler]

    noteid=2H48SR12C

    zeplogin "${testusername:?}" "${testuserpass:?}"

    zepnbexecstep "${noteid}"

    >   Para [20200826-105718_1698521515][Set the resolution level and define the query]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "ERROR"
    >     }
    >   }

    #
    # Hmm .. broken again.
    # Not expecting that.
    #


    #
    # If we comment out the default filesystem setting.
    #

    zeppelin
    sudo vi /opt/hadoop/etc/hadoop/core-site.xml

        <!--property>
            <name>fs.default.name</name>
            <value>hdfs://master01:9000</value>
        </property-->

    The sparkStaging defaults to the fedora user's home directory,
    and Spark 'uploads' to 'file:/home/fedora/.sparkStaging'

    >    ....
    >    INFO [2022-05-26 16:14:12,996] ({FIFOScheduler-interpreter_1853955287-Worker-1} Logging.scala[logInfo]:57) - Uploading resource file:/mnt/cinder/vdc/spark/temp/spark-70619fe4-9984-4c20-8bef-7284fc5616d7/__spark_libs__2519042166635202816.zip -> file:/home/fedora/.sparkStaging/application_1653574728480_0003/__spark_libs__2519042166635202816.zip
    >    INFO [2022-05-26 16:14:13,387] ({FIFOScheduler-interpreter_1853955287-Worker-1} Logging.scala[logInfo]:57) - Uploading resource file:/opt/spark/R/lib/sparkr.zip#sparkr -> file:/home/fedora/.sparkStaging/application_1653574728480_0003/sparkr.zip
    >    INFO [2022-05-26 16:14:13,411] ({FIFOScheduler-interpreter_1853955287-Worker-1} Logging.scala[logInfo]:57) - Uploading resource file:/opt/spark/python/lib/pyspark.zip -> file:/home/fedora/.sparkStaging/application_1653574728480_0003/pyspark.zip
    >    INFO [2022-05-26 16:14:13,429] ({FIFOScheduler-interpreter_1853955287-Worker-1} Logging.scala[logInfo]:57) - Uploading resource file:/opt/spark/python/lib/py4j-0.10.9-src.zip -> file:/home/fedora/.sparkStaging/application_1653574728480_0003/py4j-0.10.9-src.zip
    >    INFO [2022-05-26 16:14:13,523] ({FIFOScheduler-interpreter_1853955287-Worker-1} Logging.scala[logInfo]:57) - Uploading resource file:/mnt/cinder/vdc/spark/temp/spark-70619fe4-9984-4c20-8bef-7284fc5616d7/__spark_conf__6829593716976306777.zip -> file:/home/fedora/.sparkStaging/application_1653574728480_0003/__spark_conf__.zip
    >    ....

    and the application fails to run

    >   ....
    >   Caused by: org.apache.spark.SparkException: Application application_1653574728480_0003 failed 2 times due to AM Container for appattempt_1653574728480_0003_000002 exited with  exitCode: -1000
    >   Failing this attempt.Diagnostics: [2022-05-26 16:14:14.206]File file:/home/fedora/.sparkStaging/application_1653574728480_0003/sparkr.zip does not exist
    >   java.io.FileNotFoundException: File file:/home/fedora/.sparkStaging/application_1653574728480_0003/sparkr.zip does not exist
    >   ....

    this is because Spar uses sparkStaging to transfer files

    ssh worker01 '
        grep -r 'sparkStaging' /var/hadoop/logs/
        '

    >   ./application_1653574728480_0002/container_1653574728480_0002_01_000011/launch_container.sh:export SPARK_YARN_STAGING_DIR="hdfs://master01:9000/user/ahth0aiS/.sparkStaging/application_1653574728480_0002"
    >   ./application_1653574728480_0002/container_1653574728480_0002_01_000040/launch_container.sh:export SPARK_YARN_STAGING_DIR="hdfs://master01:9000/user/ahth0aiS/.sparkStaging/application_1653574728480_0002"
    >   ./application_1653574728480_0002/container_1653574728480_0002_01_000035/launch_container.sh:export SPARK_YARN_STAGING_DIR="hdfs://master01:9000/user/ahth0aiS/.sparkStaging/application_1653574728480_0002"
    >   ./application_1653574728480_0002/container_1653574728480_0002_01_000043/launch_container.sh:export SPARK_YARN_STAGING_DIR="hdfs://master01:9000/user/ahth0aiS/.sparkStaging/application_1653574728480_0002"
    >   ./application_1653574728480_0002/container_1653574728480_0002_01_000005/launch_container.sh:export SPARK_YARN_STAGING_DIR="hdfs://master01:9000/user/ahth0aiS/.sparkStaging/application_1653574728480_0002"

    ssh worker02 '
        grep -r 'sparkStaging' /var/hadoop/logs/
        '

    >   /var/hadoop/logs/application_1653574728480_0002/container_1653574728480_0002_01_000031/launch_container.sh:export SPARK_YARN_STAGING_DIR="hdfs://master01:9000/user/ahth0aiS/.sparkStaging/application_1653574728480_0002"
    >   /var/hadoop/logs/application_1653574728480_0002/container_1653574728480_0002_01_000034/launch_container.sh:export SPARK_YARN_STAGING_DIR="hdfs://master01:9000/user/ahth0aiS/.sparkStaging/application_1653574728480_0002"
    >   /var/hadoop/logs/application_1653574728480_0002/container_1653574728480_0002_01_000016/launch_container.sh:export SPARK_YARN_STAGING_DIR="hdfs://master01:9000/user/ahth0aiS/.sparkStaging/application_1653574728480_0002"
    >   /var/hadoop/logs/application_1653574728480_0002/container_1653574728480_0002_01_000010/launch_container.sh:export SPARK_YARN_STAGING_DIR="hdfs://master01:9000/user/ahth0aiS/.sparkStaging/application_1653574728480_0002"
    >   /var/hadoop/logs/application_1653574728480_0002/container_1653574728480_0002_01_000004/launch_container.sh:export SPARK_YARN_STAGING_DIR="hdfs://master01:9000/user/ahth0aiS/.sparkStaging/application_1653574728480_0002"
    >   /var/hadoop/logs/hadoop-fedora-nodemanager-iris-gaia-blue-20220526-worker02.log:2022-05-26 16:14:14,205 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: { file:/home/fedora/.sparkStaging/application_1653574728480_0003/sparkr.zip, 1653581653000, ARCHIVE, null } failed: File file:/home/fedora/.sparkStaging/application_1653574728480_0003/sparkr.zip does not exist
    >   /var/hadoop/logs/hadoop-fedora-nodemanager-iris-gaia-blue-20220526-worker02.log:java.io.FileNotFoundException: File file:/home/fedora/.sparkStaging/application_1653574728480_0003/sparkr.zip does not exist
    >   /var/hadoop/logs/hadoop-fedora-nodemanager-iris-gaia-blue-20220526-worker02.log:2022-05-26 16:14:14,206 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl: Container container_1653574728480_0003_02_000001 sent RELEASE event on a resource request { file:/home/fedora/.sparkStaging/application_1653574728480_0003/sparkr.zip, 1653581653000, ARCHIVE, null } not present in cache.

    #
    # Sooo .. mud becomes clearer.
    # Spark uses sparkStaging as a shared filesystem to pass objects (data and code) to the workers.
    # Something is setting the sparkStaging location to
    #

    #
    # So we need the default FS to be HDFS.

    zeppelin
    sudo vi /opt/hadoop/etc/hadoop/core-site.xml

        <property>
            <name>fs.default.name</name>
            <value>hdfs://master01:9000</value>
        </property>

    #
    # .. but we also need to set the user's 'home directory' path within that,
    #


./sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala:        val uriPath = new Path(s"/user/${System.getProperty("user.name")}/")
./core/src/test/scala/org/apache/spark/SparkContextSuite.scala:        val badURL = s"$scheme://user:pwd/path"


